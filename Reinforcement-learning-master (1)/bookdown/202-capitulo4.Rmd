# Métodos de Montecarlo

En este capítulo consideramos nuestros primeros métodos de aprendizaje para estimar las funciones de valor y descubrir las políticas óptimas. A diferencia del capítulo anterior, aquí no suponemos un conocimiento completo del entorno. Los métodos de Monte Carlo requieren sólo experiencia -muestras de secuencias de estados, acciones y recompensas de la interacción real o simulada con un entorno. Aprender de la experiencia real es sorprendente porque no requiere un conocimiento previo de la dinámica del entorno, pero aún así puede lograr un comportamiento óptimo. Aprender de la experiencia simulada también es poderoso. Aunque se requiere un modelo, el modelo sólo necesita generar transiciones de muestra, no las distribuciones de probabilidad completas de todas las transiciones posibles que se requieren para la programación dinámica (DP). En muchos casos es sorprendentemente lo fácil que es generar experiencia muestreada de acuerdo a las distribuciones de probabilidad deseadas, pero no es factible obtener las distribuciones en forma explícita.


Los métodos de Monte Carlo son formas de resolver el problema de Reinforcement Learning basado en el promedio de los resultados de las muestras. Para asegurar que los retornos bien definidos estén disponibles, aquí definimos los métodos de Monte Carlo sólo para las tareas episódicas. Es decir, asumimos que la experiencia está dividida en episodios, y que todos los episodios terminan finalmente sin importar qué acciones se seleccionen. Sólo cuando se completa un episodio se cambian las estimaciones de valor y las políticas. Por lo tanto, los métodos de Monte Carlo pueden ser incrementales en el sentido de episodio por episodio, pero no en el sentido de paso a paso (en línea). El término "Monte Carlo" se utiliza a menudo de manera más amplia para cualquier método de estimación cuya operación implica un componente aleatorio significativo. Aquí lo usamos específicamente para métodos basados en promediar retornos completos (a diferencia de los métodos que aprenden de los retornos parciales, considerados en el siguiente capítulo).


Los métodos de Monte Carlo muestran y promedian los resultados de cada par de accion-estatado de manera muy parecida a los métodos de bandidos que exploramos en el Capítulo 2, y las recompensas promedio para cada acción. La principal diferencia es que ahora hay múltiples estados, cada uno actuando como un problema de bandidos diferente (como una búsqueda asociativa o un bandido contextual) y los diferentes problemas de bandidos están interrelacionados. Es decir, el retorno después de tomar una acción en un estado depende de las acciones tomadas en estados posteriores en el mismo episodio. Debido a que todas las selecciones de acciones están en aprendizaje, el problema se vuelve no estacionario desde el punto de vista del estado anterior. 

Para manejar la no estacionalidad, adaptamos la idea de iteración de políticas generales (GPI) desarrollada en el Capítulo 4 para DP. Mientras que allí calculamos funciones de valor a partir del conocimiento del MDP, aquí aprendemos funciones de valor a partir de los retornos de muestras con el MDP. Las funciones de valor y las políticas correspondientes todavía interactúan para alcanzar la optimización de la misma manera (GPI). Como en el capítulo de DP, primero consideramos el problema de la predicción (el cálculo de $v_\pi$ y $q_\pi$ para una política arbitraria fija $\pi$), luego la mejora de la política y, finalmente, el problema del control y su solución por parte de GPI. Cada una de estas ideas tomadas de DP se extiende al caso de Monte Carlo, en el que sólo se dispone de una muestra de experiencia.

## Predicción con Monte Carlo

Comenzamos por considerar los métodos de Monte Carlo para aprender la función de valor estado para una política determinada. Recordemos que el valor de un estado es el retorno esperado, recompensa esperada acumulada descontada en el futuro a partir de ese estado. Una manera obvia de estimarlo a partir de la experiencia, entonces, es simplemente promediar los rendimientos observados después de las visitas a ese estado. A medida que se observan más retornos, el promedio debe converger hacia el valor esperado. Esta idea es la base de todos los métodos de Monte Carlo.

En particular, supongamos que deseamos estimar $v_\pi(s)$, el valor de un estado $s$ bajo la política $\pi$, dado un conjunto de episodios obtenidos siguiendo $\pi$ y pasando por $s$. Cada ocurrencia de estados $s$ en un episodio se llama una visita a $s$. Por supuesto, $s$ puede ser visitado varias veces en el mismo episodio; llamemos a la primera vez que se visita en un episodio la primera visita a $s$. El método MC de la primera visita estima $v_\pi(s)$ como el promedio de los retornos después de las primeras visitas a $s$, mientras que el método MC de cada visita promedia los retornos después de todas las visitas a $s$. Estos dos métodos de Monte Carlo (MC) son muy similares pero tienen propiedades teóricas ligeramente diferentes. La primera visita al MC ha sido la más ampliamente estudiada, ya que se remonta a la década de 1940, y es en la que nos centramos en este capítulo. Cada visita a MC se extiende de manera más natural a la aproximación funcional y a los rastros de elegibilidad, como se discute en los Capítulos 9 y 12. El MC de la primera visita se muestra en forma de procedimiento acontiniuación.


**Predicción de la primera visita del MC, para estimar $V\approx v_\pi$ **

* Inicializamos:
    + $\pi \leftarrow$ política a ser evaluada.
    + $V \leftarrow$ una función arbitraria de valor estado.
    + Retorno$(s)$ una lista vacia, para todo $s \in S$
* Repetimos por siempre:
    + Generamos un episodio usando $\pi$
    + Para cada estado $s$ aparentemente en el episodio:
        - $G \leftarrow$ el retorno que proviene de la primera ocurrencia de $s$
        - Añadir $G$ al Retorno($s$)    
        - $V(s) \leftarrow$ Promedio(Retorno($s$))

Tanto el MC de la primera visita como el MC de cada visita convergen a $v_\pi(s)$, ya que el número de visitas (o primeras visitas) a $s$ es infinito. Esto es fácil de ver en el caso de la primera visita del MC. En este caso, cada retorno es una estimación independiente e idénticamente distribuida de $v_\pi(s)$ con varianza finita. Según la ley de los grandes números, la secuencia de los promedios de estas estimaciones converge a su valor esperado. Cada promedio es en sí mismo una estimación imparcial, y la desviación estándar de su error es de $\frac{1}{\sqrt{n}}$, donde $n$ es el número de retornos promediados. Cada visita a MC es menos directa, pero sus estimaciones también convergen cuadráticamente a $v_\pi(s)$ 

El método de Monte Carlo será ilustrado en el siguiente ejemplo.

**Ejemplo: Blackjack** El objetivo del popular juego de casino de cartas de blackjack es obtener cartas cuya suma de valores numéricos sea lo más grande posible sin exceder 21. Todas las cartas cuentan como 10, y un as puede contar como 1 o como 11. Consideramos la versión en la que cada jugador compite independientemente contra el dealer. El juego comienza con dos cartas repartidas tanto al crupier como al jugador. Una de las cartas de la banca está boca arriba y la otra boca abajo. Si el jugador tiene 21 inmediatamente (un as y una carta de 10), se llama natural. A continuación, gana a menos que el dealer también tiene un natural, en cuyo caso el juego es un empate. Si el jugador no tiene un natural, entonces puede pedir cartas adicionales, una por una (hits), hasta que se detenga (sticks) o exceda 21 (va a la quiebra). Si va a la quiebra, pierde; si se detiene, entonces se convierte en el turno del dealer. El crupier se detiene o pide otra carta de acuerdo a una estrategia fija sin elección: se detiene a cualquier suma de 17 o más, y pide otra carta en caso contrario. Si el dealer va a la quiebra entonces el jugador gana; en otro caso, el resultado (salida), ganar, empatar o perder, es determinado por el que tenga el resultado mas cercano a 21. 

Jugar blackjack está naturalmente formulado como un MDP episódico finito. Cada juego de blackjack es un episodio. Las recompensas de +1, -1 y 0 se dan por ganar, perder y empatar, respectivamente. Todas las recompensas dentro de un juego son cero, y no hacemos descuentos (γ = 1); por lo tanto, estas recompensas terminales son también los retornos. Las acciones del jugador son los hits o sticks. Los estados dependen de las cartas del jugador y de la carta que muestre el dealer.  Suponemos que las cartas se reparten desde una baraja infinita (es decir, con reemplazo) de modo que no hay ventaja en llevar un registro de las cartas ya repartidas. Si el jugador tiene un as que podría contar como 11 sin quebrar, entonces se dice que el as es utilizable. En este caso siempre se cuenta como 11 porque contarlo como 1 haría la suma de 11 o menos, en cuyo caso no hay que tomar ninguna decisión porque, obviamente, el jugador siempre debe hacer hit. Así, el jugador toma decisiones sobre la base de tres variables: su suma actual (12-21), la carta que muestra el crupier (as-10), y si tiene o no un as utilizable. Esto hace un total de 200 estados. 

Considere la política que hace stick si la suma del jugador es 20 o 21, y de otra manera hace hit. Para encontrar la función de valor estado para esta política mediante un enfoque de Monte Carlo, se simulan muchos juegos de blackjack utilizando la política y se promedian los retornos después de cada estado. Tenga en cuenta que en esta tarea el mismo estado nunca se repite dentro de un episodio, por lo que no hay diferencia entre los métodos de MC de primera visita y los de cada visita. De esta manera, obtuvimos las estimaciones de la función estado-valor mostradas en la siguiente fº. Las estimaciones para los estados con un as utilizable son menos seguras y menos regulares porque estos estados son menos comunes. En cualquier caso, después de 500.000 partidas la función de valor está muy bien aproximada.

Aunque tenemos un conocimiento completo del entorno en esta tarea, no sería fácil aplicar métodos DP para calcular la función de valor. Los métodos DP requieren la distribución de los siguientes eventos (en particular, requieren la dinámica del entornos tal y como la da la función de cuatro argumentos $p$) y no es fácil determinar esto para el blackjack. Por ejemplo, supongamos que la suma del jugador es 14 y decide quedarse. ¿Cuál es su probabilidad de terminar con una recompensa de +1 en función de la carta que muestre el dealer? Todas las probabilidades deben ser calculadas antes de que DP pueda ser aplicado, y tales cálculos son a menudo complejos y propensos a errores. En contraste, generar los juegos de muestra requeridos por los métodos de Monte Carlo es fácil. Este es el caso que ocurre con mucha frecuencia; la capacidad de los métodos de Monte Carlo para trabajar con episodios de muestra puede ser una ventaja significativa incluso cuando se tiene un conocimiento completo de la dinámica del entorno.

![Serie de movimientos\label{fig:"sd"}](~/Reinforcement-learning/as.png)


  ¿Podemos generalizar la idea de los diagramas de apoyo a los algoritmos de Monte Carlo? La idea general de un diagrama de apoyo es mostrar en la parte superior el nodo raíz que se va a actualizar y mostrar debajo todas las transiciones y nodos de hoja cuyas recompensas y valores estimados contribuyen a la actualización. Para la estimación de Monte Carlo de $v_\pi$, la raíz es un nodo de estado, y debajo está la trayectoria completa de las transiciones a lo largo de un episodio en particular, terminando en el estado terminal.

Un hecho importante sobre los métodos de Monte Carlo es que las estimaciones para cada estado son independientes. La estimación para un estado no se basa en la estimación de cualquier otro estado, como es el caso de DP. En otras palabras, los métodos Monte Carlo no arrancan como lo definimos en el capítulo anterior.

En particular, tenga en cuenta que el gasto computacional de estimar el valor de un solo estado es independiente del número de estados. Esto puede hacer que los métodos de Monte Carlo sean particularmente atractivos cuando se requiere el valor de sólo uno o un subconjunto de estados. Se pueden generar muchos episodios de muestra a partir de los estados de interés, promediando los retornos sólo de estos estados, ignorando todos los demás. Esta es una tercera ventaja que los métodos de Monte Carlo pueden tener sobre los métodos DP (después de la capacidad de aprender de la experiencia real y de la experiencia simulada).

## Estimación de Monte Carlo de los Valores de Acción

Si no se dispone de un modelo, resulta especialmente útil estimar los valores de acción (los valores de los pares  acción-estado) en lugar de los valores de estado. Con un modelo, los valores de los estados por sí solos son suficientes para determinar una política; uno simplemente mira hacia adelante un paso y elige cualquier acción que lleve a la mejor combinación de recompensa y el siguiente estado, como hicimos en el capítulo sobre DP. Sin embargo, sin un modelo, los valores de los estados por sí solos no son suficientes. Hay que estimar explícitamente el valor de cada acción para que los valores sean útiles a la hora de sugerir una política. Por lo tanto, uno de nuestros principales objetivos para los métodos de Monte Carlo es estimar $q_*$. Para lograr esto, primero consideramos el problema de la evaluación de políticas para los valores de acción.

El problema de evaluación de políticas para un par acción estado es estimar $q_\pi(s,a)$, lo cual es el retorno final esperado al iniciar en el estado $s$, tomar la acción $a$ y de ahí en adelante seguir la política $\pi$. Los métodos de Monte Carlo para esto son esencialmente los mismos que los que se acaban de presentar para los valores de los estados, excepto que ahora hablamos de visitas a un par de acción-estado en lugar de un estado. Un par acción-estado $s,a$ se dice que se visita en un episodio si alguna vez se visita el estado $s$ y se toma la acción $a$ en él. El método MC en cada visita estima el valor de un par de acción de estado como el promedio de los retornos que han seguido a todas las visitas a él. El método MC de la primera visita promedia los retornos después de la primera vez en cada episodio en que se visitó el estado y se seleccionó la acción. Estos métodos convergen cuadráticamente, como antes, a los verdaderos valores esperados a medida que el número de visitas a cada par de acción-estado se acerca al infinito.

La única complicación es que muchos pares de acción-estado nunca pueden ser visitados o son visitados con muy poca frecuencia. Si es una política ya determinada entonces, al seguirla, se observarán los retornos sólo para una de las acciones de cada estado. Sin retornos al promedio, las estimaciones de Monte Carlo de las otras acciones no mejorarán con la experiencia. Se trata de un problema grave porque el propósito del aprendizaje de los valores de acción es ayudar a elegir entre las acciones disponibles en cada estado. Para comparar alternativas necesitamos estimar el valor de todos los acciones de cada estado, no sólo la que actualmente favorecemos o tomamos con mayor frecuencia.

Este es el problema general de *mantener la exploración*, como se discutió en el contexto del problema del bandido de $k$-brasos en el capítulo 2. Para que la evaluación de políticas funcione a favor de los valores de acción, debemos asegurar una continua exploración. Una forma de hacerlo es especificar que los episodios comienzan en un par acción-estado, y que cada par tiene una probabilidad distinta de cero de ser seleccionado como inicio. Esto garantiza que todo par acción-estado serán visitados un número mínimo de veces en el límite de un número mínimo de episodios. Llamamos a esto como la suposición de que la exploración comienza.

La suposición de comenzar a explorar es a veces útil, pero por supuesto no se puede confiar en ella en general, particularmente cuando se aprende directamente de la interacción real con un entorno. En ese caso, es poco probable que las condiciones de partida sean tan útiles. El enfoque alternativo más común para asegurar que se encuentren todos los pares de acción-estado es considerar sólo políticas estocásticas con una probabilidad no nula de seleccionar todas las acciones en cada estado. Discutiremos dos variantes importantes de este enfoque en secciones posteriores. Por ahora, mantenemos el supuesto de que la exploración comienza y completa la presentación de un método de control completo de Monte Carlo.

## Métodos de Monte Carlo con control

Ahora estamos listos para considerar cómo se puede utilizar la estimación de Monte Carlo en el control, es decir, para aproximar las políticas óptimas. La idea general es proceder de acuerdo con el mismo patrón que en el capítulo de DP, es decir, de acuerdo con la idea de la iteración generalizada de políticas (GPI). En GPI se mantiene tanto una política aproximada como una función de valor aproximada. La función de valor se altera repetidamente para aproximarse más a la función de valor de la política actual, y la política se mejora repetidamente con respecto a la función de valor actual, como se sugiere en la siguiente. Estos dos tipos de cambios se contraponen hasta cierto punto, ya que cada uno de ellos crea un objetivo móvil para el otro, pero juntos hacen que tanto la política como la función de valor se acerquen a la optimización.

![Serie de movimientos\label{fig:"sd"}](~/Reinforcement-learning/mc.png)

Para empezar, consideremos una versión clásica del Monte Carlo de iteración de política. En este método, alternamos pasos completos de evaluación y mejora de políticas, comenzando con una política arbitraria $\pi_0$ y terminando con la política óptima y la función óptima de valor-acción:$$\pi_0\stackrel{E}{\rightarrow}q_{\pi_0}\stackrel{I}{\rightarrow}\pi_1\stackrel{E}{\rightarrow}q_{\pi_1}\stackrel{I}{\rightarrow}\pi_2\stackrel{E}{\rightarrow}\cdots\stackrel{I}{\rightarrow}\pi_*\stackrel{E}{\rightarrow}q_*$$ donde $\stackrel{E}{\rightarrow}$ denota la evaluación de la política completa y $\stackrel{I}{\rightarrow}$ denota la mejora  de la política. La evaluación de políticas se realiza exactamente como se describe en la sección anterior. Se experimentan muchos episodios, en los que la función del valor de acción aproximado se aproxima asintóticamente a la función real. Por el momento, supongamos que observamos un número mínimo de episodios y que, además, los episodios se generan con el inicio de la exploración. Sobre estas supocisiones los métodos de Monte Carlo calcularan cada $q_{\pi_{k}}$  para cada $\pi_k$










![Serie de movimientos\label{fig:"sd"}](~/Reinforcement-learning/mc2.png)


## Métodos de Monte Carlo con control sin iniciar exploración

¿Cómo podemos evitar la improbable suposición de que comienza la exploración? La única manera general de asegurar que todas las acciones se seleccionen de forma innata es que el agente continúe seleccionándolas. Existen dos enfoques para garantizarlo, lo que da lugar a lo que llamamos métodos políticos y métodos no políticos. Los métodos políticos intentan evaluar o mejorar la política que se utiliza para tomar decisiones, mientras que los métodos no políticos evalúan o mejoran una política diferente de la que se utiliza para generar los datos. En esta sección mostramos cómo se puede diseñar un método de control de Monte Carlo político que no utilice el supuesto poco realista de que se inicia la exploración. Los métodos no políticos se consideran en la siguiente sección.

En los métodos de control políticos, la política es generalmente blanda, lo que significa que $\pi(a|s) > 0$ para todos los $s \in S$ y todos los $a \in A(s)$, pero gradualmente se va acercando cada vez más a una política determinista óptima. Muchos de los métodos discutidos en el Capítulo 2 proporcionan mecanismos para ello. El método político que presentamos en esta sección utiliza "políticas codiciosas, lo que significa que la mayoría de las veces se elige una acción que tiene un valor de acción estimado máximo, pero con probabilidad " en vez de seleccionan una acción al azar. Es decir, a todas las acciones no deseadas se les da la mínima probabilidad de selección, $\frac{\epsilon}{|A(s)|}$, y el resto de la probabilidad, $1-\epsilon+\frac{\epsilon}{|A(s)|}$, se escoge la acción codiciosa. Las  políticas $\epsilon$-codiciosas son un ejemolo de políticas $\epsilon$-suaves. definidas como políticas para las cuales $\pi(a|s)\geq\frac{\epsilon}{|A(s)|}$, para todos estados y accione, y para algun $\epsilon>0$. Entre políticas $\epsilon$-suaves, las politícas $\epsilon$-codiciosas son, en cierto sentido, las más cercanas a la codiciosa.

La idea general de los Métodos políticos de control de Monte Carlo sigue siendo la de GPI. Al igual que en la ES Monte Carlo, utilizamos métodos de MC de primera visita para estimar la función de valor acción para la política actual. Sin embargo, sin el supuesto de que se inicie la exploración, no podemos simplemente mejorar la política haciéndola codiciosa con respecto a la función de valor actual, porque eso impediría que se siguieran explorando las acciones no deseadas. Afortunadamente, GPI no requiere que la política sea llevada hasta el final a una política codiciosa, sólo que sea movida hacia una política codiciosa. En nuestro método político lo moveremos sólo a una "política codiciosa". Para cualquier política $\epsilon$-blanda,$\pi$, cualquier "política codiciosa con respecto a $q_\pi$ está garantizada que sera mejor o igual a $\pi$. El algoritmo completo se da acontinuación.

* Inicializamos para todo $s\in S$, $a\in A(s)$
    + $Q(s,a)\leftarrow$ arbitrario
    + Retorno$(a,s)\leftarrow$ list vacia
    + $\pi(s|a)\leftarrow$ una arbitraria política $\epsilon$-suave.
* Repetimos por siempre:
    + Generamos un episodio usando $\pi$
    + Para cada par$(s,a)$ aparentemente en el episodio:
        - $G\leftarrow$ retorno que sigue de l primera ocuerrencia de $s,a$
        - Agregamos $G$ al retorno($s,a$)
        - $Q(s,a)\leftarrow$ promedio (retorno($s,a$))
    + Para cada $s$ en el episodio
        - $A^*\leftarrow argmax_a Q(s,a)$
        - Para todo $a\in A(s):$


$$\pi(a|s)= \left\{ \begin{array}{lcc}
             1-\epsilon+\frac{\epsilon}{|A(s)|} &   si & a= A^* \\
             \\ \frac{\epsilon}{|A(s)|} &  si & a\neq A^*\\
             \end{array}
   \right.$$

Que cualquier "política codiciosa" con respecto a $q$ es una mejora sobre cualquier "política blanda" está asegurado por el teorema de la mejora de políticas. Sea $\pi^´$ una política $\epsilon$-codiciosa. Las condiciones del teorema de mejoras de políticas aplican por que para cualquier $s$:$$\begin{align}
 q_{\pi}(s,\pi^´(s)) &= \sum\pi^´(a|s)q_\pi(s,a)\\
&=\frac{\epsilon}{|A(s)|}\sum_a q_\pi(s,a)+(1-\epsilon)max_a q_\pi(s,a) \\
&\geq \frac{\epsilon}{|A(s)|}\sum q_\pi(s,a)+(1-\epsilon)\sum_{a}\frac{\pi(a|s)-\frac{\epsilon}{|A(s)|}}{1-\epsilon}q_\pi(s,a) \\
\end{align}$$ (la suma es un promedio ponderado con pesos no negativos que suman 1, y como tal debe ser menor o igual que el mayor número promediado.)$$\begin{align}
 &= \frac{\epsilon}{|A(s)|}\sum q_\pi(s,a)-\frac{\epsilon}{|A(s)|}\sum q_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)\\
&=v_\pi(s) \\
\end{align}$$ Entonces por el teorema de mejora de políticas, $\pi´\geq\pi$ (i.e., $v_{\pi´}(s)\geq v_{\pi}(s)$, para todo $s$). Ahora demostramos que la igualdad sólo puede mantenerse cuando ambos $\pi´$ y $\pi$  son óptimos entre las políticas $\epsilon$-blandas, es decir, cuando son mejores o iguales que todas las demás "políticas blandas".

Consideremos un nuevo entorno que sea igual al entorno original, excepto con la exigencia de que las políticas se "muevan suavemente dentro" del entorno. El nuevo entorno tiene las mismas acciónes y estados que el original y se comporta de la siguiente manera. Si estamos en el estado s y tomamos la acción a, entonces con probabilidad $1-\epsilon$, el nuevo entorno se comporta exactamente igual que el viejo entorno. Con probabilidad $\epsilon$ repite la acción al azar, con probabilidades iguales, y luego se comporta como el viejo entorno con la nueva acción al azar. Lo mejor que se puede hacer en este nuevo entorno con políticas generales es lo mismo que lo mejor que se puede hacer en el entorno original con políticas "blandas". Sean $\widetilde{v_*}(s)$ y $\widetilde{q_*}(s,a)$ indican las funciones de valor óptimo para el nuevo entorno. Entonces la política $\pi$ es optimo entre las políticas $\epsilon$-blandas si y solo si $v_\pi=\widetilde{v_{*}}$. De la definición de $\widetilde{v_{*}}$ sabemos que es la única solución para: $$\begin{align}\widetilde{v_{*}} &= (1-\epsilon)max_a\widetilde{q_{*}}(s,a)+\frac{\epsilon}{|A(s)|}\sum_a\widetilde{q_{*}}(s,a)\\
&=(1-\epsilon)max_a\sum_{s´,r}p(s´,r|s,a)[r+\gamma \widetilde{v_*}(s)]+ \frac{\epsilon}{|A(s)|}\sum_a\sum_{s´,r}p(s´,r|s,a)[r+\gamma \widetilde{v_*}(s´)]\\
\end{align}$$ Cuando la igualdad se mantiene y la política $\epsilon$-blanda $\pi$ ya no se mejora, entonces:

$$\begin{align}v_\pi(s) &= (1-\epsilon)max_aq_{\pi}(s,a)+\frac{\epsilon}{|A(s)|}\sum_aq_{\pi}(s,a)\\
&=(1-\epsilon)max_a\sum_{s´,r}p(s´,r|s,a)[r+\gamma v_\pi(s´)]+ \frac{\epsilon}{|A(s)|}\sum_a\sum_{s´,r}p(s´,r|s,a)[r+\gamma v_\pi(s´)]\\
\end{align}$$ Sin embargo, esta ecuación es la misma que la anterior, excepto por la sustitución de $v_\pi$ por $\widetilde{v_*}$. Como $\widetilde{v_*}$ es la única solución, debe ocurrir entonces que $v_\pi=\widetilde{v_*}$.

En esencia, hemos mostrado en las últimas páginas que la iteración de políticas funciona para "políticas blandas". Utilizando la noción natural de política codiciosa para las "políticas blandas", se asegura la mejora en cada paso, excepto cuando se ha encontrado la mejor política entre las "políticas blandas". Este análisis es independiente de cómo se determinan las funciones de valor de acción en cada etapa, pero asume que se calculan exactamente. Esto nos lleva aproximadamente al mismo punto que en la sección anterior. Ahora sólo conseguimos la mejor política entre las "políticas $\epsilon$-blandas", pero por otro lado, hemos eliminado el supuesto de los comienzos de la exploración.


## Predicciones no políticas via muestreos de importancia.

Todos los métodos de control del aprendizaje se enfrentan a un dilema: buscan aprender valores de acción condicionados a un comportamiento óptimo posterior, pero necesitan comportarse de manera no óptima para explorar todas las acciones (para encontrar las acciones óptimas). ¿Cómo pueden aprender la política óptima mientras se comportan de acuerdo con una política exploratoria? El enfoque político de la sección anterior es en realidad un compromiso: aprende valores de acción no para la política óptima, sino para una política casi óptima que todavía se está explorando. Un enfoque más directo es utilizar dos políticas, una que se aprende y que se convierte en la política óptima, y otra que es más exploratoria y se utiliza para generar un comportamiento. La política que se está aprendiendo se llama la política objetivo, y la política utilizada para generar comportamiento se llama la política de comportamiento.  En este caso decimos que el aprendizaje es a partir de datos fuera de la política objetivo, y el proceso general se denomina aprendizaje no político.

A lo largo de este libro consideraremos métodos políticos y no políticos.  Los métodos no políticos requieren conceptos y notaciones adicionales, y debido a que los datos se deben a una política diferente, estos son a menudo de mayor varianza y su convergencia es más lenta. Por otro lado, los métodos no políticos son más poderosos y generales. Incluyen métodos políticos como el caso especial en el que las políticas de objetivo y de comportamiento son las mismas. Los métodos no políticos también tienen una gran variedad de usos en  aplicaciones. Por ejemplo, a menudo se pueden aplicar para aprender de los datos generados por un sistema de control convencional que no es de aprendizaje, o de un experto humano. 

En esta sección comenzamos el estudio de los métodos fuera de la política considerando el problema de la predicción, en el que se fijan tanto las políticas de objetivo como las de comportamiento. Esto es, supongamos que deseamos estimar $v_\pi$ o $q_\pi$ pero todo lo que tenemos son episodios que siguen a otra política $b$, donde $b\neq\pi$. En este caso, $\pi$ es la política objetivo, y $b$ es la política del comportamiento, y ambas políticas son consideradas y dadas. Para utilizar los episodios de $b$% para estimar los valores de $\pi$, requerimos que cada acción tomada sobre $\pi$ en b también se tome, al menos ocasionalmente, en b. Esto significa, que si $\pi(a|s)>0$ implica que $b(a|s)>0$. Esto se llama la suposición de la cobertura. Por lo que la cobertura de $b$ debe ser estocastica en los estados donde no es identicamente a $\pi$. La política objetivo $\pi$, por otro lado, puede ser determinista y, de hecho, este es un caso de particular interés en los problemas de control. En control, la política de objetivo es típicamente la política codiciosa determinística con respecto a la estimación actual de la función acción-valor. Esta política se convierte en una política determinista óptima mientras que la política de comportamiento permanece estocástica y más exploratoria, por ejemplo, las políticas $\epsilon$-codiciosas. En esta sección, sin embargo, consideramos los problemas de predicción, en los cuales $\pi$ no cambia y es dada. 

Casi todos los métodos no políticos utilizan muestreos de importancias, una tecnica general para estimar valores esperados bajo una distribución dsdo una muestra proveniente de otra. Aplicamos el muestreo de importancia al aprendizaje no político mediante la ponderación de los resultados de acuerdo con la probabilidad relativa de que sus trayectorias ocurran bajo las políticas de objetivo y de comportamiento, lo que se denomina la proporción de muestreo de importancia. Dado un estado inicial $S_t$, la probabilidad de la trayectoria estado-acción $A_t,S_{t+1},A_{t+1},....,,S_{T}$, ocurriendo bajo una política $\pi$ es: $$\begin{align}
 P(A_t,S_{t+1},A_{t+1},....,,S_{T}|S_t,A_{t:T-1}\sim\pi) & \\
&=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})....p(S_{T}|S_{T-1},A_{T-1}) \\
&=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k) \\
\end{align}$$donde $p$ es la probabilidad de transición de estados. Así, la probabilidad relativa de la trayectoria bajo las políticas objetivo y comportamiento (la relación importancia-muestreo) es $$\rho_{t:T-1}\doteq\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k) }{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_k,A_k) } =\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)}{\prod_{k=t}^{T-1}b(A_k|S_k)}$$

Aunque las probabilidades de trayectoria dependen de las probabilidades de transición del MDP, que son generalmente desconocidas, aparecen idénticamente tanto en el numerador como en el denominador, y por lo tanto se cancelan. La relación de muestreo de la importancia termina dependiendo sólo de las dos políticas y de la secuencia, no del MDP. 

Ahora estamos listos para dar un algoritmo de Monte Carlo que utiliza un grupo de episodios observados siguiendo la política $b$ para estimar $v_\pi(s)$. Aquí es conveniente numerar los pasos de tiempo de una manera que aumente a través de los límites de los episodios. Es decir, si el primer episodio del grupo termina en un estado terminal en el momento 100, entonces el siguiente episodio comienza en el momento $t = 101$. Esto nos permite utilizar números de pasos de tiempo para referirnos a pasos particulares en episodios particulares. En particular, se puede deducir el conjunto de todos los pasos de tiempo en los que el estado $s$ es visitada denotado por $\mathfrak{T}(s)$. Esto es para un método de cada visita; para un método de primera visita, $\mathfrak{T}(s)$,sólo incluiría pasos de tiempo que eran las primeras visitas a $s$ dentro de sus episodios. Ademas sea $T(t)$ la primera  finalización después del tiempo $t$, y $G_t$ denota el retorno desde de $t$ hasta $T(t)$. Entonces $\{G_t\}_{t\in\mathfrak{T}(s)}$ son los retornos que pertenecen al estado s, y $\{\rho_{t:T-1}\}_{t\in\mathfrak{T}(s)}$ son las correspondientes relaciones importancia-muestreo. Para estimar $v_\pi(s)$ simplemente se escalan los retornos por los ratios y se promedian los resultados:$$V(s)\doteq\frac{\sum_{\mathfrak{T}(s)}\rho_{t:T(t)-1}G_t}{|\mathfrak{T}(s)|}$$ Cuando el muestreo de importancia se hace como un promedio simple de esta manera, se le llama muestreo de importancia ordinaria. Una alternativa importante es el muestreo de importancia ponderada, que utiliza un promedio ponderado, denotado como: $$V(s)\doteq\frac{\sum_{\mathfrak{T}(s)}\rho_{t:T(t)-1}G_t}{\sum_{\mathfrak{T}(s)}\rho_{t:T(t)-1}}$$ o cero si el denominador es cero. Para entender estas dos variedades de muestreo de importancia, considere sus estimaciones después de observar una simple retorno. En la estimación de la media ponderada, la radio $\rho_{t:T(t)-1}$ para el retorno simple se cancela en el numerador y denominador, de modo que la estimación es igual al retorno observado independientemente de la relación (suponiendo que la relación es distinta de cero). Dado que esta rentabilidad fue la única observada, se trata de una estimación razonable, pero su esperanza es $v_b(s)$ mas que $v_\pi(s)$ y en este sentido estadístico está sesgada. En contraste el simple promedio es siempre $v_\pi(s)$ en esperanza (es insesgado), pero puede ser extremo. Supongamos que la proporción fuera de diez, lo que indica que la trayectoria observada es diez veces más probable bajo la política objetivo que bajo la política de comportamiento. En este caso, la estimación del muestreo de importancia ordinaria sería diez veces mayor que el retorno observado. Es decir, se alejaría bastante del retorno observado, a pesar de que la trayectoria del episodio se considera muy representativa de la política objetivo.

Formalmente, la diferencia entre los dos tipos de muestreo de importancia se expresa en sus sesgos y varianzas. El estimador de muestreo de importancia ordinaria es imparcial, mientras que el estimador de muestreo de importancia ponderada es sesgado (el sesgo converge asintóticamente a cero). Por otra parte, la varianza del estimador ordinario de muestreo de importancia es en general no acotada porque la varianza de las proporciones puede ser no acotada, mientras que en el estimador ponderado el mayor peso en un solo retorno es uno. De hecho, suponiendo retornos limitados, la varianza del estimador ponderado de muestreo de importancia converge a cero, incluso si la varianza de los ratios en sí es mínima. En la práctica, el estimador ponderado usualmente tiene una varianza dramáticamente menor y es fuertemente preferido. Sin embargo, no abandonaremos totalmente el muestreo de importancia ordinaria ya que es más fácil extenderlo a los métodos aproximados utilizando la aproximación funcional que exploramos en la segunda parte de este libro.

**Ejemeplo: Estimación no política de los valores-estado del Blackjack**

Aplicaremos los mmétodos de muestreo con importancia ordinario y ponderado para estimar los valores de los estados de blackjack de una data no política. Recordemos que una de las ventajas de los métodos de Monte Carlo es que pueden ser utilizados para evaluar un solo estado sin formar estimaciones para otros estados. En este ejemplo, hemos evaluado el estado en el que el dealer está mostrando un dos, la suma de las cartas del jugador es 13, y el jugador tiene un as utilizable (es decir, el jugador tiene un as y un dos, o tres ases equivalentes). Los datos se generaron comenzando en este estado y luego eligiendo hacer hit o stick al azar con la misma probabilidad (la conducta de la política). Los objetivos de la polítican es hacer stick solo cuando la suma es 20 o 21. El valor de este estado bajo la política objetivo es de aproximadamente -0.27726 (esto se calculo usando 100 millones de repeticiones usando la política objetivo y el promedio de sus retornos). Ambos métodos no políticos despues de 1000 repeticiones se acercaron mucho a este valor usando políticas aleatorias. Para asegurarnos de que lo hicieran de forma fiable, realizamos 100 ejecuciones independientes, cada una partiendo de estimaciones de cero y aprendiendo durante 10.000 episodios. La siguiente figura muestra el resultado de la curva de aprendizaje, promediado sobre 100 ejecuciones. El error se aproxima a cero para ambos, pero el muestreo con importancia ponderado tiene un error mucho menor al inicio.
![Serie de movimientos\label{fig:"sd"}](~/Reinforcement-learning/black12.png)

## Implementación incremental

Los métodos de predicción de Monte Carlo pueden implementarse de forma incremental, episodio por episodio, utilizando extensiones de las técnicas descritas en el Capítulo 2. Mientras que en el Capítulo 2 hicimos un promedio de las recompensas, en los métodos de Monte Carlo hicimos un promedio de los retornos. En todos los demás aspectos, pueden utilizarse exactamente los mismos métodos que se utilizan en el capítulo 2 para los métodos políticos de Monte Carlo.  Para los métodos no políticos de Monte Carlo, necesitamos considerar por separado los que utilizan el muestreo de importancia ordinaria y los que utilizan el muestreo de importancia ponderada.

En el muestreo de importancia ordinaria, los retornos son escalados por el radio de muestreo de importancia $\rho_{t:T(t)-1}$, entonces es el simple promedio. Para estos métodos podemos usar nuevamente los métodos incrementales del Capítulo 2, pero usando los retornos escalonados en lugar de las recompensas de ese capítulo. Esto deja el caso de los métodos de no políticos que utilizan el muestreo de importancia ponderada. Aquí tenemos que formar un promedio ponderado de los retornos, y se requiere un algoritmo incremental ligeramente diferente.

Supongamos que tenenmos una sucesión de retornos $G_1,G_2,...,G_{n-1}$, todos iniciando desde el mismo estado y cada uno con su correspondiente peso aleatorio $W_i$ (por ejemplo, $W_i=\rho_{t:T(t)-1}$). Deseamos formar la estimación $$V_n=\frac{\sum^{n-1}_{k=1}W_kG_k}{\sum^{n-1}_{k=1}W_k},\qquad n\geq1$$y$$C_{n+1}\doteq C_n+W_{n+1}$$donde $C_0=0$ (y $V_1$ es arbitrario y ni necesita ser especificado). Acontinuación se presenta el algoritmo incremental completo, episodio por episodio, para la evaluación de la política de Monte Carlo. El algoritmo es nominalmente para el caso no político, usando un muestreo de importancia ponderada, pero se aplica también al caso no-político con sólo elegir las políticas de objetivo y de comportamiento como las mismas. La aproximación $Q$ converge a $q\pi$ (para todos los pares de estado-acción encontrados) mientras que las acciones se seleccionan de acuerdo con una política potencialmente diferente, $b$.

**Predicción no política de Monte Carlo**

* Entrada: Una política arbitraria $\pi$

* Iniciamos, para todo $s\in S, \quad a\in A(s):$
    - $Q(s,a)\leftarrow \textrm{ arbitario}$
    - $C(s,a)\leftarrow \textrm{ arbitario}$
    
* Repetir por siempre:
    - $b\leftarrow \textrm{ cualquier política que cubra a } \pi$
    - Generamos un episodio usando $b: \quad S_0,A_0,R_1,....,S_{T-1},R_T,S_T$
    - $G\leftarrow 0$
    - $W \leftarrow 1$
    - Para $t= T-1,T-2...., \textrm{ hasta }0$
        + $G\leftarrow \gamma G+R_{t+1}
        + $C(S_t,A_t)\leftarrow C(S_t,A_t)+W$
        + $Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\frac{W}{C(S_t,A_t)}[G-Q(S_t,A_t)]$
        + $\pi(S_t)\leftarrow argmax_a Q(S_t,a)$
        + Si $A_t\neq \pi(S_t)$ entinces salir del ciclo
        + $W \leftarrow W\frac{1}{b(A_t|S_t)}$
  
  
## Monte Carlo no político con control
  
Ahora estamos listos para presentar un ejemplo de la segunda clase de métodos de control del aprendizaje que consideramos en este libro: métodos no políticos. Recordemos que la característica distintiva de los métodos políticos es que estiman el valor de una política mientras la utilizan para el control. En los métodos no políticos, estas dos funciones están separadas. La política utilizada para generar comportamiento, llamada política de comportamiento, puede, de hecho, no estar relacionada con la política que se evalúa y mejora, llamada política objetivo. Una ventaja de esta separación es que la política objetivo puede ser determinista (por ejemplo, codiciosa), mientras que la política de comportamiento puede continuar muestreando todas las acciones posibles.

Los métodos de control no políticos de Monte Carlo utilizan una de las técnicas presentadas en las dos secciones anteriores. Ellos siguen la política de comportamiento mientras aprenden y mejoran la política objetivo. Estas técnicas requieren que la política de comportamiento tenga una probabilidad distinta de cero de seleccionar todas las acciones que podrían ser seleccionadas por la política objetivo (cobertura). Para explorar todas las posibilidades, requerimos que la política de conducta sea suave (es decir, que seleccione todas las acciones en todos los estados con una probabilidad distinta a cero).

El proximo recuadro muestra un método de control de Monte Carlo basado en el GPI y el muestreo de importancia ponderada para estimar $\pi_*$ y $q_*$. La política onjetivo $\pi\approx\pi_*$ es la política ambiciosa con respecto a $Q$, la cual es un estimador de $q_\pi$. La política de comportamiento $b$ puede ser cualquier cosa, pero para asegurar la convergencia de la política óptima, se debe obtener un número mínimo de retornos para cada par de estados y acciones. Esto se puede asegurar eligiendo $b$ para que sea "$\epsilon$-blanda". La política $\pi$ converge hacia la óptima en todos los estados encontrados, aunque las acciones se seleccionan de acuerdo con una política blanda $b$, que puede cambiar entre episodios o incluso dentro de ellos.

Un problema potencial es que este método sólo aprende de las colas de los episodios, cuando todas las acciones restantes del episodio son codiciosas. Si las acciones no deseadas son comunes, entonces el aprendizaje será lento, particularmente para los estados que aparecen en las primeras porciones de los episodios largos. Potencialmente, esto podría retrasar enormemente el aprendizaje. No ha habido suficiente experiencia con los métodos de Monte Carlo para evaluar la gravedad de este problema. Si es s

* Inicializamos para todo $s\in S \quad a\in A(s):$
    + $Q(s,a) \leftarrow\textrm{arbitrario}$
    + $C(s,a)\leftarrow 0$
    + $\pi(s)\leftarrow argmax_a Q(S_t,a)$
* Repetir por siempre
    + $b \leftarrow\textrm{cualquier política suave}$
    + $\textrm{Generamos un episodio usando } b$
        - $S_0,A_0,R_1,...,S_{T-1},A_{T-1},R_{T},S_{T}$
    + $G \leftarrow 0$
    + $W\leftarrow 1$
    + $\textrm{Para } t=T-1,T-2,...,\textrm{hasta }0$
        - $G \leftarrow \gamma G+R_{t+1}$
        - $C(S_t,A_t)\leftarrow C(S_t,A_t)+W$
        - $Q(S_t,A_t) \leftarrow Q(S_t,A_t)+ \frac{w}{C(S_t,A_t)}[G-Q(S_t,A_t)]$
        - $\pi(S_t)\leftarrow argmax_aQ(S_t,a)$
        - Si $A_t\neq\pi(S_t)\textrm{ entonces salir del ciclo}$
        - $W\leftarrow W\frac{1}{b(A_t|S_t)}$
    


  
  
  
  
  
  
  
    





























