# Bootstrapping en $n$-pasos

En este capítulo unificamos los métodos de Monte Carlo (MC) y de dierencia temporal de un solo paso (TD). métodos presentados en los dos capítulos anteriores. Ni los métodos MC ni los métodos TD de un solo paso son siempre el mejor. En este capítulo presentamos métodos de TD en n pasos que generalizan ambos métodos de manera que uno puede cambiar de uno a otro sin problemas según sea necesario para satisfacer las demandas de una tarea en particular. Metodos en  n-paso abarcan un espectro con métodos MC en un extremo y métodos TD de un paso en el otro. Los mejores métodos son a menudo intermedios entre los dos extremos.

Otra forma de ver los beneficios de los métodos de n-paso es que nos liberan de la maldición del paso del tiempo. Con los métodos de TD de un solo paso, el mismo paso de tiempo determina con qué frecuencia se puede cambiar la acción y el intervalo de tiempo durante el cual se realiza el bootstrapping. En muchas aplicaciones uno quiere ser capaz de actualizar la acción muy rápidamente para tener en cuenta cualquier cosa que haya cambiado, pero el bootstrapping funciona mejor si es durante un periodo de tiempo en el que se ha producido un cambio de estado significativo y reconocible. Con los métodos de TD de un solo paso, estos intervalos de tiempo son los mismos, por lo que se debe llegar a un compromiso. Los métodos de n pasos permiten que el bootstrapping ocurra en múltiples pasos, liberándonos de la tiranía de un solo paso de tiempo.

La idea de los métodos de $n$ pasos se utiliza generalmente como una introducción a la idea algorítmica de las trazas de elegibilidad (Capítulo 12), que permite el bootstrapping a lo largo de múltiples intervalos de tiempo simultáneamente. Aquí, en cambio, consideramos la idea de bootstrapping de $n$ pasos por sí sola, posponiendo el tratamiento de los mecanismos de trazado de eligibilidad hasta más adelante. Esto nos permite separar mejor los temas, tratando tantos de ellos como sea posible en la configuración más simple de $n$ pasos.

Como de costumbre, primero consideramos el problema de la predicción y luego el problema del control. Es decir, primero consideramos cómo los métodos de $n$ pasos pueden ayudar a predecir los retornos como una función del estado para una política fija (es decir, al estimar $v_\pi$). Luego ampliamos las ideas a valores de acción y métodos de control.

## Predicción de TD en $n$ pasos

¿Cuál es el espacio de métodos entre los métodos de Monte Carlo y TD? Considere la posibilidad de estimar $v_\pi$ a partir de episodios de la muestra generados mediante $\pi$. Los métodos de Monte Carlo realizan una actualización para cada estado basada en la secuencia completa de recompensas observadas desde ese estado hasta el final del episodio. La actualización de los métodos de TD de un solo paso, por otro lado, se basa en la siguiente recompensa, arrancando desde el valor del estado un paso más tarde como un proxy para las recompensas restantes. Un tipo de método intermedio, entonces, realizaría una actualización basada en un número intermedio de recompensas: más de una, pero menos de todas hasta la terminación. Por ejemplo, una actualización de dos pasos se basaría en las dos primeras recompensas y el valor estimado del estado dos pasos después. Del mismo modo, podríamos tener actualizaciones de tres pasos, actualizaciones de cuatro pasos, y así sucesivamente. La siguiente figura muestra los diagramas de respaldo del espectro de actualizaciones de $n$-pasos para $v_\pi$, con la actualización de TD de un paso a la izquierda y la actualización de Monte Carlo hasta el final a la derecha.

![\label{fig:"sd"}](~/Reinforcement-learning/61.png)
Los métodos que utilizan actualizaciones de $n$ pasos siguen siendo métodos de TD porque todavía cambian una estimación anterior basada en cómo difiere de una estimación posterior. Ahora bien, la estimación posterior no es un paso más tarde, sino $n$ pasos más tarde. Los métodos en los que la diferencia temporal se extiende sobre $n$ pasos se denominan métodos de TD de $n$ pasos. Todos los métodos de TD introducidos en el capítulo anterior utilizaban actualizaciones de un solo paso, por lo que los llamamos métodos de TD de un solo paso.

De manera más formal, considere la actualización del valor estimado del estado $S_t$ como resultado de la secuencia de recompensas del estado, $S_t,R_{t+1},S_{t+1},R_{t+2},...,R_T,S_T$ (omitiendo las acciones). Sabemos que en Monte Carlo se actualiza la estimación de $v_\pi(S_t)$ en la dirección del retorno completo: $$G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}$$ donde T es el último paso del episodio. Llamemos a esta cantidad el objetivo de la actualización. Mientras que en las actualizaciones de Monte Carlo el objetivo es el retorno, en las actualizaciones de un paso el objetivo es la primera recompensa más el valor estimado descontado del siguiente estado, que llamamos el retorno de un paso:$$G_{t:t+1}=R_{t+1}+\gamma V_t(S_{t+1})$$ donde $V_t:S\rightarrow \mathbb{R}$ es la estimación en el tiempo $t$ de $v_\pi$. Los subíndices de $G_{t:t+1}$ indican que se trata de un retorno truncado para el tiempo $t$ utilizando recompensas hasta el tiempo $t + 1$, con la estimación descontada $\gamma V_t(S_{t+1})$ tomada en lugar de los terminos $\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}$ del retorno completo, como se discutió en el capítulo anterior. Nuestro punto ahora es que esta idea tiene tanto sentido después de dos pasos como después de uno. El objetivo de una actualización en dos pasos es el retorno en dos pasos: $$G_{t:t+2}=R_{t+1}+\gamma V_t(S_{t+1})+\gamma^2 V_t(S_{t+2})$$ donde $\gamma^2 V_t(S_{t+2})$ corrige la ausencia de los términos $\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}$. Del mismo modo, el objetivo de una actualización arbitraria de $n$ pasos es el retorno de $n$ pasos: $$G_{t:t+2}=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{n-1} R_{n}+\gamma^n V_{t+n-1}(S_{t+n})$$ para todo $n,t$, tal que $n\ge1$ y $0\le t<T-n$. Todas los retornos de $n$ pasos pueden considerarse aproximaciones al retorno completo, truncadas después de $n$ pasos y luego corregidas para tener en cuenta los plazos restantes que faltan, de la siguiente manera por $V_{t+n-1}(S_{t+n})$. Si $t+n\ge T$ (si el retorno de $n$ pasos se extiende hasta o más allá del termino) entonces todos los términos que faltan se toman como cero, y el retorno de $n$ pasos se define como igual al retorno total ordinario ($G_{t:t+n}= G_t$ si $t+n \ge T$).

Note que el retorno en $n$ pasos para $n>1$, implican recompensas futuras y estados que no están disponibles en el momento de la transición de $t$ a $t + 1$. Ningún algoritmo real puede usar el retorno de $n$ pasos hasta que haya visto $R_{t+n}$ y calculado $V_{t+n-1}$. La primera vez que están disponibles es $t+n$. El algoritmo natural de aprendizaje de valor estado para el uso de los retornos de $n$ pasos es así:$$V_{t+n}(S_t)=V_{t+n-1}(S_t)+\alpha[G_{t:t+n}-V_{t+n-1}(S_t)],\quad 0\le t<T$$mientras que los valores de todos los demás estados permanecen inalterados: $V_{t+n}(s) = V_{t+n-1}(s)$ para todo $s\neq S_t$. Llamamos a este algoritmo TD de $n$ pasos. Tengamos en cuenta que no se realizan cambios en absoluto durante los primeros $n-1$ pasos de cada episodio. Para compensar esto, se hace un número igual de actualizaciones adicionales al final del episodio, después de la terminación y antes de comenzar el siguiente episodio. 

**TD en $n$ pasos para estimar $V\approx v_\pi$**

* Iniciamos $V(s)$ arbitrariamente, $s\in S$.
* Parametros: tamaño del paso $\alpha\in(0,1]$, un entero positivo $n$ 
* Todas las medidas de almacenamiento y acceso (Para $S_t$ y $R_t$) pueden tomar indices sobre $n$

* Repetimos (para cada episodio):
    - Iniciamos y almcenamos $S_0\neq$ terminal.
    - $T\leftarrow\infty$
    - Para $t=0,1,2,...$
        + Si $t<T$, entonces:
            * Tomamos una acción de acuerdo a $\pi(\cdot|S_t)$
            * Observamos y almacenamos la siguiente recompensa comno $R_{t+1}$ y el proximo estado como $S_{t+1}$
            * Si $S_{t+1}$ es terminal, entonces $T\leftarrow t+1$
        + $\tau\leftarrow t-n-1$ ($\tau$ es el tiempo cuya estimación del estado se está actualizando).
        + Si $\tau\ge 0$
            - $G \leftarrow \sum_{i=\tau+1}^{min(\tau+n,T)}\gamma^{i-\tau-1}R_i$
            - Si $\tau+n<T$, entonces: $G\leftarrow G+\gamma^nV(S_{\tau+n})$
            - $V(S_\tau)\leftarrow V(S_\tau)+\alpha[G-V(S_{\tau})]$
        + Hasta que $\tau=T-1$
        
        
        
        
        
        
        
        
        
        
        
        




![\label{fig:"sd"}](~/Reinforcement-learning/62.png)









