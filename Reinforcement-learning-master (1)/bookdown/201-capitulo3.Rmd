# Programación dinámica

El termino de programación dínamica (DP) se refiere a la colección de algoritmos que pueden ser usados para calcular polítics óptimas dado un entorno que sigue un MDP perfecto. Algoritmps clásicos de DP son de utilidas limitada en Reinforcement Learning, debido a que asumen la perfeccion del modelo y tienen un alto costo computacional, pero son importante en el ámbito teórico. DP nos provee de factores fundamentales para el entendimiento de los métodos presentados en el resto de este libro. En efecto,todos estos métodos pueden ser vistos como intentos de lograr el mismo efecto que DP, sólo que con menos computación y sin asumir un modelo perfecto del entorno.

Supondremos que estamos en presencia de MDP finito, es decir el conjunto de estados, acciones y recompensas, $\textit{S}$ ,$\textit{A}$, $\textit{R}$ son finitos, y que el comportamiento dinámico esta bien definido por $p(s´,r|s,a)$, para todo $s\in \textit{S}$, $a\in \textit{A}(s)$, $r\in \textit{R}$ y  $s´\in \textit{S}^+$, donde $\textit{S^+}$ es el conjunto de estados no terminales, en el contexto de las tareas episódicas. Aunque las ideas de DP pueden aplicarse a problemas con espacios de estados y acciónes continuas, las soluciones exactas sólo son posibles en casos especiales. Una forma común de obtener soluciones aproximadas para tareas con estados y acciones continuas es cuantificar el estado y los espacios de acción y luego aplicar los métodos de DP que se usan en estados finitos.

La idea clave en DP, y en Reinforcement Learning generalmente, es el uso de funciones de valor para organizar y estructurar  la busqueda de buenas políticas. En este capitulo mostraremos como DP puede ser usado para computar las funciones de valor definidas en el capítulo anterior. Nosotros podemos obtener políticas óptimas una vez hemos encontrdo la funcion de valor óptima, $v_*$ o $q_*$, las cuales satisfacen la ecuación de Bellman. Veremos, que los algoritmos de DP son obtenidos con manipulaciones de la ecuación de Bellman, es decir, en reglas de actualización para mejorar las aproximaciones de las funciones de valor deseadas.

## Políticas evaluadas (Predicción)

Primero consideremos como computar funciones de valor $v_\pi$ para una arbitraria política $\pi$. Esto es llamado políticas evaluadas en la literatura de DP. Ademas nos referiremos a esto como un problema de predicción. Recordemos del capítulo 3 que, para todo $s\in\textit{S}$ $$\begin{align}
 v_{\pi}(s) &= \mathbb{E}_\pi[G_t|S_t=s]\\
 &=  \mathbb{E}_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
 &=\mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \\
&=\sum_a\pi(a|s)\sum_{s´,r}p(s´,r|s,a)\big{[}r+\gamma v_\pi(s´)\big{]} \\
\end{align}$$

Donde $\pi(a|s)$ es la probabilidad de tomar la acción $a$ en el estado $s$ bajo la política $\pi$, la existencia de $v_\pi$ esta garantizada mientras $\gamma$ nos garantiza la convergencia.

Si la dinámica del entorno es completamente conocida, la solución  al sistema anterior depende del cardinal de los estados, cabe acotar que es un sistema lineal. Su solución es directa, aunque tediosa, computacionalmente. Para nuestros propositos, la métodos de solución interactiva son mas deseados. Consideremos una sucesión de funciones de valores $v_0,v_1,...,$ cada una mapeando de $\textit{S^+}$ a $\mathbb{R}$. La aproximación inicial es, $v_0$, es escogida arbitrariamente (excepto para los estados terminales, en esos debe ser 0), y cada aproximación sucesiva es alcanzada usando la ecuación de Bellman como dicta la siguiente regla de aproximación:$$\begin{align}
 v_{k+1}(s) &=\mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \\
&=\sum_a\pi(a|s)\sum_{s´,r}p(s´,r|s,a)\big{[}r+\gamma v_\pi(s´)\big{]} \\
\end{align}$$ para todo $s\in\textit{S}$, si $v_k=v_\pi$ hace que la regla de actualización sea constante. Puede mostrarse que la sucesión $\{v_k\}$ converge a $v_\pi$, sobre algunas condiciones que garantizen la existencia de $v_\pi$. Este algoritmo se llama evaluación iterativa de políticas.

Para proceder a cada sucesiva aproximación, $v_{k+1}$ desde $v_k$, este algoritmo la misma operación a cada estado $s$: esto reemplaza el anterior estimación de $s$ con el nuevo valor obtenido a partir de la operacion que involucra al intiguo valor y las recompensas esperdas a un solo paso, a lo largo de todas las transiciones a un solo paso posibles. Llamamos a todos estos tipos de operaciones una actualizacion estimada. Cada iteracion del algoritmo evalua la aproximación de cada estado una vez se produce la nueva estimación del valor de la función $v_{k+1}$. Existen ademas diferentes tipos de actualizaciones esperadas, dependiendo de si un estado o un par acción-estado estan siendo actualizados. y dependiendo de la forma precisa en que se combinen los valores estimados de los estados sucesores. Todas las actuañizaciones en DP son llamadas actualizaciones esperadas debido a que involucran una esperanza sobre todos los posibles proximos estados mas que una simple muestra del estado siguiente. Aunque teóricamente el algoritmo converge en el infinito, en la vida real necesitamos algun criterio, en genreal se calcula el $max_{s\in S}|v_{k+1}(s)-v_{k}(s)|$ despues de cada paso hasta alcanzar un valor propuesto.  Una completa versión del algoritmo es presentado acontinuación

* Iniciamos con una política $\pi$ a ser evaluada
* Inicializamos una matriz $V(s)=0$ para todo $s\in S^+$
* Repetimos
  + $\Delta  \leftarrow  0$
  + Para cada $s\in S$:
    - $v \leftarrow V(s)$
    - $V(s)  \leftarrow\sum_a\pi(a|s)\sum_{s´,r}p(s´,r|s,a)[r+\gamma V(s´)]$
    - $\Delta  \leftarrow max(\Delta,|v-V(s)|)$

* Hasta que $\Delta < \theta$ (un numero positivo pequeño )

* Como salida $V\approx v_\pi$

**Ejemplo:** Consideremos la cuadrícula 4x4 mostrada acontunuación

![\label{fig:"sd7"}](~/Reinforcement-learning/grid3.png)


Los estados no terminales son $S=\{1,2,3,...,14\}$, hay 4 acciones posibles por estado $A=\{$arriba, abajo, derecha, izquierda$\}$, las cuales deterministicamente causan estados de transición, excepto las acciones que sacarían al agente de la cuadrícula, de hecho dejan el estado sin cambios. Esto es por ejemplo $p(6,-1|5,$derecha$)=1$, $p(7,-1|7,$derecha$)=1$, $p(10,r|5,$derecha$)=1$, para todo $r\in R$. Esto es una tarea episódica. Las recompensas son -1 para todas las transiciones hasta encontrar el estado terminal (los cuales estan sombreados en la cuadricula). La función de recompensa esperada es $r(s,a,s´)=-1$ para todo par de estados $s , s'$ y acción $a$. Supondremos que el agente sigue una política a leatoria equiprobable. La columna izquierda de la proxima imagen representa el de $\{v_k\}$ calcula con el algoritmo de aproximación.


![Convergencia del algoritmo en la cuadricula. La columna de la derecha es la sucesion de políticas codiciosas correspondientes a las funciones de valores estimadas\label{fig:"sd6"}](~/Reinforcement-learning/grid2.png)


## Mejora de las políticas


Una razon para calcular el valor de las función de valor para una política es encontrar una mejor. Supongamos que hemos determinado el valor de la función $v_\pi$ para una política determinada $\pi$. Para algun estado $s$ nos gustaria saber si o no deberiamos cambiar la política para la escogencia de una acción $a\neq\pi(s)$. Sabemos lo bueno que es seguir la política actual desde el principio desde un estado $s$ (esto es $v_\pi(s)$), pero ¿ seria mejor o peor cambiar a una nueva política?. Una menera de responder esta pregunta es considerar $a$ y $s$ y luego seguir la política existente $\pi$. El valor de esta forma de actuar es: 
$$\begin{align}
 q_{\pi}(s,a) &=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a] \\
&=\sum_{s´,r}p(s´,r|s,a)\big{[}r+\gamma v_\pi(s´)\big{]} \\
\end{align}$$

El criterio clave es verificar si esta cantidad es mayor o menor que $v_\pi(s)$. Esto significa una vez en el estado $s$ preguntarse si es mejor seleccionar la acción $a$ y luego seguir la política $\pi$ que simplemente seguir la politica $\pi$ desde el principio. Si fuera mejor seleccionar la acción $a$ en vez de seguir la política desde el principio, entonces habremos encontrado una nueva política que sería mejor. La realidad de esta afirmación es un caso especial del popular teorema de mejoras de políticas. Sean $\pi$ y $\pi'$ un par de políticas cualesquiera tal que para todo $s\in S$: $$q_\pi(s,\pi'(s))\geq v_\pi(s)$$ entonces la política $\pi_*$ debe ser igual de buena o mejor que $\pi$. Esto a ademas implica que se debe obtener un mayor o igual valor esperado para todo los estados $s\in S$: $$v_{\pi'}(s)\geq v_{\pi}(s)$$

Más aun si la desigualdad es estricta para todo los estados, entonces, entonces tambien es estricta para todos los para acción-estado. La idea detras de la demostración del teorema de mejoras de políticas es facil de entender:

$$\begin{align}
 v_{\pi}(s) &\leq q_{\pi}(s,\pi'(s))\\
&=\mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a] \\
&=\mathbb{E}_\pi'[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
&\leq \mathbb{E}_\pi'[R_{t+1}+\gamma q_\pi(S_{t+1},\pi(S_{t+1}))|S_t=s]\\
&= \mathbb{E}_\pi'[R_{t+1}+\gamma \mathbb{E}[R_{t+2}+\gamma v_\pi(S_{t+2})]|S_t=s]\\
&= \mathbb{E}_\pi'[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s]\\
&\leq \mathbb{E}_\pi'[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3v_\pi(S_{t+3})|S_t=s]\\
&\vdots\\
&\leq \mathbb{E}_\pi'[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3R_{t+4}+...|S_t=s]\\
&=v_{\pi'}(s)
\end{align}$$

Hasta ahora hemos visto como dado una política y su función de valor, podemos facilmente evaluar un cambio en la poolítica en un simple estado dado una acción particular. Es una extensión natural para considerar los cambios en todos los estados y en todas las acciones posibles, seleccionando en cada estado la acción que mejor se presenta en función de $q_\pi(s,a)$. En otras palabras, consideraremos la nueva política codiciosa, $\pi'$ dado por: $$\begin{align}
 \pi'(s) &= arg max_a q_{\pi}(s,a)\\
&=arg max_a \mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a] \\
&=\arg max_a \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]\\
\end{align}$$

Pero esto lo mismo que la ecuación de optimalidad de Bellman, así, v_{\pi'} debe ser $v_*$ y por lo tanto la política es la óptima. Así la forma de mejorar políticas presentada en esta sección nos da una manera de mejorar políticas, siempre y cuando esta ya sea óptima.



## Iteración de políticas

Una vez una política, $\pi$, ha sido mejorada usando $v_\pi$ obteniendo, $\pi'$ nosotros podemos computar $v_{\pi'}$, mejorarla de nuevo y obtener una mejor política $\pi^{''}$, así podemos obtener una sucesión de políticas mejoradas y sus funciones de valores: $$\pi_0\stackrel{E}{\rightarrow}v_{\pi_0}\stackrel{I}{\rightarrow}\pi_1\stackrel{E}{\rightarrow}v_{\pi_1}\stackrel{I}{\rightarrow}\pi_2\stackrel{E}{\rightarrow}\cdots\stackrel{I}{\rightarrow}\pi_*\stackrel{E}{\rightarrow}v_*$$  donde $\stackrel{E}{\rightarrow}$ denota la evaluación de la política y $\stackrel{I}{\rightarrow}$ la mejora de la política. Cada política esta garantizada con una estricta mejoras sobre las anteriores (a menos que ya sea la óptima). Como estamos trabajando sobre un MDP finito debe haber un numero finito, por lo cual este proceso debe converger a la política óptima y a la función de valor óptima.

Esta forma d ehallar políticas optimas es conocido como iteración de políticas. El algoritmo es presentado acontinuación,  note que cada política evaluada, en si misma lleva un cómputo iterativo, este inicia con el cálculo de la función de valor de la política previa. Esto típicamente resulta en un gran aumento de la velocidad de convergencia en la evaluación de las políticas 



* Inicializamos:
$V(s)\in\mathbb{R}$ y $\pi(s)\in A(s)$ arbitrario para todo $s\in S$

* Evaluamos la política:
  + Repetimos:
    - $\Delta \leftarrow 0$ 
    - Para cada $s\in S$
    - $v \leftarrow V(s)$
    - $V(s)\leftarrow \sum_{s',r}p(s',r|s,\pi(s))[r+\gamma V(s')]$
    - $\Delta \leftarrow max(\Delta, |v-V(s)|)$
    - Hasta $\Delta<\theta$ (un numero positivo pequeño)

* Mejoramos la política:
  + Política-Estable $\leftarrow$ true
  + Para cada $s\in S$:
    - Vieja-acción $\leftarrow\pi(s)$
    - Ahora $\pi$ $\leftarrow$ $argmax_a\sum_{s',r}$ $p(s',r|s,a)[r+\gamma V(s')]$
    - Si vieja-acción $\neq\pi(s)$, entonces politica-estable $\leftarrow$ false
    - Si la política-estable, entonces parar y retornar $V \approx v_{+}$  y $\pi\approx \pi_+$: sino ir de nuevo a la mejora de política.
    
    
## Iteración de valores 

Una desventaja de la iteración de políticas es cada iteración involucra  la evaluación de una política, la cual puede ser un cálculo prolongado a traves de la cantidad de estados que existan. Ahora si la iteración se lleva a cabo, la convergencia ocurre en el infinito. Pero, ¿Debemos esperar hasta el infinito, para la convergencia exacta, o podemos parar en cierto momento?.

En efecto, la evaluación de las politicas puede ser truncada de varias maneras sin perder las garantias de convergencia. Un importante caso especial es cuando la evaluacion es parada despues de un solo barrido (una actualización de cada estado). Este algoritmo es llamado es valores iterados. Puede ser escrito como una operación de actualización particularmente simple que combina la mejora de la política y los pasos truncados de la evaluación.$$v_{k+1}(s) = max_a\mathbb{E}[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a]=max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_k(s')]$$ para todo $s\in S$. Para un arbitrario $v_0$, la sucesión $\{v_k\}$ puede mostrar que converge a $v_*$  bajo las mismas condiciones que garantizan  la existencia de $v_*$.

Otra forma de entender la iteración de valores es hacer referencia a la ecuación de optimalidad Bellman. Notemos que que el valor iterado se obtiene simplemente como una regla de actualizacion sobre la ecuación. Ademas la actualización por valor iterado coincide la actualizacion de evaluación de políticas excepto que esta requiere la maximización sobre toda las acciones.

Finalmente, consideremos cómo termina la iteración de valores. Como la evaluación de políticas, valores iterados formalmente requieren un número infinito de iteraciones para garantizar la convergencia exacta a $v_*$. En la practica esto no es necesario, en general denetemos las iteraciones despues que las cambios son indignificantes según algún criterio. Acontinuación mostramos el algoritmo:

* Iniciamos una matriz arbitraria $V$ (por ejemplo $V(s)=0$ para todo $s^S{+}$

* Repetimos
  + $\Delta \leftarrow 0$
  + Para cada $s\in S$:
    - $v \leftarrow V(s)$
    - $V(s) \leftarrow max_a\sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]
    - $\Delta \leftarrow max(\Delta,|v-V(s)|)$
* Hasta que $\Delta < \theta$ (un numero positivo pequeño)

* Salida: Una determinada política , $\pi \approx \pi_*$, tal que:
  + $\pi(s)=argmax_a\sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$


El valor iterado efectivamente combina, en cada barrido, un barrido de la evaluación de la política y un barrido de mejora de políticas. A menudo se logra una convergencia más rápida mediante la interposición de múltiples barridos de evaluación de políticas entre cada barrido de mejora de políticas. En genenral, toda la clase de algoritmos de iteración de políticas truncadas puede considerarse como secuencias de barridos, algunos de los cuales utilizan actualizaciones de evaluación de políticas y otros utilizan actualizaciones de iteración de valores.


**Ejemplo: El problema del jugador**: Un jugador tiene la opotunidad de hacer apuestas sobre el resultado de una sucesion de lanzamientos de una moneda. Si la moneda sale cara, el jugador gana lo mismo  que aposto si sale cruz pierde todo. El juego se acaba cuando el jugador llega a 100 dolares o se queda sin dinero. En cada lanzamiento el jugador debe decidir que cantidad de dinero apuesta, solo se le permite hacer apuesta que representen numeros enteres. Este problema puede ser planteado como un problema no discontiunuo y episodico, con MDP finito. Los estados son el capital del jugador, $s\in \{1,2,3,...,98,99\}$ y las acciones son las apuestas, $a\in \{0,1,2,....,min(s,100-s)\}$, la recompensa es cero en todas las transiciones exepto cuando alcanza la meta, en este caso es 1. La función de valor estado da la probabilidad de ganar en todo estado. Una política es una funcion del nivel de capital a las apuestas. La política óptima maximiza la probabilidad de alcanzar la meta. Sea $p_h$ la probabilidad de obtener una cara. Si $p_h$ es conocido, entonces se conoce todo el problema y se puede resolver, por ejemplo, mediante iteración de valores. La siguiente figura muestre el cambio en la funcion de valor de acuerdo a sucesivos barridos de valores iterados, y la política final encontrada, para el caso $p_h=0.4$. Esta política es óptima pero no única. En efecto, existe una familia de políticas óptimas, toda correspondientes a valores comunes en la función de valor.

![\label{fig:"sd62"}](~/Reinforcement-learning/gambler.png)


## Programación dinámica asincrónica.

Una gran desventaja de los métodos de DP que hemos visto es que involucran un intesivo barrido por cada actualización, es decir, necesita actualizar todos los estados existente, esto puede hacer preacticamente imposible la tarea. Por ejemplo el juego de Backgammon posee alrededor de $10^{20}$, si inclusive pudieramos barrer un millos de estados por segundo, la tarea de una solo actualización nos tomaria 1000 años en ser completada.

PD asincrónica son algoritmos que no requieren un barrido exhaustivo de los estados, estos algoritmos actualizan los valores de los estados en cualquier orden. Utilizando los valores de otros estados que esten disponibles. Los valores de unos estados pueden ser actualizados varias veces, mientras que otros una sola vez. Sin embargo para obtener la correcta convengencia, debemso actualizar todos los estados frecuentemente. DP asincrónica permite mucha flexibilidaden la seleccion de los estados a actualizar.

Por supuesto, evitar los barridos no significa necesariamente que podamos salirnos con la nuestra. Sólo significa que un algoritmo no tiene que quedar atrapado en un barrido desesperadamente largo antes de que pueda progresar en la mejora de una política. Podemos intentar aprovechar esta flexibilidad seleccionando los estados en los que aplicamos las actualizaciones para mejorar el ritmo de progreso del algoritmo. Podemos intentar aprovechar esta flexibilidad seleccionando los estados en los que aplicamos las actualizaciones para mejorar el ritmo de progreso del algoritmo. Podemos intentar ordenar las actualizaciones para que la información de valor se propague de estado a estado de manera eficiente. Es posible que algunos estados no necesiten que sus valores se actualicen con tanta frecuencia como otros .Incluso podríamos tratar de omitir la actualización de algunos estados por completo si no son relevantes para un comportamiento óptimo.

Los algoritmos asíncronos también facilitan la mezcla de la computación con la interacción en tiempo real. Para resolver un problema de MDP, podemos ejecutar un algoritmo iterativo de DP al mismo tiempo que un agente está experimentando con el MDP. La experiencia del agente puede utilizarse para determinar los estados en los que el algoritmo de DP aplica sus actualizaciones. Al mismo tiempo, la información más reciente sobre valores y políticas del algoritmo DP puede guiar la toma de decisiones del agente. Por ejemplo, podemos aplicar actualizaciones a los estados a medida que el agente los visita. Esto permite enfocar las actualizaciones del algoritmo DP en las partes del conjunto de estados que son más relevantes para el agente. Este tipo de enfoque es un tema repetido en el Reinforcement Learning.

## Iteración generalizada de políticas

La iteración de políticas consiste en dos procesos simultáneos e interactivos, uno que hace que la función de valor sea coherente con la política actual (evaluación de políticas), y el otro que hace que la política sea codiciosa con respecto a la función de valor actual (mejora de políticas). En la iteración de políticas, estos dos procesos se alternan, cada uno completandose antes de que el otro comience, pero esto no es realmente necesario. En la iteración del valor, por ejemplo, sólo se realiza una única iteración de la evaluación de políticas entre cada mejora de las mismas. En los métodos DP asíncronos, los procesos de evaluación y mejora se intercalan a un grado uniforme. En algunos casos, un solo estado se actualiza en un paso, antes de volver al otro. Mientras ambos procesos continúen actualizando todos los estados, el resultado final es típicamente la misma convergencia hacia la función de valor óptimo y una política óptima.

Utilizamos el término iteración generalizada de políticas (GPI) para referirnos a la idea general de permitir que la evaluación de políticas y los procesos de mejora de políticas interactúen, independientemente de la granularidad y otros detalles de los dos procesos. Casi todos los métodos de Reinforcement Learning son descritos de buena forma usando GPI. Es decir, todas tienen políticas identificables y funciones de valor, con la política siempre siendo mejorada con respecto a la función de valor y la función de valor siempre siendo dirigida hacia la función de valor para la política, como lo sugiere la próxima figura. Es fácil ver que si tanto el proceso de evaluación como el proceso de mejora se estabilizan, es decir, ya no producen cambios, entonces la función de valor y la política deben ser óptimas. La función de valor se estabiliza sólo cuando es consistente con la política actual, y la política se estabiliza sólo cuando es codiciosa con respecto a la función de valor actual. Por lo tanto, ambos procesos se estabilizan sólo cuando se ha encontrado una política que es codiciosa con respecto a su propia función de evaluación. Esto implica que la ecuación de optimización de Bellman se mantiene, y por lo tanto que la política y la función de valor son óptimas.

![\label{fig:"sd622"}](~/Reinforcement-learning/diag.png)


También se podría pensar en la interacción entre los procesos de evaluación y mejora en los GPI en términos de dos limitaciones u objetivos, por ejemplo, como dos líneas en un espacio bidimensional, como se sugiere en la próxima figura. Aunque la geometría real es mucho más complicada, el diagrama sugiere lo que sucede en el caso real. Cada proceso conduce la función de valor o política hacia una de las líneas que representan una solución a uno de los dos objetivos. Las metas interactúan porque las dos líneas no son ortogonales. Conducir directamente hacia una meta causa que se aleje un poco de la otra meta. Sin embargo, inevitablemente, el proceso conjunto se acerca al objetivo general de la optimización. Las flechas de este diagrama corresponden al comportamiento de la iteración de políticas en el sentido de que cada una de ellas lleva al sistema hasta la consecución completa de uno de los dos objetivos. En GPI también se pueden dar pasos más pequeños e incompletos hacia cada meta. En cualquier caso, los dos procesos juntos logran el objetivo general de la optimización, aunque ninguno de los dos intente alcanzarlo directamente.


![\label{fig:"sd642"}](~/Reinforcement-learning/diag2.png)

## Eficiencia de la programación dinámica

DP puede no ser práctico para problemas muy grandes, pero en comparación con otros métodos para resolver MDPs, los métodos DP son en realidad bastante eficientes. Si ignoramos algunos detalles técnicos, entonces el tiempo que (en el peor de los casos) toman los métodos de DP para encontrar una política óptima es polinomial en el número de estados y acciones. Si$n$ y $k$ denotan el número de estados y acciones, esto significa que un método DP toma un número de operaciones computacionales que es menor que alguna función polinómica de $n$ y $k$. Se garantiza un método DP para encontrar una política óptima en tiempo polinómico aunque el número total de políticas (determinista) sea $k^n$. En este sentido, DP es exponencialmente más rápido de lo que podría ser cualquier búsqueda directa en el espacio de políticas, porque la búsqueda directa tendría que examinar exhaustivamente cada política para proporcionar la misma garantía. Los métodos de programación lineal también pueden utilizarse para resolver MDPs, y en algunos casos sus garantías de convergencia en el peor de los casos son mejores que las de los métodos DP. Pero los métodos de programación lineal resultan poco prácticos en un número mucho menor de estados que los métodos DP (por un factor de aproximadamente 100). Para los problemas más grandes, sólo los métodos DP son viables.

A veces se piensa que la DP es de aplicabilidad limitada debido a la maldición de la dimensionalidad, el hecho de que el número de estados a menudo crece exponencialmente con el número de variables de estado. Los grandes conjuntos de estados crean dificultades, pero éstas son inherentes al problema, no a DP como método de solución. De hecho, DP es comparativamente más adecuada para manejar grandes espacios de estados que los métodos de la competencia como la búsqueda directa y la programación lineal.

En la práctica, los métodos DP se pueden utilizar con los ordenadores actuales para resolver MDPs con millones de estados. Tanto la iteración de políticas como la iteración de valores se utilizan ampliamente, y no está claro cuál es mejor en general. En la práctica, estos métodos suelen converger mucho más rápido que sus tiempos de ejecución teóricos en el peor de los casos, especialmente si se inician con funciones o políticas de buen valor inicial.

En caso de problemas con grandes espacios de estado, a menudo se prefieren los métodos DP asincrónicos. Para completar incluso un barrido de un método sincrónico, se requiere computación y memoria para cada estado. Para algunos incluso esta cantidad de memoria y computación no es práctica, sin embargo, el problema sigue siendo potencialmente solucionable porque se producen relativamente pocos estados a lo largo de las trayectorias óptimas de la solución. Métodos asíncronos y otras variaciones de las GPI pueden aplicarse en tales casos y pueden encontrar políticas buenas u óptimas en muchos casos. más rápido que los métodos síncronos.