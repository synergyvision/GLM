<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Reinforcement-learning/" />
  <meta property="og:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Reinforcement-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2019-01-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="acerca-del-autor.html">
<link rel="next" href="modelos-lineales.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#ejemplos"><i class="fa fa-check"></i><b>1.2</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#elementos-del-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> Elementos del Reinforcement Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#limitaciones-y-alcance"><i class="fa fa-check"></i><b>1.4</b> Limitaciones y alcance</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#un-ejemplo-clasico-tres-en-linea."><i class="fa fa-check"></i><b>1.5</b> Un ejemplo clásico: tres en linea.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>2</b> Modelos Lineales</a><ul>
<li class="chapter" data-level="2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal</a></li>
<li class="chapter" data-level="2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros-del-modelo"><i class="fa fa-check"></i><b>2.2</b> Estimación de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>2.3</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="2.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelo-de-regresion-lineal-en-notacion-matricial"><i class="fa fa-check"></i><b>2.4</b> Modelo de Regresión Lineal en notación matricial</a></li>
<li class="chapter" data-level="2.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros"><i class="fa fa-check"></i><b>2.5</b> Estimación de los Parámetros</a></li>
<li class="chapter" data-level="2.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-ajustados-y-residuos"><i class="fa fa-check"></i><b>2.6</b> Valores Ajustados y Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#observaciones"><i class="fa fa-check"></i><b>2.7</b> Observaciones</a></li>
<li class="chapter" data-level="2.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generalizados-glm"><i class="fa fa-check"></i><b>2.8</b> Modelos Lineales Generalizados (GLM)</a><ul>
<li class="chapter" data-level="2.8.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#componentes-de-un-modelo-lineal-generalizado-glm"><i class="fa fa-check"></i><b>2.8.1</b> Componentes de un modelo lineal generalizado (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#un-problema-de-bandido-k-brasos"><i class="fa fa-check"></i><b>2.9</b> Un problema de bandido k-brasos</a></li>
<li class="chapter" data-level="2.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#mtodos-de-accin-valor"><i class="fa fa-check"></i><b>2.10</b> M??todos de acci??n valor</a></li>
<li class="chapter" data-level="2.11" data-path="modelos-lineales.html"><a href="modelos-lineales.html#pruebas-sobre-el-problema-del-bandido-de-10-brasos"><i class="fa fa-check"></i><b>2.11</b> Pruebas sobre el problema del bandido de 10 brasos</a></li>
<li class="chapter" data-level="2.12" data-path="modelos-lineales.html"><a href="modelos-lineales.html#aplicacin-progresiva"><i class="fa fa-check"></i><b>2.12</b> Aplicaci??n progresiva</a></li>
<li class="chapter" data-level="2.13" data-path="modelos-lineales.html"><a href="modelos-lineales.html#problemas-no-estacionarios"><i class="fa fa-check"></i><b>2.13</b> Problemas no estacionarios</a></li>
<li class="chapter" data-level="2.14" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ptimos-valores-iniciales"><i class="fa fa-check"></i><b>2.14</b> ??ptimos valores iniciales</a></li>
<li class="chapter" data-level="2.15" data-path="modelos-lineales.html"><a href="modelos-lineales.html#cota-superior-de-confianza-en-la-seleccin-de-acciones-csc"><i class="fa fa-check"></i><b>2.15</b> Cota superior de confianza en la selecci??n de acciones (CSC)</a></li>
<li class="chapter" data-level="2.16" data-path="modelos-lineales.html"><a href="modelos-lineales.html#algoritmo-del-gradiente"><i class="fa fa-check"></i><b>2.16</b> Algoritmo del gradiente</a></li>
<li class="chapter" data-level="2.17" data-path="modelos-lineales.html"><a href="modelos-lineales.html#investigacin-asociativa"><i class="fa fa-check"></i><b>2.17</b> Investigaci??n asociativa</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html"><i class="fa fa-check"></i><b>3</b> Procesos de decision de Markov finitos</a><ul>
<li class="chapter" data-level="3.1" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#el-agente-un-interface-del-entorno"><i class="fa fa-check"></i><b>3.1</b> El agente, Un interface del entorno</a></li>
<li class="chapter" data-level="3.2" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#metas-y-recompensas."><i class="fa fa-check"></i><b>3.2</b> Metas y recompensas.</a></li>
<li class="chapter" data-level="3.3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#retornos-y-episodios"><i class="fa fa-check"></i><b>3.3</b> Retornos y episodios</a></li>
<li class="chapter" data-level="3.4" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#notacion-unificada-tanto-para-tareas-episodicas-y-continuas."><i class="fa fa-check"></i><b>3.4</b> Notación unificada tanto para tareas episodicas y continuas.</a></li>
<li class="chapter" data-level="3.5" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#politicas-y-funciones-de-valor"><i class="fa fa-check"></i><b>3.5</b> Políticas y funciones de valor</a></li>
<li class="chapter" data-level="3.6" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#funciones-de-valor-y-politicas-optimas"><i class="fa fa-check"></i><b>3.6</b> Funciones de valor y políticas optimas</a></li>
<li class="chapter" data-level="3.7" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#optimalidad-y-aproximacion"><i class="fa fa-check"></i><b>3.7</b> Optimalidad y aproximación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html"><i class="fa fa-check"></i><b>4</b> Programación dinámica</a><ul>
<li class="chapter" data-level="4.1" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#politicas-evaluadas-prediccion"><i class="fa fa-check"></i><b>4.1</b> Políticas evaluadas (Predicción)</a></li>
<li class="chapter" data-level="4.2" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#mejora-de-las-politicas"><i class="fa fa-check"></i><b>4.2</b> Mejora de las políticas</a></li>
<li class="chapter" data-level="4.3" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-de-politicas"><i class="fa fa-check"></i><b>4.3</b> Iteración de políticas</a></li>
<li class="chapter" data-level="4.4" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-de-valores"><i class="fa fa-check"></i><b>4.4</b> Iteración de valores</a></li>
<li class="chapter" data-level="4.5" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#programacion-dinamica-asincronica."><i class="fa fa-check"></i><b>4.5</b> Programación dinámica asincrónica.</a></li>
<li class="chapter" data-level="4.6" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-generalizada-de-politicas"><i class="fa fa-check"></i><b>4.6</b> Iteración generalizada de políticas</a></li>
<li class="chapter" data-level="4.7" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#eficiencia-de-la-programacion-dinamica"><i class="fa fa-check"></i><b>4.7</b> Eficiencia de la programación dinámica</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html"><i class="fa fa-check"></i><b>5</b> Métodos de Montecarlo</a><ul>
<li class="chapter" data-level="5.1" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#prediccion-con-monte-carlo"><i class="fa fa-check"></i><b>5.1</b> Predicción con Monte Carlo</a></li>
<li class="chapter" data-level="5.2" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#estimacion-de-monte-carlo-de-los-valores-de-accion"><i class="fa fa-check"></i><b>5.2</b> Estimación de Monte Carlo de los Valores de Acción</a></li>
<li class="chapter" data-level="5.3" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control"><i class="fa fa-check"></i><b>5.3</b> Métodos de Monte Carlo con control</a></li>
<li class="chapter" data-level="5.4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control-sin-iniciar-exploracion"><i class="fa fa-check"></i><b>5.4</b> Métodos de Monte Carlo con control sin iniciar exploración</a></li>
<li class="chapter" data-level="5.5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#predicciones-no-politicas-via-muestreos-de-importancia."><i class="fa fa-check"></i><b>5.5</b> Predicciones no políticas via muestreos de importancia.</a></li>
<li class="chapter" data-level="5.6" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#implementacion-incremental"><i class="fa fa-check"></i><b>5.6</b> Implementación incremental</a></li>
<li class="chapter" data-level="5.7" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#monte-carlo-no-politico-con-control"><i class="fa fa-check"></i><b>5.7</b> Monte Carlo no político con control</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html"><i class="fa fa-check"></i><b>6</b> Aprendizaje por Diferencia Temporal</a><ul>
<li class="chapter" data-level="6.1" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#prediccion"><i class="fa fa-check"></i><b>6.1</b> Predicción</a></li>
<li class="chapter" data-level="6.2" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#ventajas-de-los-metodos-de-prediccion-de-td"><i class="fa fa-check"></i><b>6.2</b> Ventajas de los métodos de predicción de TD</a></li>
<li class="chapter" data-level="6.3" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#calidad-de-td0"><i class="fa fa-check"></i><b>6.3</b> Calidad de TD(0)</a></li>
<li class="chapter" data-level="6.4" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-td-politico-con-control"><i class="fa fa-check"></i><b>6.4</b> Sarsa: TD político con control</a></li>
<li class="chapter" data-level="6.5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#q-learning-td-no-politico-con-control"><i class="fa fa-check"></i><b>6.5</b> Q-Learning: TD no político con control</a></li>
<li class="chapter" data-level="6.6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-esparada"><i class="fa fa-check"></i><b>6.6</b> Sarsa esparada</a></li>
<li class="chapter" data-level="6.7" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sesgo-de-maximizacion-y-doble-aprendizaje"><i class="fa fa-check"></i><b>6.7</b> Sesgo de maximización y doble aprendizaje</a></li>
<li class="chapter" data-level="6.8" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#juegos-afterstates-y-otros-casos-especiales"><i class="fa fa-check"></i><b>6.8</b> Juegos, afterstates y otros casos especiales</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping en <span class="math inline">\(n\)</span>-pasos</a><ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html#prediccion-de-td-en-n-pasos"><i class="fa fa-check"></i><b>7.1</b> Predicción de TD en <span class="math inline">\(n\)</span> pasos</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduccion" class="section level1">
<h1><span class="header-section-number">Capítulo 1</span> Introducción</h1>
<p>Cuando pensamos sobre el aprendizaje que engloba los seres biológicos lo primero que se nos viene a la mente es en el aprendizaje producto de la interacción con el ambiente que lo rodea. Cuando un bebé juega, mueve los brazos o mira a su alrededor, no tiene un maestro explícito, pero tiene una conexión sensomotora directa con su entorno. Ejerciendo esto, la conexión produce una gran cantidad de información sobre la causa y el efecto, sobre las consecuencias de las acciones y sobre qué hacer para alcanzar las metas. A lo largo de nuestras vidas, estas interacciones son, sin duda, una fuente importante de conocimiento sobre nuestro medio ambiente y sobre nosotros mismos. Ya sea que estemos aprendiendo a conducir un automóvil o a mantener una conversación, estamos muy conscientes de cómo nuestro entorno responde a lo que hacemos, y tratamos de entender lo que sucede a través de nuestro comportamiento. Aprender de la interacción es una idea fundamental que subyace en casi todas las teorías del aprendizaje y la inteligencia.</p>
<p>En este libro exploramos un enfoque computacional para aprender de la interacción. En lugar de teorizar directamente sobre cómo aprenden las personas o los animales, exploramos situaciones de aprendizaje idealizadas y evaluamos la efectividad de varios métodos de aprendizaje. Es decir, adoptamos la perspectiva de un investigador o ingeniero de inteligencia artificial. El enfoque que exploramos, llamado Reinforcement Learning, está mucho más enfocado en el aprendizaje dirigido por objetivos a partir de la interacción que otros enfoques de Machine Learning.</p>
<div id="reinforcement-learning" class="section level2">
<h2><span class="header-section-number">1.1</span> Reinforcement Learning</h2>
<p>El aprendizaje de refuerzo es aprender que hacer, cómo relacionar las situaciones con las acciones, con el fin de maximizar una señal numérica de recompensa. No se le dice al algoritmo que acciones debe tomar, sino que debe descubrir qué acciones producen la mayor recompensa al probarlos. En los casos más interesantes y desafiantes, las acciones pueden afectar no sólo a la recompensa inmediata sino también a la siguiente situación y, a través de ella, a todas las recompensas subsecuentes. Estas dos características de búsqueda de prueba, error y recompensa retrasadas son las dos características distintivas más importantes del Reinforcement Learning.</p>
<p>Formalizamos el problema del aprendizaje de refuerzo usando ideas de la teoría de sistemas dinámicos, específicamente, como el control óptimo de procesos de decisión de Markov incompletos. Los detalles sobre este tipo de procesos serán vistos en los próximos capítulos, pero la idea básica es simplemente capturar los aspectos más importantes del problema real que enfrenta un agente de aprendizaje que interactúa en el tiempo con su entorno para lograr una meta. Un agente de aprendizaje debe ser capaz de sentir el estado de su entorno hasta cierto punto y debe ser capaz de tomar acciones que afecten al estado. El agente también debe tener una meta o metas relacionadas con el estado del entorno. Los procesos de decisión de Markov pretenden incluir sólo estos tres aspectos: sensación, acción y meta en sus formas más simples posibles sin trivializar ninguna de ellos. Cualquier método que se adapte bien a la resolución de estos problemas lo consideramos un método de Reinforcement Learning.</p>
<p>Reinforcement Learning es diferente del aprendizaje supervisado, este tipo de aprendizaje es el mas común y estudiado en el machine learning. En general el aprendizaje supervisado funciona de la siguiente manera: a través de un conjunto de datos ya conocidos y con valores de interés ya bien sabidos (train), se entrena al modelo que se valla a implementar, con el fin de a partir de él, extrapolar lo aprendido con los datos de train y lograr predecir con cierta precisión los niveles de interés de un conjunto de datos de interés que poseen ciertas características desconocidas. Este tipo de aprendizaje aunque muy importante, solo, no es adecuado para el aprendizaje por interacción.</p>
<p>Reinforcement Learning es además diferente del aprendizaje no supervisado el cual como objetivo principal busca hallar alguna estructura oculta en una colección de datos no procesados.</p>
<p>Los términos de aprendizaje supervisado y no supervisado parecieran un intento de crear paradigmas en el machine learning, pero en realidad no es así. Aunque pudiéramos estar tentados en pensar que el Reinforcement Learning es un tipo de aprendizaje no supervisado pues el no se basa en ejemplos de comportamiento correcto, el esta enfocado en tratar de maximizar una señal de recompensa en vez de tratar de encontrar un estructura oculta. Aunque el descubrir cierta estructura que ayude al agente a tomar decisiones mas productivas no se descarta como objetivo secundario. Es por esto que es preferible o considerar al reinforcement learning como un tercer paradigma.</p>
</div>
<div id="ejemplos" class="section level2">
<h2><span class="header-section-number">1.2</span> Ejemplos</h2>
<p>Los siguientes ejemplos nos ayudarán a entender mejor los problemas en los cuales el enfoque de Reinforcement Learning surge de manera natural.</p>
<ul>
<li><p>Un maestro de ajedrez hace un movimiento. La elección se basa tanto en planificación, anticipando posibles réplicas y contra réplicas y por juicios inmediatos e intuitivos sobre posiciones particulares y movimientos futuros.</p></li>
<li><p>Un controlador que se adapta para ajustar a tiempo real los parametros en las operaciones de una refinería petrolera. El controlador optimiza la relación rendimiento/coste/calidad sobre la base de los costes marginales especificados sin atenerse estrictamente a los puntos de ajuste sugeridos originalmente por los ingenieros.</p></li>
<li><p>Una gacela con pocos minutos de nacido no puede controlar sus pantorrillas, pero transcurrida una hora puede correr a 20 millas por hora.</p></li>
<li><p>Un robot móvil decide si debe entrar a un cuarto a recoger la basura tomando en cuanta el costo que debe pagar en cuanto a consumo de baterías y espacio recorrido.</p></li>
</ul>
<p>Todos estos ejemplos comparten las mismas características básicas, todos están relacionados por un agente que toma decisiones sobre como debe actuar dependiendo del ambiente que lo rodea y con el fin de alcanzar una meta concreta, por ejemplo, el maestro de ajedrez desea ganar la partida, el controlador en la refinería optimizar costos, la gacela aprender a correr y el robot debe recoger la basura gastando la menor cantidad de energía posible.</p>
<p>Al mismo tiempo, en todos estos ejemplos, es imposible tomar acciones a partir de decisiones completamente predictivas, el agente debe estar monitoreando frecuentemente el entorno para adaptarse mejor a la situación actual. Para terminar esta sección debemos destacar que el agente debe usar la experiencia para que a través de las constantes señales de información que recibe, relacione posibles acciones a tomar con otras ya tomadas.</p>
</div>
<div id="elementos-del-reinforcement-learning" class="section level2">
<h2><span class="header-section-number">1.3</span> Elementos del Reinforcement Learning</h2>
<p>Detrás del agente y del entorno, podemos identificar subelementos que engloba este tipo de aprendizaje: Una política, señal de ganancia, función de valor, y, opcionalmente, un modelo del entorno.</p>
<ul>
<li><p>Una política define como debe comportarse el agente en algún momento dado. Hablando de manera mas formal, una política es una función que tiene como dominio el conjunto de los estados posibles y como rango el conjunto de las posibles acciones a tomar. La política es el núcleo en el Reinforcement Learning, en el sentido de que con ella es posible determinar el comportamiento del agente. En genera una política puede ser estocástica.</p></li>
<li><p>La señal de ganancia define la meta en el Reinforcement Learning, en cada momento, el entorno envía al agente información cuantificada por él mismo que llamaremos ganancias o recompensas. El agente tendrá como objetivo maximizar las ganancias a lo largo del tiempo. Estas señales indican que acciones son buenas o malas dependiendo el momento o estado al que el agente se enfrente. Por ejemplo, en sistemas biológicos, la señales de ganancias pudieran estar relacionadas con las experiencias de placer y dolor. Estas señales serán de vital importancias, pues con ellas podremos hacer cambios en las políticas para obtener resultados mas precisos.</p></li>
<li><p>Mientras las señales de ganancias nos indican que tan buenas son las acciones en el sentido inmediato, una función valor nos indica que tan buena es una acción a largo del tiempo. En pocas palabras, la función de valor nos indica la cantidad total de ganancia que podemos esperar en el futuro partiendo de un estado en especifico.</p></li>
</ul>
<p>Las ganancias son en cierto sentido primarias, mientras que los valores de la función valor, son secundarios, esto es claro pues sin ganancias no habría función de valor, y el único propósito de estimar valores es obtener mas ganancias. Sin embargo, a la hora de tomar decisiones la función de valor es el primer referente a tomar, pues recordemos que no estamos interesados en grandes ganancias inmediatas, sino en grandes ganancias en el transcurso del tiempo. Imaginemos una partida de ajedrez donde el rival nos tiende una trampa al ceder una pieza de alto valor, como por ejemplo la dama, pero nos amenaza de mate con la torre, tomar el cebo nos pudiera dar una gran ganancia inmediata, pero habremos perdido la partida.</p>
<ul>
<li>El cuarto elemento en el Reinforcement Learning es el modelo del entorno. Esto es alguna veces imitar el comportamiento del entorno, es decir, predecir como cambiara el sistema para prepararnos de antemano a dichos cambios, esto dicho de forma mas clara, es una forma de elaborar planes de acción y así lograr que el agente no se distraiga en considerar un conjunto de mayores posibilidades que las que de verdad existen.</li>
</ul>
</div>
<div id="limitaciones-y-alcance" class="section level2">
<h2><span class="header-section-number">1.4</span> Limitaciones y alcance</h2>
<p>La mayoría de los métodos de Reinforement Learning que nosotros consideraremos son modelos orientados a la estimación de funciones de valor, pero esto no es estrictamente necesario. Por ejemplo, métodos tales como los algoritmos genéticos, programación genética, algoritmo de recocido simulado y otros métodos de optimización han sido usados para evitar el calculo de funciones de valor. Estos métodos evalúan el comportamiento a lo largo de la vida del agente, sin que el mismo aprenda, luego en general van cambiando de políticas, hasta hallar la que mejor se comporte y adapte a nuestro problema, es decir, nos arrogan mas recompensas. Estos métodos son conocidos como métodos evolucionados pues su comportamiento es análogo al de organismos biológicamente evolucionados con conocimiento heredado de pasadas generaciones (instinto). Si el espacio de políticas es relativamente pequeño o bien estructurados estos metodos pueden ser muy efectivos pues resulta fácil a través de simulaciones hallar la política óptima. Estos metodos además tienen la ventaja cuando el agente no puede percibir todos los estímulos provenientes del entorno.</p>
<p>Sin embargo, nuestro foco esta orientado a los algoritmos que aprenden por interacción con el entorno debido a que a través de ellos podemos extraer información del problema y entender como las acciones afectan directamente al entorno. Además es claro que los métodos evolucionados ignoran muchas de las estructuras básicas del Reinforcement Learning. En el presente libro serán incluidos algunos métodos evolucionados para tener un mayor campo de aplicación.</p>
</div>
<div id="un-ejemplo-clasico-tres-en-linea." class="section level2">
<h2><span class="header-section-number">1.5</span> Un ejemplo clásico: tres en linea.</h2>
<p>Para ilustrar la idea general del Reinforcement learning y sus aproximaciones estudiaremos el juego de tres en linea.</p>
<p>Este juego se juega entre dos individuos los cuales deben escoger un símbolo “O” o “X”, luego en un cuadro 3x3 a turnos deben marcas en alguna casilla su símbolo, un jugador gana el juego cuando logra a linear tres de sus símbolos, ya sea vertical, horizontal o diagonal. A modo de simplificar el problema, supondremos que nuestro rival es imperfecto, esto es para evitar empates, así el problema consiste en encontrar las imperfecciones de nuestro rival, para de ese modo lograr ganar la partida.</p>
<p>aunque parece un problema sencillo, el aplicar métodos clásicos de teoría de juegos no son correctos, pues ellos requieren que conozcamos a nuestro oponente antes de computar una estrategia de juego. De igual manera con métodos de programación dinámica, aunque en general pueden darnos soluciones muy precisas requieren de igual forma información detallada del oponente como por ejemplo las probabilidades de transición de cada movimiento. En general esta información no es conocida a priori, pues no se pretende particularizar a un oponente. Una solución a este problema seria estimar estas probabilidades realizando muchos encuentres simulados con oponentes con estrategias similares, modelando así su comportamiento, hasta cierto punto, luego aplicar métodos de programación dinámica para hallar la estrategia ganadora. Al final esto es la filosofía del Reinforcement Learning.</p>
<p>Un método evolucionado estudiaría todo el espacio de posibilidades hallando la estrategia ganadora. En este ejemplo en particular por la sencillez este tipo de modelos fueran los mas óptimos, pero debemos tomar en cuenta que para distintos entornos con miles de combinaciones esto puede tender hacer muy laborioso el calculo de la estrategia.</p>
<p>Para realizar el la aproximación a través de funciones de valor podemos crear unas tablas para cada posible estado del juego, cada numero denotara la posibilidad de ganar el juego desde ese estado, trataremos de estimar el valor de ese estado, para luego una vez rellanadas las tablas tomar la estrategia. El estado “A” tiene mayor valor que el estado “B”, o es considerado mejor, si pose mayor probabilidad de ganar el juego. una vez logremos llegar al estado de tres en linea, le asignaremos probabilidad 1, pues ya habremos ganado, y si de contrario el rival obtiene tres en linea le asignaremos a ese estado probabilidad 0 pues ya habremos perdido, inicialmente a todos los estados que no tengan tres en linea le asignaremos 0,5 de probabilidad.</p>
<p>En general seremos ambiciosos, y escogeremos las acciones que no conduzcan a los estados con mayor probabilidad de ganar, pero de forma aleatoria escogeremos algunas acciones no tan prometedoras para estudiar sus consecuencias, esto es llamado exploración, de esta forma iremos calibrando el valor real de las acciones, en cada respectivo estado, en la siguiente imagen se ilustra el procedimiento</p>
<div class="figure">
<img src="~/Reinforcement-learning/reinforcement.png" alt="Serie de movimientos" />
<p class="caption">Serie de movimientos</p>
</div>
<p>Mientras jugamos, cambiamos los valores de los estados en los que nos encontramos durante el juego. Intentamos que sean estimaciones más precisas de las probabilidades de ganar. Para hacer esto, volvemos el valor del estado después de cada movimiento codicioso al estado antes del movimiento, como lo sugiere la figura anterior. Más precisamente, el valor actual del estado anterior se actualiza para estar más cerca de el valor del estado posterior. Esto se puede hacer moviendo el valor del estado anterior una fracción del camino hacia el valor del estado posterior. Si denotamos <span class="math inline">\(s\)</span> el estado antes del movimiento codicioso, y <span class="math inline">\(s&#39;\)</span> el estado después del movimiento, entonces la actualización al valor estimado de s, denotado <span class="math inline">\(V(s)\)</span>, puede escribirse como <span class="math display">\[V(s)= V(s)+\alpha[V(s&#39;)-V(s)]\]</span> donde <span class="math inline">\(\alpha\)</span> es una constante positiva pequeña llamada paremetro de ajuste, el cual influye de gran manera en el radio de aprendizaje. Esta regla de actualización es un ejemplo de los metodos de apendizaje basados en diferencias temporales.</p>
<p>El metodo descrito anteriormente se ajusta bastante bien a la tarea, por ejemplo si el parametro se reduce de forma adecuada durante el tiempo, el metodo convergera para cualquier oponente fijo.</p>
<p>Este ejemplo ilustra las diferencias entre métodos evolucionados y métodos que aprenden a traves de funciones de valor. Para evaluar una política en un método evolucionado, mantenemos la política fija y simulamos varios encuentros, la frequencia de victorias será un estimador insesgado de la probabilidad de victorias con esa política, pero en general, ignoran movimientos intermedios, los cuales pueden ser menos productivos a la hora de escoger políticas para oponentes similares. Los métodos basados en funciones de valores en contraste si se enfocan en los estados intermedios. Este ejemplo aunque muy basico puede involucrar más de 300 mil acciones, inclusive juegos como backgammon, pueden ser que posen mas de <span class="math inline">\(10^{20}\)</span> estados, se les puede hallar solucciones muy satisfactorias con Reinforcement Learning.</p>

</div>
</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="acerca-del-autor.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modelos-lineales.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Analisis_y_Medida/edit/master/bookdown/010-introduction.Rmd",
"text": "Edit"
},
"download": ["Reinforcement Learning.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
