<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Reinforcement-learning/" />
  <meta property="og:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Reinforcement-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2019-01-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="aprendizaje-por-diferencia-temporal.html">
<link rel="next" href="software-tools.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#ejemplos"><i class="fa fa-check"></i><b>1.2</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#elementos-del-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> Elementos del Reinforcement Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#limitaciones-y-alcance"><i class="fa fa-check"></i><b>1.4</b> Limitaciones y alcance</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#un-ejemplo-clasico-tres-en-linea."><i class="fa fa-check"></i><b>1.5</b> Un ejemplo clásico: tres en linea.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>2</b> Modelos Lineales</a><ul>
<li class="chapter" data-level="2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal</a></li>
<li class="chapter" data-level="2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros-del-modelo"><i class="fa fa-check"></i><b>2.2</b> Estimación de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>2.3</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="2.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelo-de-regresion-lineal-en-notacion-matricial"><i class="fa fa-check"></i><b>2.4</b> Modelo de Regresión Lineal en notación matricial</a></li>
<li class="chapter" data-level="2.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros"><i class="fa fa-check"></i><b>2.5</b> Estimación de los Parámetros</a></li>
<li class="chapter" data-level="2.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-ajustados-y-residuos"><i class="fa fa-check"></i><b>2.6</b> Valores Ajustados y Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#observaciones"><i class="fa fa-check"></i><b>2.7</b> Observaciones</a></li>
<li class="chapter" data-level="2.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generalizados-glm"><i class="fa fa-check"></i><b>2.8</b> Modelos Lineales Generalizados (GLM)</a><ul>
<li class="chapter" data-level="2.8.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#componentes-de-un-modelo-lineal-generalizado-glm"><i class="fa fa-check"></i><b>2.8.1</b> Componentes de un modelo lineal generalizado (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generelizados-para-datos-binarios"><i class="fa fa-check"></i><b>2.9</b> Modelos Lineales Generelizados para datos binarios</a></li>
<li class="chapter" data-level="2.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-logistica"><i class="fa fa-check"></i><b>2.10</b> Regresión Logística</a></li>
<li class="chapter" data-level="2.11" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-probit"><i class="fa fa-check"></i><b>2.11</b> Regresión Probit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html"><i class="fa fa-check"></i><b>3</b> Procesos de decision de Markov finitos</a><ul>
<li class="chapter" data-level="3.1" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#el-agente-un-interface-del-entorno"><i class="fa fa-check"></i><b>3.1</b> El agente, Un interface del entorno</a></li>
<li class="chapter" data-level="3.2" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#metas-y-recompensas."><i class="fa fa-check"></i><b>3.2</b> Metas y recompensas.</a></li>
<li class="chapter" data-level="3.3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#retornos-y-episodios"><i class="fa fa-check"></i><b>3.3</b> Retornos y episodios</a></li>
<li class="chapter" data-level="3.4" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#notacion-unificada-tanto-para-tareas-episodicas-y-continuas."><i class="fa fa-check"></i><b>3.4</b> Notación unificada tanto para tareas episodicas y continuas.</a></li>
<li class="chapter" data-level="3.5" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#politicas-y-funciones-de-valor"><i class="fa fa-check"></i><b>3.5</b> Políticas y funciones de valor</a></li>
<li class="chapter" data-level="3.6" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#funciones-de-valor-y-politicas-optimas"><i class="fa fa-check"></i><b>3.6</b> Funciones de valor y políticas optimas</a></li>
<li class="chapter" data-level="3.7" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#optimalidad-y-aproximacion"><i class="fa fa-check"></i><b>3.7</b> Optimalidad y aproximación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html"><i class="fa fa-check"></i><b>4</b> Métodos de Montecarlo</a><ul>
<li class="chapter" data-level="4.1" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#prediccion-con-monte-carlo"><i class="fa fa-check"></i><b>4.1</b> Predicción con Monte Carlo</a></li>
<li class="chapter" data-level="4.2" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#estimacion-de-monte-carlo-de-los-valores-de-accion"><i class="fa fa-check"></i><b>4.2</b> Estimación de Monte Carlo de los Valores de Acción</a></li>
<li class="chapter" data-level="4.3" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control"><i class="fa fa-check"></i><b>4.3</b> Métodos de Monte Carlo con control</a></li>
<li class="chapter" data-level="4.4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control-sin-iniciar-exploracion"><i class="fa fa-check"></i><b>4.4</b> Métodos de Monte Carlo con control sin iniciar exploración</a></li>
<li class="chapter" data-level="4.5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#predicciones-no-politicas-via-muestreos-de-importancia."><i class="fa fa-check"></i><b>4.5</b> Predicciones no políticas via muestreos de importancia.</a></li>
<li class="chapter" data-level="4.6" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#implementacion-incremental"><i class="fa fa-check"></i><b>4.6</b> Implementación incremental</a></li>
<li class="chapter" data-level="4.7" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#monte-carlo-no-politico-con-control"><i class="fa fa-check"></i><b>4.7</b> Monte Carlo no político con control</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html"><i class="fa fa-check"></i><b>5</b> Aprendizaje por Diferencia Temporal</a><ul>
<li class="chapter" data-level="5.1" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#prediccion"><i class="fa fa-check"></i><b>5.1</b> Predicción</a></li>
<li class="chapter" data-level="5.2" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#ventajas-de-los-metodos-de-prediccion-de-td"><i class="fa fa-check"></i><b>5.2</b> Ventajas de los métodos de predicción de TD</a></li>
<li class="chapter" data-level="5.3" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#calidad-de-td0"><i class="fa fa-check"></i><b>5.3</b> Calidad de TD(0)</a></li>
<li class="chapter" data-level="5.4" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-td-politico-con-control"><i class="fa fa-check"></i><b>5.4</b> Sarsa: TD político con control</a></li>
<li class="chapter" data-level="5.5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#q-learning-td-no-politico-con-control"><i class="fa fa-check"></i><b>5.5</b> Q-Learning: TD no político con control</a></li>
<li class="chapter" data-level="5.6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-esparada"><i class="fa fa-check"></i><b>5.6</b> Sarsa esparada</a></li>
<li class="chapter" data-level="5.7" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sesgo-de-maximizacion-y-doble-aprendizaje"><i class="fa fa-check"></i><b>5.7</b> Sesgo de maximización y doble aprendizaje</a></li>
<li class="chapter" data-level="5.8" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#juegos-afterstates-y-otros-casos-especiales"><i class="fa fa-check"></i><b>5.8</b> Juegos, afterstates y otros casos especiales</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping en <span class="math inline">\(n\)</span>-pasos</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html#prediccion-de-td-en-n-pasos"><i class="fa fa-check"></i><b>6.1</b> Predicción de TD en <span class="math inline">\(n\)</span> pasos</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bootstrapping-en-n-pasos" class="section level1">
<h1><span class="header-section-number">Capítulo 6</span> Bootstrapping en <span class="math inline">\(n\)</span>-pasos</h1>
<p>En este capítulo unificamos los métodos de Monte Carlo (MC) y de dierencia temporal de un solo paso (TD). métodos presentados en los dos capítulos anteriores. Ni los métodos MC ni los métodos TD de un solo paso son siempre el mejor. En este capítulo presentamos métodos de TD en n pasos que generalizan ambos métodos de manera que uno puede cambiar de uno a otro sin problemas según sea necesario para satisfacer las demandas de una tarea en particular. Metodos en n-paso abarcan un espectro con métodos MC en un extremo y métodos TD de un paso en el otro. Los mejores métodos son a menudo intermedios entre los dos extremos.</p>
<p>Otra forma de ver los beneficios de los métodos de n-paso es que nos liberan de la maldición del paso del tiempo. Con los métodos de TD de un solo paso, el mismo paso de tiempo determina con qué frecuencia se puede cambiar la acción y el intervalo de tiempo durante el cual se realiza el bootstrapping. En muchas aplicaciones uno quiere ser capaz de actualizar la acción muy rápidamente para tener en cuenta cualquier cosa que haya cambiado, pero el bootstrapping funciona mejor si es durante un periodo de tiempo en el que se ha producido un cambio de estado significativo y reconocible. Con los métodos de TD de un solo paso, estos intervalos de tiempo son los mismos, por lo que se debe llegar a un compromiso. Los métodos de n pasos permiten que el bootstrapping ocurra en múltiples pasos, liberándonos de la tiranía de un solo paso de tiempo.</p>
<p>La idea de los métodos de <span class="math inline">\(n\)</span> pasos se utiliza generalmente como una introducción a la idea algorítmica de las trazas de elegibilidad (Capítulo 12), que permite el bootstrapping a lo largo de múltiples intervalos de tiempo simultáneamente. Aquí, en cambio, consideramos la idea de bootstrapping de <span class="math inline">\(n\)</span> pasos por sí sola, posponiendo el tratamiento de los mecanismos de trazado de eligibilidad hasta más adelante. Esto nos permite separar mejor los temas, tratando tantos de ellos como sea posible en la configuración más simple de <span class="math inline">\(n\)</span> pasos.</p>
<p>Como de costumbre, primero consideramos el problema de la predicción y luego el problema del control. Es decir, primero consideramos cómo los métodos de <span class="math inline">\(n\)</span> pasos pueden ayudar a predecir los retornos como una función del estado para una política fija (es decir, al estimar <span class="math inline">\(v_\pi\)</span>). Luego ampliamos las ideas a valores de acción y métodos de control.</p>
<div id="prediccion-de-td-en-n-pasos" class="section level2">
<h2><span class="header-section-number">6.1</span> Predicción de TD en <span class="math inline">\(n\)</span> pasos</h2>
<p>¿Cuál es el espacio de métodos entre los métodos de Monte Carlo y TD? Considere la posibilidad de estimar <span class="math inline">\(v_\pi\)</span> a partir de episodios de la muestra generados mediante <span class="math inline">\(\pi\)</span>. Los métodos de Monte Carlo realizan una actualización para cada estado basada en la secuencia completa de recompensas observadas desde ese estado hasta el final del episodio. La actualización de los métodos de TD de un solo paso, por otro lado, se basa en la siguiente recompensa, arrancando desde el valor del estado un paso más tarde como un proxy para las recompensas restantes. Un tipo de método intermedio, entonces, realizaría una actualización basada en un número intermedio de recompensas: más de una, pero menos de todas hasta la terminación. Por ejemplo, una actualización de dos pasos se basaría en las dos primeras recompensas y el valor estimado del estado dos pasos después. Del mismo modo, podríamos tener actualizaciones de tres pasos, actualizaciones de cuatro pasos, y así sucesivamente. La siguiente figura muestra los diagramas de respaldo del espectro de actualizaciones de <span class="math inline">\(n\)</span>-pasos para <span class="math inline">\(v_\pi\)</span>, con la actualización de TD de un paso a la izquierda y la actualización de Monte Carlo hasta el final a la derecha.</p>
<p><img src="~/Reinforcement-learning/61.png" alt="" /> Los métodos que utilizan actualizaciones de <span class="math inline">\(n\)</span> pasos siguen siendo métodos de TD porque todavía cambian una estimación anterior basada en cómo difiere de una estimación posterior. Ahora bien, la estimación posterior no es un paso más tarde, sino <span class="math inline">\(n\)</span> pasos más tarde. Los métodos en los que la diferencia temporal se extiende sobre <span class="math inline">\(n\)</span> pasos se denominan métodos de TD de <span class="math inline">\(n\)</span> pasos. Todos los métodos de TD introducidos en el capítulo anterior utilizaban actualizaciones de un solo paso, por lo que los llamamos métodos de TD de un solo paso.</p>
<p>De manera más formal, considere la actualización del valor estimado del estado <span class="math inline">\(S_t\)</span> como resultado de la secuencia de recompensas del estado, <span class="math inline">\(S_t,R_{t+1},S_{t+1},R_{t+2},...,R_T,S_T\)</span> (omitiendo las acciones). Sabemos que en Monte Carlo se actualiza la estimación de <span class="math inline">\(v_\pi(S_t)\)</span> en la dirección del retorno completo: <span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}\]</span> donde T es el último paso del episodio. Llamemos a esta cantidad el objetivo de la actualización. Mientras que en las actualizaciones de Monte Carlo el objetivo es el retorno, en las actualizaciones de un paso el objetivo es la primera recompensa más el valor estimado descontado del siguiente estado, que llamamos el retorno de un paso:<span class="math display">\[G_{t:t+1}=R_{t+1}+\gamma V_t(S_{t+1})\]</span> donde <span class="math inline">\(V_t:S\rightarrow \mathbb{R}\)</span> es la estimación en el tiempo <span class="math inline">\(t\)</span> de <span class="math inline">\(v_\pi\)</span>. Los subíndices de <span class="math inline">\(G_{t:t+1}\)</span> indican que se trata de un retorno truncado para el tiempo <span class="math inline">\(t\)</span> utilizando recompensas hasta el tiempo <span class="math inline">\(t + 1\)</span>, con la estimación descontada <span class="math inline">\(\gamma V_t(S_{t+1})\)</span> tomada en lugar de los terminos <span class="math inline">\(\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}\)</span> del retorno completo, como se discutió en el capítulo anterior. Nuestro punto ahora es que esta idea tiene tanto sentido después de dos pasos como después de uno. El objetivo de una actualización en dos pasos es el retorno en dos pasos: <span class="math display">\[G_{t:t+2}=R_{t+1}+\gamma V_t(S_{t+1})+\gamma^2 V_t(S_{t+2})\]</span> donde <span class="math inline">\(\gamma^2 V_t(S_{t+2})\)</span> corrige la ausencia de los términos <span class="math inline">\(\gamma^2 R_{t+3}+...+\gamma^{T-t-1} R_{T}\)</span>. Del mismo modo, el objetivo de una actualización arbitraria de <span class="math inline">\(n\)</span> pasos es el retorno de <span class="math inline">\(n\)</span> pasos: <span class="math display">\[G_{t:t+2}=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{n-1} R_{n}+\gamma^n V_{t+n-1}(S_{t+n})\]</span> para todo <span class="math inline">\(n,t\)</span>, tal que <span class="math inline">\(n\ge1\)</span> y <span class="math inline">\(0\le t&lt;T-n\)</span>. Todas los retornos de <span class="math inline">\(n\)</span> pasos pueden considerarse aproximaciones al retorno completo, truncadas después de <span class="math inline">\(n\)</span> pasos y luego corregidas para tener en cuenta los plazos restantes que faltan, de la siguiente manera por <span class="math inline">\(V_{t+n-1}(S_{t+n})\)</span>. Si <span class="math inline">\(t+n\ge T\)</span> (si el retorno de <span class="math inline">\(n\)</span> pasos se extiende hasta o más allá del termino) entonces todos los términos que faltan se toman como cero, y el retorno de <span class="math inline">\(n\)</span> pasos se define como igual al retorno total ordinario (<span class="math inline">\(G_{t:t+n}= G_t\)</span> si <span class="math inline">\(t+n \ge T\)</span>).</p>
<p>Note que el retorno en <span class="math inline">\(n\)</span> pasos para <span class="math inline">\(n&gt;1\)</span>, implican recompensas futuras y estados que no están disponibles en el momento de la transición de <span class="math inline">\(t\)</span> a <span class="math inline">\(t + 1\)</span>. Ningún algoritmo real puede usar el retorno de <span class="math inline">\(n\)</span> pasos hasta que haya visto <span class="math inline">\(R_{t+n}\)</span> y calculado <span class="math inline">\(V_{t+n-1}\)</span>. La primera vez que están disponibles es <span class="math inline">\(t+n\)</span>. El algoritmo natural de aprendizaje de valor estado para el uso de los retornos de <span class="math inline">\(n\)</span> pasos es así:<span class="math display">\[V_{t+n}(S_t)=V_{t+n-1}(S_t)+\alpha[G_{t:t+n}-V_{t+n-1}(S_t)],\quad 0\le t&lt;T\]</span>mientras que los valores de todos los demás estados permanecen inalterados: <span class="math inline">\(V_{t+n}(s) = V_{t+n-1}(s)\)</span> para todo <span class="math inline">\(s\neq S_t\)</span>. Llamamos a este algoritmo TD de <span class="math inline">\(n\)</span> pasos. Tengamos en cuenta que no se realizan cambios en absoluto durante los primeros <span class="math inline">\(n-1\)</span> pasos de cada episodio. Para compensar esto, se hace un número igual de actualizaciones adicionales al final del episodio, después de la terminación y antes de comenzar el siguiente episodio.</p>
<p><strong>TD en <span class="math inline">\(n\)</span> pasos para estimar <span class="math inline">\(V\approx v_\pi\)</span></strong></p>
<ul>
<li>Iniciamos <span class="math inline">\(V(s)\)</span> arbitrariamente, <span class="math inline">\(s\in S\)</span>.</li>
<li>Parametros: tamaño del paso <span class="math inline">\(\alpha\in(0,1]\)</span>, un entero positivo <span class="math inline">\(n\)</span></li>
<li><p>Todas las medidas de almacenamiento y acceso (Para <span class="math inline">\(S_t\)</span> y <span class="math inline">\(R_t\)</span>) pueden tomar indices sobre <span class="math inline">\(n\)</span></p></li>
<li>Repetimos (para cada episodio):
<ul>
<li>Iniciamos y almcenamos <span class="math inline">\(S_0\neq\)</span> terminal.</li>
<li><span class="math inline">\(T\leftarrow\infty\)</span></li>
<li>Para <span class="math inline">\(t=0,1,2,...\)</span>
<ul>
<li>Si <span class="math inline">\(t&lt;T\)</span>, entonces:
<ul>
<li>Tomamos una acción de acuerdo a <span class="math inline">\(\pi(\cdot|S_t)\)</span></li>
<li>Observamos y almacenamos la siguiente recompensa comno <span class="math inline">\(R_{t+1}\)</span> y el proximo estado como <span class="math inline">\(S_{t+1}\)</span></li>
<li>Si <span class="math inline">\(S_{t+1}\)</span> es terminal, entonces <span class="math inline">\(T\leftarrow t+1\)</span></li>
</ul></li>
<li><span class="math inline">\(\tau\leftarrow t-n-1\)</span> (<span class="math inline">\(\tau\)</span> es el tiempo cuya estimación del estado se está actualizando).</li>
<li>Si <span class="math inline">\(\tau\ge 0\)</span>
<ul>
<li><span class="math inline">\(G \leftarrow \sum_{i=\tau+1}^{min(\tau+n,T)}\gamma^{i-\tau-1}R_i\)</span></li>
<li>Si <span class="math inline">\(\tau+n&lt;T\)</span>, entonces: <span class="math inline">\(G\leftarrow G+\gamma^nV(S_{\tau+n})\)</span></li>
<li><span class="math inline">\(V(S_\tau)\leftarrow V(S_\tau)+\alpha[G-V(S_{\tau})]\)</span></li>
</ul></li>
<li>Hasta que <span class="math inline">\(\tau=T-1\)</span></li>
</ul></li>
</ul></li>
</ul>
<div class="figure">
<img src="~/Reinforcement-learning/62.png" alt="" />
<p class="caption"></p>
</div>


</div>
</div>



<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="aprendizaje-por-diferencia-temporal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="software-tools.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Analisis_y_Medida/edit/master/bookdown/204-capitulo6.Rmd",
"text": "Edit"
},
"download": ["Reinforcement Learning.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
