<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Reinforcement-learning/" />
  <meta property="og:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Reinforcement-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2019-02-04">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduccion.html">
<link rel="next" href="software-tools.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#ejemplos"><i class="fa fa-check"></i><b>1.2</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#elementos-del-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> Elementos del Reinforcement Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#limitaciones-y-alcance"><i class="fa fa-check"></i><b>1.4</b> Limitaciones y alcance</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#un-ejemplo-clasico-tres-en-linea."><i class="fa fa-check"></i><b>1.5</b> Un ejemplo clásico: tres en linea.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>2</b> Modelos Lineales</a><ul>
<li class="chapter" data-level="2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal</a></li>
<li class="chapter" data-level="2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros-del-modelo"><i class="fa fa-check"></i><b>2.2</b> Estimación de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>2.3</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="2.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelo-de-regresion-lineal-en-notacion-matricial"><i class="fa fa-check"></i><b>2.4</b> Modelo de Regresión Lineal en notación matricial</a></li>
<li class="chapter" data-level="2.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros"><i class="fa fa-check"></i><b>2.5</b> Estimación de los Parámetros</a></li>
<li class="chapter" data-level="2.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-ajustados-y-residuos"><i class="fa fa-check"></i><b>2.6</b> Valores Ajustados y Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#observaciones"><i class="fa fa-check"></i><b>2.7</b> Observaciones</a></li>
<li class="chapter" data-level="2.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generalizados-glm"><i class="fa fa-check"></i><b>2.8</b> Modelos Lineales Generalizados (GLM)</a><ul>
<li class="chapter" data-level="2.8.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#componentes-de-un-modelo-lineal-generalizado-glm"><i class="fa fa-check"></i><b>2.8.1</b> Componentes de un modelo lineal generalizado (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generelizados-para-datos-binarios"><i class="fa fa-check"></i><b>2.9</b> Modelos Lineales Generelizados para datos binarios</a></li>
<li class="chapter" data-level="2.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-logistica"><i class="fa fa-check"></i><b>2.10</b> Regresión Logística</a></li>
<li class="chapter" data-level="2.11" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-probit"><i class="fa fa-check"></i><b>2.11</b> Regresión Probit</a></li>
<li class="chapter" data-level="2.12" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-de-poisson"><i class="fa fa-check"></i><b>2.12</b> Regresión de Poisson</a></li>
<li class="chapter" data-level="2.13" data-path="modelos-lineales.html"><a href="modelos-lineales.html#funciones-link-mas-usadas"><i class="fa fa-check"></i><b>2.13</b> Funciones link más usadas</a></li>
<li class="chapter" data-level="2.14" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-datos-continuos"><i class="fa fa-check"></i><b>2.14</b> Modelos de datos continuos</a></li>
<li class="chapter" data-level="2.15" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-de-datoa-discretos"><i class="fa fa-check"></i><b>2.15</b> Modelos de datoa discretos</a></li>
<li class="chapter" data-level="2.16" data-path="modelos-lineales.html"><a href="modelos-lineales.html#parametro-de-dispersion"><i class="fa fa-check"></i><b>2.16</b> Parámetro de dispersión</a></li>
<li class="chapter" data-level="2.17" data-path="modelos-lineales.html"><a href="modelos-lineales.html#sobredispersion"><i class="fa fa-check"></i><b>2.17</b> Sobredispersión</a></li>
<li class="chapter" data-level="2.18" data-path="modelos-lineales.html"><a href="modelos-lineales.html#maxima-verosimilitud"><i class="fa fa-check"></i><b>2.18</b> Máxima Verosimilitud</a></li>
<li class="chapter" data-level="2.19" data-path="modelos-lineales.html"><a href="modelos-lineales.html#metodo-de-scoring-fisher"><i class="fa fa-check"></i><b>2.19</b> Método de Scoring Fisher</a></li>
<li class="chapter" data-level="2.20" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-del-parametro-de-dispersion"><i class="fa fa-check"></i><b>2.20</b> Estimación del Parámetro de Dispersión</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelos-lineales" class="section level1">
<h1><span class="header-section-number">Capítulo 2</span> Modelos Lineales</h1>
<p>En estadística, el término modelo lineal es usado en diferentes maneras de acuerdo al contexto. La manera más frecuente es en conexión con modelos de regresión y el término a menudo se toma como un sinónimo del modelo de regresión lineal.</p>
<div id="regresion-lineal" class="section level2">
<h2><span class="header-section-number">2.1</span> Regresión lineal</h2>
<p>Permite determinar el grado de dependencia de las series de valores X e Y, prediciendo el valor y estimado que se obtendría para un valor x que no está en la distribución.</p>
<p>El modelo de regresión lineal simple tiene la siguiente expresión:</p>
<p><span class="math display">\[Y= \beta_0 + \beta_1 X + \varepsilon\]</span></p>
<p>En esta expresión estamos admitiendo que todos los factores o causas que influyen en la variable respuesta Y pueden dividirse en dos grupos: el primero contiene a una variable explicativa X y el segundo incluye un conjunto amplio de factores no controlados que englobaremos bajo el nombre de perturbación o error aleatorio, <span class="math inline">\(\varepsilon\)</span>, que provoca que la dependencia entre las variables dependiente e independiente no sea perfecta, sino que está sujeta a incertidumbre.</p>
<p>Lo que sería deseable en un modelo de regresión es que estos errores aleatorios sean en media cero para cualquier valor x de X, es decir, <span class="math inline">\(E[\varepsilon/X = x] = E[\varepsilon]=0\)</span>, y por lo tanto:</p>
<p><span class="math display">\[E [Y /X = x] = \beta_0 + \beta_1x + E [\varepsilon/X = x] = \beta_0 + \beta_1x\]</span></p>
<p>De la expresión anterior se observa:</p>
<ul>
<li>La media de <span class="math inline">\(Y\)</span>, para un valor fijo <span class="math inline">\(x\)</span>, varía linealmente con <span class="math inline">\(x\)</span>.</li>
<li><p>Para un valor <span class="math inline">\(x\)</span> se predice un valor en <span class="math inline">\(Y\)</span> dado por <span class="math inline">\(\hat y = E[Y /X = x] = \beta_0 + \beta_1 x\)</span>, por lo que el modelo de predicción puede expresarse también como <span class="math inline">\(\hat Y = \beta_0 + \beta_1 X\)</span>.</p></li>
<li><p>El parámetro <span class="math inline">\(\beta_0\)</span> es la ordenada al origen del modelo (punto de corte con el eje <span class="math inline">\(Y\)</span>) y <span class="math inline">\(\beta_1\)</span> la pendiente, que puede interpretarse como el incremento de la variable dependiente por cada incremento en una unidad de la variable independiente. Estos par??metros son desconocidos y habrá que estimarlos de cara a realizar predicciones.</p></li>
</ul>
<p>Además de la hipótesis establecida sobre los errores de que en media han de ser cero, se establecen las siguientes hipótesis:</p>
<p>Para hacer una estimación del modelo de regresión lineal simple, trataremos de buscar una recta de la forma:</p>
<ol start="2" style="list-style-type: decimal">
<li><p>La varianza de <span class="math inline">\(\varepsilon\)</span> es constante para cualquier valor de <span class="math inline">\(x\)</span>, es decir, <span class="math inline">\(Var(\varepsilon/X = x) = \sigma^ 2\)</span></p></li>
<li><p>La distribución de <span class="math inline">\(\varepsilon\)</span> es normal, de media 0 y desviación <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Los errores asociados a los valores de <span class="math inline">\(Y\)</span> son independientes unos de otros.</p></li>
</ol>
<p>En consecuencia, la distribución de <span class="math inline">\(Y\)</span> para <span class="math inline">\(x\)</span> fijo es normal, con varianza constante <span class="math inline">\(\sigma^ 2\)</span>, y media que var??a linealmente con <span class="math inline">\(x\)</span>, dada por <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. Adem??s los valores de <span class="math inline">\(Y\)</span> son independientes entre sí.</p>
</div>
<div id="estimacion-de-los-parametros-del-modelo" class="section level2">
<h2><span class="header-section-number">2.2</span> Estimación de los parámetros del modelo</h2>
<p>Partimos de una muestra de valores de X e Y medidos sobre n individuos: <span class="math inline">\((x_1, y_1),(x_2, y_2), ...,(x_n,y_n)\)</span>, y queremos estimar valores en Y según el modelo <span class="math display">\[\hat Y=  \beta_0 +  \beta_1 X\]</span>. Debemos encontrar entonces de entre todas las rectas la que mejor se ajuste a los datos observados, Para un valor <span class="math inline">\(x_i\)</span>, el modelo estima un valor en Y igual a <span class="math inline">\(\hat y_i = \beta_0 + \beta_1 x_i\)</span> y el valor observado en Y es igual a <span class="math inline">\(y_i\)</span>, con lo cual el error de estimación en ese caso vendría dado por <span class="math inline">\(e_i = y_i - \hat y_i = y_i - (\beta_0 + \beta_1 x_i)\)</span>.Entonces tomaremos como estimaciones de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> , que notamos por <span class="math inline">\(\hat \beta_0\)</span> y <span class="math inline">\(\hat \beta_1\)</span> , aquellos valores que hagan mínima la suma de los errores al cuadrado, que viene dada por:</p>
<p><span class="math display">\[SSE= \sum_{i=1}^{n} e_i ^2= \sum_{i=1}^{n} (y_i - \hat y_1)^ 2 = (y_i - (\beta_0 + \beta_1 x_1 ))^2\]</span> Al método de estimación se le llame método de mínimos cuadrados. La solución se obtiene, derivando SSE con respecto a <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> e igualando a 0. Los estimadores resultan:</p>
<p><span class="math display">\[\hat \beta_1 = \frac{SS_{xy}}{SS_{xx}}\]</span> <span class="math display">\[\hat \beta_0 = \bar y - \hat \beta_1 \bar x \]</span></p>
<p>Donde <span class="math inline">\(SS_{xy}\)</span> y <span class="math inline">\(SS_{xx}\)</span> son:</p>
<p><span class="math display">\[SS{xy}= \sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)= \sum_{i=1}^{n}x_i y_i - n \bar x \bar y ,\]</span></p>
<p><span class="math display">\[SS_{xx}= \sum_{i=1}^{n}(x_i - \bar x)^2= \sum_{i=1}^{n} x_i ^2 - n \bar x ^2= n \sigma_x ^2\]</span></p>
<p>Luelo la recta de regresión lineal de <span class="math inline">\(Y\)</span> sobre <span class="math inline">\(X\)</span> es, <span class="math inline">\(\hat Y = \hat \beta_0 - \hat \beta_1 X\)</span></p>
<p><span class="math display">\[\hat Y= \hat \beta_0 + \hat \beta_1 X\]</span></p>
<p>El siguiente parámetro a estimar en el modelo es la varianza de los errores (<span class="math inline">\(\sigma^2\)</span>). A su estimador se le denomina varianza residual y viene dada por:</p>
<p><span class="math display">\[\hat s_R ^2 = \frac {SSE}{n-2}= \frac{\sum_{i=1}^{n} e_i}{n-2}= \frac{SS_{yy}- \hat \beta_1 SS_{xy}}{n-2}\]</span></p>
</div>
<div id="regresion-lineal-multiple" class="section level2">
<h2><span class="header-section-number">2.3</span> Regresión Lineal Múltiple</h2>
<p>El modelo de regresión lineal múltiple es uno de los modelos más utilizados entre todos los modelos estadísticos. En la mayoría de las situaciones prácticas en las que se quiere explicar una variable continua Y se dispone de muchas potenciales variables predictoras. Usualmente, el modelo de regresión lineal simple provee una descripción inadecuada de la respuesta ya que suele suceder que son muchas las variables que ayudan a explicar la respuesta y la afectan de formas distintas e importantes. Entonces es necesario trabajar con modelos más complejos, que contengan variables predictoras adicionales, para proporcionar predicciones más precisas y colaborar en la cuanti􏰀cación del vínculo entre ellas. En este sentido, el modelo de regresión múltiple es una extensión natural del modelo de regresión lineal simple, aunque presenta características propias que es de interés estudiar en detalle. El modelo de regresión múltiple se puede utilizar tanto para datos observacionales como para estudios controlados a partir de ensayos aleatorizados o experimentales.</p>
<p>El modelo de regresión lineal múltiple es un modelo para la variable aleatoria <span class="math inline">\(Y\)</span> cuando se conocen <span class="math inline">\(X_1, X_2, \cdots, X_{p-1}\)</span> las variables regresoras. El modelo es</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1X_{i1}+ \beta_2 X_{i2}+ \cdots + \beta_{p-1}X_{ip-1}+ \varepsilon_i,\]</span></p>
<p>donde <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_{p-1}\)</span> son parámetros númericos desconocidos, <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1}\)</span> son las variables independientes o predictoras en la i-ésima posición, con <span class="math inline">\(1 ≤ i ≤ n\)</span>, <span class="math inline">\(n\)</span> es el tamaño de muestra, <span class="math inline">\(Y_i\)</span> es la variable dependiente o variable de respuesta en la i-ésima posición y <span class="math inline">\(\varepsilon_i\)</span> es el error en esa misma i-ésima posición. Supongamos que:</p>
<p><span class="math inline">\(\varepsilon_i ∼ N(0, \sigma^2)\)</span>, <span class="math inline">\(1≤i≤n\)</span>, independientes entre sí, es decir,</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen media cero, <span class="math inline">\(E(\varepsilon_i)=0\)</span>.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen todos la misma varianza desconocida que llamaremos <span class="math inline">\(\sigma^2\)</span> y que es el otro parámetro del modelo, <span class="math inline">\(Var (\varepsilon_i) = \sigma^2\)</span>.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen distribución normal.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> son independientes entre sí, e independientes de las covariables <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1}\)</span>.</p>
<p>Si definimos <span class="math inline">\(X_{i0}=1\)</span>, para todo <span class="math inline">\(i\)</span>, podemos reescribir el modelo de la siguiente manera</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1X_{i1}+ \beta_2 X_{i2}+ \cdots + \beta_{p-1}X_{ip-1}+ \varepsilon_i= \sum_{j=0}^{p-1} \beta_jX_{ij}+\varepsilon_i .\]</span> Del hecho de que los <span class="math inline">\(\varepsilon\)</span> son independientes y tienen distribución <span class="math inline">\(N(0,\sigma^2)\)</span> se deduce que condicional a <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1},Y_i ∼ N(\sum_{j=0}^{p-1} \beta_jX_{ij}, \sigma^2)\)</span>, independientes entre sí.</p>
</div>
<div id="modelo-de-regresion-lineal-en-notacion-matricial" class="section level2">
<h2><span class="header-section-number">2.4</span> Modelo de Regresión Lineal en notación matricial</h2>
<p>Ahora presentaremos el modelo en notación matricial. Definimos:</p>
<p><span class="math inline">\(Y=\begin{bmatrix}Y_1 \\ Y_2\\ \vdots \\ Y_n \end{bmatrix}_{n\times 1}\)</span> <span class="math inline">\(X= \begin{bmatrix} 1 &amp;X_{11} &amp; X_{12}&amp; \cdots &amp; X_{1p-1} \\ 1 &amp;X_{21} &amp; X_{22}&amp; \cdots &amp; X_{2p-1} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots&amp; \vdots \\ 1&amp; X_{n1}&amp; X_{n2} &amp; \cdots &amp; X_{np-1}\end{bmatrix}_{n\times p}\)</span> <span class="math inline">\(\beta=\begin{bmatrix} \beta_0 \\ \beta_1\\ \vdots \\ \beta_p-1 \end{bmatrix}_{p\times 1}\)</span> <span class="math inline">\(\varepsilon = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n\end{bmatrix}_{n\times 1}\)</span></p>
<p>Así, el modelo de forma matricial se escribe de la siguiente manera:</p>
<p><span class="math display">\[Y= X \beta + \varepsilon\]</span> Donde,</p>
<p><span class="math inline">\(Y\)</span> es un vector de respuestas, <span class="math inline">\(\beta\)</span> es un vector de parámetros, <span class="math inline">\(X\)</span> es una matriz de covariables, <span class="math inline">\(\varepsilon\)</span> es un vector de variables aleatorias normales independientes con esperanza <span class="math inline">\(E (\varepsilon) = 0\)</span> y matriz de varianzas y covarianzas,</p>
<p><span class="math display">\[Var(\varepsilon)= \begin{bmatrix}\sigma^2 &amp;0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma^2 &amp; \cdots &amp;0\\ \vdots &amp; \vdots&amp; \ddots&amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp;\sigma^2 \end{bmatrix}= \sigma^2I\]</span></p>
<p>Tomando condicional a las variables <span class="math inline">\(X\)</span>, la esperanza de <span class="math inline">\(Y\)</span> resulta ser</p>
<p><span class="math display">\[E(Y/X)=X\beta\]</span> y la matriz de covarianza de las <span class="math inline">\(Y\)</span> resulta ser la misma que la de <span class="math inline">\(\varepsilon\)</span></p>
<p><span class="math inline">\(Var(Y/X)= \sigma^2 I\)</span></p>
</div>
<div id="estimacion-de-los-parametros" class="section level2">
<h2><span class="header-section-number">2.5</span> Estimación de los Parámetros</h2>
<p>Usamos el método de mínimos cuadrados para ajustar el modelo. O sea, de􏰀nimos la siguiente función</p>
<p><span class="math display">\[g(b_0, b_1, \cdots, b_{p-1})= \sum_{i=1}^n (Y_i- b_0X_{i0} - b_1X_{i1} - b_2 X_{i2} - \cdots - b_{p-1}X_{ip-1})^2\]</span> y los estimadores <span class="math inline">\(\hat \beta_0, \hat \beta_1, \cdots, \hat \beta_{p-1}\)</span> serán aquellos valores de <span class="math inline">\(b_o, b_1, \cdots, b_{p-1}\)</span> que minimicen a g. Los llamaremos estimadores de mínimos cuadrados y denotaremos el vector coeficiente como <span class="math inline">\(\hat \beta\)</span></p>
<p><span class="math inline">\(\hat \beta=\begin{bmatrix} \hat \beta_0 \\ \hat \beta_1\\ \vdots \\\hat \beta_p-1 \end{bmatrix}_{p\times 1}\)</span></p>
<p>Las ecuaciones de mínimos cuadrados normales para el modelo de regresión lineal general son:</p>
<p><span class="math display">\[X^t X \hat \beta= X^t Y\]</span></p>
<p>donde <span class="math inline">\(X^t\)</span> quiere decir la matriz traspuesta. Los estimadores de mínimos cuadrados son:</p>
<p><span class="math display">\[\hat \beta= (X^t X)^{-1} X^t Y \]</span> Observaciones</p>
<p>-Para encontrar los estimadores de β no se necesita que los errores sean normales. -En el caso de la regresión lineal, los estimadores de mínimos cuadrados de los betas coinciden también con los estimadores de máxima verosimilitud para el modelo antes descripto, es decir, cuando se asume normalidad de los errores.</p>
</div>
<div id="valores-ajustados-y-residuos" class="section level2">
<h2><span class="header-section-number">2.6</span> Valores Ajustados y Residuos</h2>
<p>Denotemos al vector de valores ajustados <span class="math inline">\(\hat Y_i\)</span> por <span class="math inline">\(\hat Y\)</span> y al vector de residuos <span class="math inline">\(e_i= Y_i - \hat Y_i\)</span> por <span class="math inline">\(e\)</span></p>
<p><span class="math inline">\(\hat Y=\begin{bmatrix}\hat Y_1 \\ \hat Y_2\\ \vdots \\ \hat Y_n \end{bmatrix}_{n\times 1}\)</span> <span class="math inline">\(e=\begin{bmatrix}e_1 \\ e_2\\ \vdots \\ e_n \end{bmatrix}_{n\times 1}\)</span></p>
<p>Los valores ajustados se calculan de la siguiente manera</p>
<p><span class="math display">\[\hat Y = X\hat \beta= X(X^t X)^{-1} X^t Y\]</span></p>
<p>Los residuos se escriben matricialmente como:</p>
<p><span class="math display">\[ e= Y - \hat Y = Y - X \hat \beta = Y -X(X^tX)^{-1} X^t Y= (I- X(X^tX)^{-1} X^t)Y\]</span> Renombraremos <span class="math inline">\(H= X(X^tX)^{-1} X^t \in \mathbb{R}^{n \times n}\)</span>.</p>
<p>Tenemos que</p>
<p><span class="math inline">\(\hat Y= HY\)</span> y <span class="math inline">\(e=(I-H)Y\)</span></p>
<p>La matriz de varianzas de los residuos es:</p>
<p><span class="math display">\[ Var(e)= \sigma^2 (I-H)\]</span></p>
</div>
<div id="observaciones" class="section level2">
<h2><span class="header-section-number">2.7</span> Observaciones</h2>
<ol style="list-style-type: decimal">
<li><p>(Residuos): El modelo de regresión lineal impone que los errores <span class="math inline">\(\varepsilon_i\)</span> sean independientes, normales y tengan todos la misma varianza. Los errores no son observables, los residuos <span class="math inline">\(ei\)</span>, que son el correlato empírico de los errores, son observables. Sin embargo, los residuos no son independientes entre sí y sus varianzas no son iguales.</p></li>
<li><p>(Teórica): <span class="math inline">\(H\)</span> y <span class="math inline">\(I-H\)</span> son matrices de proyección (es decir, <span class="math inline">\(H^2=H\)</span> y <span class="math inline">\((I-H)^2=I-H\)</span>). <span class="math inline">\(H\)</span> proyecta el subespacio de <span class="math inline">\(\mathbb{R}^n\)</span> generado por las columnas de <span class="math inline">\(X\)</span>.</p></li>
</ol>
</div>
<div id="modelos-lineales-generalizados-glm" class="section level2">
<h2><span class="header-section-number">2.8</span> Modelos Lineales Generalizados (GLM)</h2>
<div id="componentes-de-un-modelo-lineal-generalizado-glm" class="section level3">
<h3><span class="header-section-number">2.8.1</span> Componentes de un modelo lineal generalizado (GLM)</h3>
<p>Un modelo lineal generalizado tiene tres componentes básicos:</p>
<ul>
<li><p><strong>Componente aleatoria:</strong> Identifica la variable respuesta y su distribución de probabilidad.</p></li>
<li><p><strong>Componente sistemática:</strong> Especifica las variables explicativas (independientes o predictoras) utilizadas en la función predictora lineal.</p></li>
<li><p><strong>Función link:</strong> Es una función del valor esperado de <span class="math inline">\(Y, E(Y)\)</span>, como una combinación lineal de las variables predictoras.</p></li>
</ul>
<p><strong>Componente aleatoria</strong></p>
<p>La componente aleatoria de un GLM consiste en una variable aleatoria <span class="math inline">\(Y\)</span> con observaciones independientes <span class="math inline">\((y_1, \cdots, y_N )\)</span>. Suponemos la distribución de <span class="math inline">\(Y\)</span> en la familia exponencial natural.</p>
<p><span class="math display">\[f(y_i| \theta_i, \phi)= e^{\left( \dfrac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i,\phi) \right)},\]</span></p>
<p>donde,</p>
<ul>
<li><span class="math inline">\(\theta_i\)</span> es el parámetro natural o canónico</li>
<li><span class="math inline">\(\phi\)</span> es una parámetro adicional de dispersión o de escala</li>
<li><span class="math inline">\(a(\cdot)\)</span>, <span class="math inline">\(b(\cdot)\)</span> y <span class="math inline">\(c(\cdot)\)</span> son funciones conocidas</li>
<li>Si <span class="math inline">\(\phi\)</span> es conocido el modelo pertenece a la familia exponencial lineal</li>
<li>Si <span class="math inline">\(\phi\)</span> es desconocido el modelo es de dispersión exponencial.</li>
</ul>
<p><strong>Componente sistemática</strong></p>
<p>La coponente sistemática de un GLM especifica las variables explicativas, que entran en forma de efectos fijos en un modelo lineal, es decir, las variables <span class="math inline">\(x_j\)</span> se relacionan mediante</p>
<p><span class="math display">\[\alpha+ \beta_{1} x_1+ \cdots+ \beta_k x_k\]</span> Esta combinación lineal se denomina predictor lineal.</p>
<p>también se puede expresar como un vector <span class="math inline">\((\eta_1, \cdots, \eta_N)\)</span> tal que</p>
<p><span class="math inline">\(\eta_i = \sum_{j} \beta_j x_{ij}\)</span> con <span class="math inline">\(i=1, \cdots, N\)</span></p>
<p>donde <span class="math inline">\(x_{ij}\)</span> es el valor del j-ésimo predictor en la i-ésima posición.</p>
<p>términoo independiente <span class="math inline">\(\alpha\)</span> se obtendría con esta notación haciendo que todos los <span class="math inline">\(x_{ij}\)</span> sean igual a 1 para todos los i.</p>
<p><strong>Función Link</strong></p>
<p>La esperanza de <span class="math inline">\(Y\)</span> la denotamos por <span class="math inline">\(E(Y) = \mu\)</span>. La función link específica una función <span class="math inline">\(g(\cdot)\)</span> que relaciona <span class="math inline">\(\mu\)</span> con el predictor lineal de la siguiente manera</p>
<p><span class="math display">\[g(\mu)= \alpha + \beta_{1} x_1+ \cdots+ \beta_k x_k.\]</span></p>
<p>De esta manera la función link relaciona las componentes aleatoria y sistemática.</p>
<p>Luego, para <span class="math inline">\(i= 1, \cdots, N\)</span> tenemos que,</p>
<p><span class="math display">\[\mu_i= E(Y_i),\]</span></p>
<p><span class="math display">\[\eta_i= g(\mu_i)= \sum_{j} \beta_j x_{ij}.\]</span> Muchos datos tienen una estructura no normal. Las herramientas usuales para tratar la ausencia de normalidad eran la transformación de la variable dependiente o la adopción de métodos no paramétricos. Otra alternativa, son los modelos lineales generalizados. Los GLM permiten especificar distintos tipos de distribución de errores tales como:</p>
<ul>
<li><p><strong>Poisson</strong>: muy útiles para conteos de acontecimientos, como por ejemplo, número deheridos por accidentes de tráfico.</p></li>
<li><p><strong>Binomial</strong>: de gran utilidad para proporciones y datos de presencia/ausencia, si/no, ejemplo, tasas de mortalidad de una enfernmedad.</p></li>
<li><p><strong>Gamma</strong>: útiles con datos que muestran un coeficiente de variaciónconstante, esto es, en donde la varianza aumenta según aumenta la media de la muestra de manera constante, ejemplo, número de heridos en función del númerode siniestros.</p></li>
</ul>
<p>Además, los modelos lineales, habituales, asumen que tanto la variable dependiente como los errores del modelo siguen una distribución normal. En ocasiones, sin embargo, la variable dependiente sigue una distribución que no es continua y por tanto, los valores estimados por el modelo han de seguir el mismo tipo de distribución que los datos de partida.</p>
</div>
</div>
<div id="modelos-lineales-generelizados-para-datos-binarios" class="section level2">
<h2><span class="header-section-number">2.9</span> Modelos Lineales Generelizados para datos binarios</h2>
<p>Se define una respuesta binaria asignada de la siguiente manera 1 en caso de éxito y 0 en elfracaso, esto es, <span class="math inline">\(Y ∼ Bin(1,\pi)\)</span>. En este caso,</p>
<p><span class="math display">\[f(y|\pi)= \pi^y(1-\pi)^{1-y}\\
          =(1-\pi)\left(\dfrac{\pi}{1-\pi}\right)^y\\
          =(1-\pi)e^{y log \left( \dfrac{\pi}{1 - \pi}\right)},\]</span></p>
<p>con <span class="math inline">\(y= 0,1\)</span>.</p>
<p>El parámetro natural es <span class="math display">\[Q(\pi)=log \left( \dfrac{\pi}{1-\pi} \right) = logit(\pi),\]</span></p>
<p>en este caso tenemos</p>
<p><span class="math display">\[E(Y)= P(Y=1)=\pi(x),\]</span> dependiente de p variables explicativas o independientes <span class="math inline">\(x= (x_1, \cdots,x_p)\)</span>, luego</p>
<p><span class="math display">\[Var(Y) =\pi(x)(1-\pi(x)).\]</span></p>
</div>
<div id="regresion-logistica" class="section level2">
<h2><span class="header-section-number">2.10</span> Regresión Logística</h2>
<p>Por lo general las relaciones entre <span class="math inline">\(\pi(x)\)</span> y <span class="math inline">\(x\)</span> no son lineales, la relación habitualmente tiene forma de curva en forma sigmoidal</p>
<div class="figure">
<img src="images/sig.png" alt="Curva sigmoidal" />
<p class="caption">Curva sigmoidal</p>
</div>
<p>Representada por la fórmula:</p>
<p><span class="math display">\[\pi(x)= \dfrac{e^{\alpha+\beta x}}{1 + e^{\alpha+\beta x} },\]</span></p>
<p>llamada función logística de la que se derivan los modelos de regresión logística:</p>
<p><span class="math display">\[1- \pi(x) =1 - \dfrac{e^{\alpha+\beta x}}{1 + e^{\alpha+\beta x} } \\
= \dfrac{1}{1 + e^{\alpha+\beta x} }\]</span></p>
<p>Opererando obtenemos</p>
<p><span class="math display">\[\dfrac{\pi(x)}{1- \pi(x)}= e^{\alpha+\beta x}.\]</span></p>
<p>Despejando <span class="math inline">\(\alpha+\beta x\)</span> tenemos lo siguiente</p>
<p><span class="math display">\[log \left( \dfrac{\pi(x)}{1- \pi(x)} \right) = \alpha+\beta x .\]</span></p>
<p>La función link <span class="math inline">\(log \left( \dfrac{\pi(x)}{1- \pi(x)} \right)\)</span> de <span class="math inline">\(\pi\)</span> se denomina función logit, de modo que así se asegura que no habra problemas estructurales respecto al rango de valores de <span class="math inline">\(\pi\)</span>. El parámetro <span class="math inline">\(\beta\)</span> dtermina el rango y la velocidad de crecimiento o decrecimiento de la curva.</p>
</div>
<div id="regresion-probit" class="section level2">
<h2><span class="header-section-number">2.11</span> Regresión Probit</h2>
<p>Una idea natural es</p>
<p><span class="math display">\[\pi(x)= F(x),\]</span></p>
<p>siendo <span class="math inline">\(F\)</span> una función de distribución. Cuando <span class="math inline">\(X\)</span> es una variable aleatoria continua, la funcion de distribucion de x tiene forma de <span class="math inline">\(S\)</span>. Esto sugiere una clase de modelos de dependencia para modelos binarios.</p>
<p>Como caso particular se puede considerar el link probit que transforma probabilidades en valores estándar de la funcion de distribución normal, <span class="math inline">\(F(x)=\Phi(x)\)</span>.</p>
<p><span class="math display">\[\pi(x)= \Phi(\alpha+ \beta x) \\ \Phi^{-1}(\pi(x))= \alpha + \beta x .\]</span> Así <span class="math inline">\(\Phi^{-1}\)</span> define un modelo probit.</p>
<p>Nota: en la práctica los modelos logit y probit prodicen ajustes similares.</p>
</div>
<div id="regresion-de-poisson" class="section level2">
<h2><span class="header-section-number">2.12</span> Regresión de Poisson</h2>
<p>La regresión de Poisson es el modelo más básico adecuado para variables de respuesta de recuento, esta distribución es unimodal y su propiedad más importante es que la esperanza y la varianza son iguales a la media.</p>
<p><span class="math display">\[E(Y)= Var(Y)= \lambda\]</span></p>
<p>En el modelo GLM por lo general se una el logarítmo de la media para la función link, de este modo el modelo log-lineal de una variable explicativa <span class="math inline">\(X\)</span> se puede expresar de la siguiente manera</p>
<p><span class="math display">\[log(\lambda) = \alpha + \beta x ,\]</span></p>
<p>despejando obtenemos</p>
<p><span class="math display">\[\lambda = e^{\alpha + \beta x} =e^\alpha(e^\beta)^x .\]</span></p>
</div>
<div id="funciones-link-mas-usadas" class="section level2">
<h2><span class="header-section-number">2.13</span> Funciones link más usadas</h2>
<table>
<thead>
<tr class="header">
<th>Link</th>
<th>Fórmula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logit</td>
<td><span class="math inline">\(log\left( \dfrac{\pi}{1- \pi} \right)\)</span></td>
</tr>
<tr class="even">
<td>Probit</td>
<td><span class="math inline">\(\Phi^{-1}(\pi)\)</span></td>
</tr>
<tr class="odd">
<td>Complementario log-log (cloglog)</td>
<td><span class="math inline">\(log(-log(1- \pi))\)</span></td>
</tr>
<tr class="even">
<td>Identidad</td>
<td><span class="math inline">\(\mu\)</span></td>
</tr>
<tr class="odd">
<td>Inverso</td>
<td><span class="math inline">\(- \dfrac{1}{\mu}\)</span></td>
</tr>
<tr class="even">
<td>Logaritmo</td>
<td><span class="math inline">\(log(\mu)\)</span></td>
</tr>
<tr class="odd">
<td>Raíz</td>
<td><span class="math inline">\(\sqrt \mu\)</span></td>
</tr>
</tbody>
</table>
<p>La elección del link dependera de la familia de distribuciones, del tipo de respuestas y de la estructura de los datos.</p>
</div>
<div id="modelos-de-datos-continuos" class="section level2">
<h2><span class="header-section-number">2.14</span> Modelos de datos continuos</h2>
<p><strong>Normal</strong></p>
<ul>
<li>Distribución <span class="math inline">\(N(\mu,\sigma ^2)\)</span></li>
<li><span class="math inline">\(E(Y)= \mu\)</span></li>
<li>Función link identidad <span class="math inline">\(g(\mu)= \mu\)</span></li>
<li><span class="math inline">\(a(\phi)= \sigma^2\)</span></li>
<li><span class="math inline">\(b(\theta)= \dfrac{\theta^2}{2}\)</span></li>
<li>Otras funciones links: logarítmo, inverso.</li>
</ul>
<p><strong>Gamma</strong></p>
<ul>
<li>Distribución <span class="math inline">\(Gamma(\lambda,k)\)</span></li>
<li><span class="math inline">\(E(Y)=\dfrac{\lambda}{k}\)</span></li>
<li>Función link inverso <span class="math inline">\(g(\mu)= - \dfrac{1}{\mu}= - \dfrac{k}{\lambda}\)</span></li>
<li><span class="math inline">\(a(\phi)=\dfrac{1}{\lambda}\)</span></li>
<li><span class="math inline">\(b(\theta)= - log(-\theta)\)</span></li>
<li>Otras funciones links: identidad, logarítmo.</li>
</ul>
</div>
<div id="modelos-de-datoa-discretos" class="section level2">
<h2><span class="header-section-number">2.15</span> Modelos de datoa discretos</h2>
<p><strong>Binomial</strong></p>
<ul>
<li>Distribución <span class="math inline">\(Bin(n,\pi)\)</span></li>
<li><span class="math inline">\(E(Y)= n \pi\)</span></li>
<li>Función link logit <span class="math inline">\(g(\mu)= log\left( \dfrac{\mu}{n- \mu} \right) = log\left( \dfrac{\pi}{1- \pi} \right)\)</span></li>
<li><span class="math inline">\(a(\phi) = 1\)</span></li>
<li><span class="math inline">\(b(\theta)= n log(1 + e^\theta)\)</span></li>
<li>Otras funciones links: probit, cauchit, log, cloglog.</li>
</ul>
<p><strong>Poisson</strong></p>
<ul>
<li>Distribución <span class="math inline">\(Po(\lambda)\)</span></li>
<li><span class="math inline">\(E(Y)= \lambda\)</span></li>
<li>Función link logarítmo <span class="math inline">\(g(\lambda)= log(\lambda)\)</span></li>
<li><span class="math inline">\(a(\phi)= 1\)</span></li>
<li><span class="math inline">\(b(\theta)= e^\theta\)</span></li>
<li>Otras funciones links: identidad, raiz.</li>
</ul>
</div>
<div id="parametro-de-dispersion" class="section level2">
<h2><span class="header-section-number">2.16</span> Parámetro de dispersión</h2>
<p>Regularmente el parámetro <span class="math inline">\(a(\phi)= \dfrac{\phi}{w_i}\)</span> con <span class="math inline">\(w_i\)</span> un peso.</p>
<ul>
<li>Para datos no agrupados <span class="math inline">\(w_i=1\)</span></li>
<li>Si las variables dependientes expresan promedios <span class="math inline">\(w_i= n_i\)</span></li>
<li>Si son la suma de <span class="math inline">\(n_i\)</span> variables individuales <span class="math inline">\(w_i=\dfrac{1}{n_i}\)</span></li>
</ul>
</div>
<div id="sobredispersion" class="section level2">
<h2><span class="header-section-number">2.17</span> Sobredispersión</h2>
<p>Este fenómeno ocurre principal con distribuciones de varianza poco dúcti, como la binomial o poisson.</p>
<p>Añadiendo un parámetro de dispersión <span class="math inline">\(\phi\)</span> se modifica la varianza</p>
<p><span class="math display">\[V[y]= a(\phi)\dfrac{d^2b(\theta)}{d\theta^2}\]</span> para simplificar la notación renombraremos <span class="math inline">\(\dfrac{d^2b(\theta)}{d\theta^2}= b&#39;&#39;(\theta)\)</span> asi la ecuacion anterior queda de la siguiente manera:</p>
<p><span class="math display">\[V[y]= a(\phi)b&#39;&#39;(\theta)\]</span></p>
<p>Puede representar una heteregeneidad no observada o una correlación positiva entre respuestas individuales. Otra forma de llamarla es extravarianza.</p>
</div>
<div id="maxima-verosimilitud" class="section level2">
<h2><span class="header-section-number">2.18</span> Máxima Verosimilitud</h2>
<p>Definimos el logaritmo de la verosimilitud de <span class="math inline">\(\theta\)</span> para las observaciones <span class="math inline">\(y\)</span> como sigue:</p>
<p><span class="math display">\[l(\theta | y) = \sum_{i=1}^{n} \dfrac{y_i \theta_i - b(\theta_i)}{a_i(\phi)} + c(y_i, \phi)\]</span></p>
<p>El principal objetivo es la estimación de <span class="math inline">\(\beta\)</span>. El estimador de máxima verosimilitud de cada <span class="math inline">\(\beta_j\)</span> anula la derivada de <span class="math inline">\(l\)</span></p>
<p><span class="math display">\[\dfrac{\partial l}{\partial \beta _j}= \sum_{i=1}^{n} \dfrac{(y_i-\mu_i)x_{ij}}{V[y_i]g&#39;(\mu_i)}\]</span></p>
<p>En general estas ecuaciones no se pueden resolver de manera directa, lo que se hace es una aproximación por procedimientos iterativos, utilizando la esperanza matemática de la segunda derivada parcial de <span class="math inline">\(l\)</span></p>
<p><span class="math display">\[E\left[\dfrac{\partial^2l}{\partial \beta_j \partial \beta_k}\right]= \sum_{i=1}^{n} \dfrac{x_{ij}x_{ik}}{V[y_i]g&#39;(\mu_i)^2}\]</span></p>
</div>
<div id="metodo-de-scoring-fisher" class="section level2">
<h2><span class="header-section-number">2.19</span> Método de Scoring Fisher</h2>
<p>Primero recordemos el <strong>algorítmo de Newton-Raphson</strong>:</p>
<p>Es un procedimiento iterativo a partir de una estimación inicial <span class="math inline">\(\beta^0\)</span></p>
<p><span class="math display">\[\beta^{\gamma +1 } = \beta^\gamma - [D^2_\beta l(\beta^\gamma)]^{-1} D_\beta (\beta^\gamma)\]</span></p>
<p>Donde <span class="math inline">\(D_\beta (\beta^\gamma)\)</span> es el vector de las primeras dervadas de <span class="math inline">\(l\)</span> y <span class="math inline">\(D^2_\beta l(\beta^\gamma)\)</span> es la matriz de las segundas derivadas evaluadas en <span class="math inline">\(\beta^\gamma\)</span>.</p>
<p><strong>Método Scoring Fisher</strong></p>
<p>Consiste en sustituir <span class="math inline">\(D^2_\beta l(\beta^\gamma)\)</span> por su esperanza matemática</p>
<p><span class="math display">\[E\left[\dfrac{\partial^2l}{\partial \beta_j \partial \beta_k}\right]= \sum_{i=1}^{n} \dfrac{x_{ij}x_{ik}}{V[y_i]g&#39;(\mu_i)^2}\]</span></p>
<p>Es equivalente a resolver un proble de mínimos cuadrados ponderados. La sucesión <span class="math inline">\(\{ \beta^\gamma \}\)</span> converge al estimador de máxima verosimilitud de <span class="math inline">\(\beta\)</span></p>
</div>
<div id="estimacion-del-parametro-de-dispersion" class="section level2">
<h2><span class="header-section-number">2.20</span> Estimación del Parámetro de Dispersión</h2>
<p>Si el parámetro <span class="math inline">\(\phi\)</span> es desconocido se procede a utilizar una estimacion para realizar el cálculo de <span class="math inline">\(V[y_i]\)</span> de la ecuacion anterior</p>
<p>Cuando <span class="math inline">\(a_i(\phi)= \dfrac{\phi}{w_i}\)</span>, la expresión de la varianza</p>
<p><span class="math display">\[V[y_i]=a_i(\phi)b&#39;&#39;(\theta)\]</span></p>
<p>otorga un estimador consistente de <span class="math inline">\(\phi\)</span> apartir de una estimación de <span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[\hat \phi= \dfrac{1}{n-p-1} \sum_{i=1}^{n} \dfrac{w_i(y_i-\hat \mu_i)^2}{b&#39;&#39;(\hat \theta_i)}\]</span></p>

</div>
</div>



</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="introduccion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="software-tools.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Analisis_y_Medida/edit/master/bookdown/100-capitulo1.Rmd",
"text": "Edit"
},
"download": ["Reinforcement Learning.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
