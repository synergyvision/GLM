<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Reinforcement-learning/" />
  <meta property="og:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Reinforcement-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2018-12-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduccion.html">
<link rel="next" href="procesos-de-decision-de-markov-finitos.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>



<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#ejemplos"><i class="fa fa-check"></i><b>1.2</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#elementos-del-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> Elementos del Reinforcement Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#limitaciones-y-alcance"><i class="fa fa-check"></i><b>1.4</b> Limitaciones y alcance</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#un-ejemplo-clasico-tres-en-linea."><i class="fa fa-check"></i><b>1.5</b> Un ejemplo clásico: tres en linea.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>2</b> Modelos Lineales</a><ul>
<li class="chapter" data-level="2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal</a></li>
<li class="chapter" data-level="2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros-del-modelo"><i class="fa fa-check"></i><b>2.2</b> Estimación de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>2.3</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="2.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelo-de-regresion-lineal-en-notacion-matricial"><i class="fa fa-check"></i><b>2.4</b> Modelo de Regresión Lineal en notación matricial</a></li>
<li class="chapter" data-level="2.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros"><i class="fa fa-check"></i><b>2.5</b> Estimación de los Parámetros</a></li>
<li class="chapter" data-level="2.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-ajustados-y-residuos"><i class="fa fa-check"></i><b>2.6</b> Valores Ajustados y Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#observaciones"><i class="fa fa-check"></i><b>2.7</b> Observaciones</a></li>
<li class="chapter" data-level="2.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generalizados-glm"><i class="fa fa-check"></i><b>2.8</b> Modelos Lineales Generalizados (GLM)</a><ul>
<li class="chapter" data-level="2.8.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#componentes-de-un-modelo-lineal-generalizado-glm"><i class="fa fa-check"></i><b>2.8.1</b> Componentes de un modelo lineal generalizado (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#un-problema-de-bandido-k-brasos"><i class="fa fa-check"></i><b>2.9</b> Un problema de bandido k-brasos</a></li>
<li class="chapter" data-level="2.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#mtodos-de-accin-valor"><i class="fa fa-check"></i><b>2.10</b> M??todos de acci??n valor</a></li>
<li class="chapter" data-level="2.11" data-path="modelos-lineales.html"><a href="modelos-lineales.html#pruebas-sobre-el-problema-del-bandido-de-10-brasos"><i class="fa fa-check"></i><b>2.11</b> Pruebas sobre el problema del bandido de 10 brasos</a></li>
<li class="chapter" data-level="2.12" data-path="modelos-lineales.html"><a href="modelos-lineales.html#aplicacin-progresiva"><i class="fa fa-check"></i><b>2.12</b> Aplicaci??n progresiva</a></li>
<li class="chapter" data-level="2.13" data-path="modelos-lineales.html"><a href="modelos-lineales.html#problemas-no-estacionarios"><i class="fa fa-check"></i><b>2.13</b> Problemas no estacionarios</a></li>
<li class="chapter" data-level="2.14" data-path="modelos-lineales.html"><a href="modelos-lineales.html#ptimos-valores-iniciales"><i class="fa fa-check"></i><b>2.14</b> ??ptimos valores iniciales</a></li>
<li class="chapter" data-level="2.15" data-path="modelos-lineales.html"><a href="modelos-lineales.html#cota-superior-de-confianza-en-la-seleccin-de-acciones-csc"><i class="fa fa-check"></i><b>2.15</b> Cota superior de confianza en la selecci??n de acciones (CSC)</a></li>
<li class="chapter" data-level="2.16" data-path="modelos-lineales.html"><a href="modelos-lineales.html#algoritmo-del-gradiente"><i class="fa fa-check"></i><b>2.16</b> Algoritmo del gradiente</a></li>
<li class="chapter" data-level="2.17" data-path="modelos-lineales.html"><a href="modelos-lineales.html#investigacin-asociativa"><i class="fa fa-check"></i><b>2.17</b> Investigaci??n asociativa</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html"><i class="fa fa-check"></i><b>3</b> Procesos de decision de Markov finitos</a><ul>
<li class="chapter" data-level="3.1" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#el-agente-un-interface-del-entorno"><i class="fa fa-check"></i><b>3.1</b> El agente, Un interface del entorno</a></li>
<li class="chapter" data-level="3.2" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#metas-y-recompensas."><i class="fa fa-check"></i><b>3.2</b> Metas y recompensas.</a></li>
<li class="chapter" data-level="3.3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#retornos-y-episodios"><i class="fa fa-check"></i><b>3.3</b> Retornos y episodios</a></li>
<li class="chapter" data-level="3.4" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#notacion-unificada-tanto-para-tareas-episodicas-y-continuas."><i class="fa fa-check"></i><b>3.4</b> Notación unificada tanto para tareas episodicas y continuas.</a></li>
<li class="chapter" data-level="3.5" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#politicas-y-funciones-de-valor"><i class="fa fa-check"></i><b>3.5</b> Políticas y funciones de valor</a></li>
<li class="chapter" data-level="3.6" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#funciones-de-valor-y-politicas-optimas"><i class="fa fa-check"></i><b>3.6</b> Funciones de valor y políticas optimas</a></li>
<li class="chapter" data-level="3.7" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#optimalidad-y-aproximacion"><i class="fa fa-check"></i><b>3.7</b> Optimalidad y aproximación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html"><i class="fa fa-check"></i><b>4</b> Programación dinámica</a><ul>
<li class="chapter" data-level="4.1" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#politicas-evaluadas-prediccion"><i class="fa fa-check"></i><b>4.1</b> Políticas evaluadas (Predicción)</a></li>
<li class="chapter" data-level="4.2" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#mejora-de-las-politicas"><i class="fa fa-check"></i><b>4.2</b> Mejora de las políticas</a></li>
<li class="chapter" data-level="4.3" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-de-politicas"><i class="fa fa-check"></i><b>4.3</b> Iteración de políticas</a></li>
<li class="chapter" data-level="4.4" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-de-valores"><i class="fa fa-check"></i><b>4.4</b> Iteración de valores</a></li>
<li class="chapter" data-level="4.5" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#programacion-dinamica-asincronica."><i class="fa fa-check"></i><b>4.5</b> Programación dinámica asincrónica.</a></li>
<li class="chapter" data-level="4.6" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#iteracion-generalizada-de-politicas"><i class="fa fa-check"></i><b>4.6</b> Iteración generalizada de políticas</a></li>
<li class="chapter" data-level="4.7" data-path="programacion-dinamica.html"><a href="programacion-dinamica.html#eficiencia-de-la-programacion-dinamica"><i class="fa fa-check"></i><b>4.7</b> Eficiencia de la programación dinámica</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html"><i class="fa fa-check"></i><b>5</b> Métodos de Montecarlo</a><ul>
<li class="chapter" data-level="5.1" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#prediccion-con-monte-carlo"><i class="fa fa-check"></i><b>5.1</b> Predicción con Monte Carlo</a></li>
<li class="chapter" data-level="5.2" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#estimacion-de-monte-carlo-de-los-valores-de-accion"><i class="fa fa-check"></i><b>5.2</b> Estimación de Monte Carlo de los Valores de Acción</a></li>
<li class="chapter" data-level="5.3" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control"><i class="fa fa-check"></i><b>5.3</b> Métodos de Monte Carlo con control</a></li>
<li class="chapter" data-level="5.4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control-sin-iniciar-exploracion"><i class="fa fa-check"></i><b>5.4</b> Métodos de Monte Carlo con control sin iniciar exploración</a></li>
<li class="chapter" data-level="5.5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#predicciones-no-politicas-via-muestreos-de-importancia."><i class="fa fa-check"></i><b>5.5</b> Predicciones no políticas via muestreos de importancia.</a></li>
<li class="chapter" data-level="5.6" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#implementacion-incremental"><i class="fa fa-check"></i><b>5.6</b> Implementación incremental</a></li>
<li class="chapter" data-level="5.7" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#monte-carlo-no-politico-con-control"><i class="fa fa-check"></i><b>5.7</b> Monte Carlo no político con control</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html"><i class="fa fa-check"></i><b>6</b> Aprendizaje por Diferencia Temporal</a><ul>
<li class="chapter" data-level="6.1" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#prediccion"><i class="fa fa-check"></i><b>6.1</b> Predicción</a></li>
<li class="chapter" data-level="6.2" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#ventajas-de-los-metodos-de-prediccion-de-td"><i class="fa fa-check"></i><b>6.2</b> Ventajas de los métodos de predicción de TD</a></li>
<li class="chapter" data-level="6.3" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#calidad-de-td0"><i class="fa fa-check"></i><b>6.3</b> Calidad de TD(0)</a></li>
<li class="chapter" data-level="6.4" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-td-politico-con-control"><i class="fa fa-check"></i><b>6.4</b> Sarsa: TD político con control</a></li>
<li class="chapter" data-level="6.5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#q-learning-td-no-politico-con-control"><i class="fa fa-check"></i><b>6.5</b> Q-Learning: TD no político con control</a></li>
<li class="chapter" data-level="6.6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-esparada"><i class="fa fa-check"></i><b>6.6</b> Sarsa esparada</a></li>
<li class="chapter" data-level="6.7" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sesgo-de-maximizacion-y-doble-aprendizaje"><i class="fa fa-check"></i><b>6.7</b> Sesgo de maximización y doble aprendizaje</a></li>
<li class="chapter" data-level="6.8" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#juegos-afterstates-y-otros-casos-especiales"><i class="fa fa-check"></i><b>6.8</b> Juegos, afterstates y otros casos especiales</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html"><i class="fa fa-check"></i><b>7</b> Bootstrapping en <span class="math inline">\(n\)</span>-pasos</a><ul>
<li class="chapter" data-level="7.1" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html#prediccion-de-td-en-n-pasos"><i class="fa fa-check"></i><b>7.1</b> Predicción de TD en <span class="math inline">\(n\)</span> pasos</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="modelos-lineales" class="section level1">
<h1><span class="header-section-number">Capítulo 2</span> Modelos Lineales</h1>
<p>En estadística, el término modelo lineal es usado en diferentes maneras de acuerdo al contexto. La manera más frecuente es en conexión con modelos de regresión y el término a menudo se toma como un sinónimo del modelo de regresión lineal.</p>
<div id="regresion-lineal" class="section level2">
<h2><span class="header-section-number">2.1</span> Regresión lineal</h2>
<p>Permite determinar el grado de dependencia de las series de valores X e Y, prediciendo el valor y estimado que se obtendría para un valor x que no está en la distribución.</p>
<p>El modelo de regresión lineal simple tiene la siguiente expresión:</p>
<p><span class="math display">\[Y= \beta_0 + \beta_1 X + \varepsilon\]</span></p>
<p>En esta expresión estamos admitiendo que todos los factores o causas que influyen en la variable respuesta Y pueden dividirse en dos grupos: el primero contiene a una variable explicativa X y el segundo incluye un conjunto amplio de factores no controlados que englobaremos bajo el nombre de perturbación o error aleatorio, <span class="math inline">\(\varepsilon\)</span>, que provoca que la dependencia entre las variables dependiente e independiente no sea perfecta, sino que está sujeta a incertidumbre.</p>
<p>Lo que sería deseable en un modelo de regresión es que estos errores aleatorios sean en media cero para cualquier valor x de X, es decir, <span class="math inline">\(E[\varepsilon/X = x] = E[\varepsilon]=0\)</span>, y por lo tanto:</p>
<p><span class="math display">\[E [Y /X = x] = \beta_0 + \beta_1x + E [\varepsilon/X = x] = \beta_0 + \beta_1x\]</span></p>
<p>De la expresión anterior se observa:</p>
<ul>
<li>La media de <span class="math inline">\(Y\)</span>, para un valor fijo <span class="math inline">\(x\)</span>, varía linealmente con <span class="math inline">\(x\)</span>.</li>
<li><p>Para un valor <span class="math inline">\(x\)</span> se predice un valor en <span class="math inline">\(Y\)</span> dado por <span class="math inline">\(\hat y = E[Y /X = x] = \beta_0 + \beta_1 x\)</span>, por lo que el modelo de predicción puede expresarse también como <span class="math inline">\(\hat Y = \beta_0 + \beta_1 X\)</span>.</p></li>
<li><p>El parámetro <span class="math inline">\(\beta_0\)</span> es la ordenada al origen del modelo (punto de corte con el eje <span class="math inline">\(Y\)</span>) y <span class="math inline">\(\beta_1\)</span> la pendiente, que puede interpretarse como el incremento de la variable dependiente por cada incremento en una unidad de la variable independiente. Estos par??metros son desconocidos y habrá que estimarlos de cara a realizar predicciones.</p></li>
</ul>
<p>Además de la hipótesis establecida sobre los errores de que en media han de ser cero, se establecen las siguientes hipótesis:</p>
<p>Para hacer una estimación del modelo de regresión lineal simple, trataremos de buscar una recta de la forma:</p>
<ol start="2" style="list-style-type: decimal">
<li><p>La varianza de <span class="math inline">\(\varepsilon\)</span> es constante para cualquier valor de <span class="math inline">\(x\)</span>, es decir, <span class="math inline">\(Var(\varepsilon/X = x) = \sigma^ 2\)</span></p></li>
<li><p>La distribución de <span class="math inline">\(\varepsilon\)</span> es normal, de media 0 y desviación <span class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Los errores asociados a los valores de <span class="math inline">\(Y\)</span> son independientes unos de otros.</p></li>
</ol>
<p>En consecuencia, la distribución de <span class="math inline">\(Y\)</span> para <span class="math inline">\(x\)</span> fijo es normal, con varianza constante <span class="math inline">\(\sigma^ 2\)</span>, y media que var??a linealmente con <span class="math inline">\(x\)</span>, dada por <span class="math inline">\(\beta_0 + \beta_1 x\)</span>. Adem??s los valores de <span class="math inline">\(Y\)</span> son independientes entre sí.</p>
</div>
<div id="estimacion-de-los-parametros-del-modelo" class="section level2">
<h2><span class="header-section-number">2.2</span> Estimación de los parámetros del modelo</h2>
<p>Partimos de una muestra de valores de X e Y medidos sobre n individuos: <span class="math inline">\((x_1, y_1),(x_2, y_2), ...,(x_n,y_n)\)</span>, y queremos estimar valores en Y según el modelo <span class="math display">\[\hat Y=  \beta_0 +  \beta_1 X\]</span>. Debemos encontrar entonces de entre todas las rectas la que mejor se ajuste a los datos observados, Para un valor <span class="math inline">\(x_i\)</span>, el modelo estima un valor en Y igual a <span class="math inline">\(\hat y_i = \beta_0 + \beta_1 x_i\)</span> y el valor observado en Y es igual a <span class="math inline">\(y_i\)</span>, con lo cual el error de estimación en ese caso vendría dado por <span class="math inline">\(e_i = y_i - \hat y_i = y_i - (\beta_0 + \beta_1 x_i)\)</span>.Entonces tomaremos como estimaciones de <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> , que notamos por <span class="math inline">\(\hat \beta_0\)</span> y <span class="math inline">\(\hat \beta_1\)</span> , aquellos valores que hagan mínima la suma de los errores al cuadrado, que viene dada por:</p>
<p><span class="math display">\[SSE= \sum_{i=1}^{n} e_i ^2= \sum_{i=1}^{n} (y_i - \hat y_1)^ 2 = (y_i - (\beta_0 + \beta_1 x_1 ))^2\]</span> Al método de estimación se le llame método de mínimos cuadrados. La solución se obtiene, derivando SSE con respecto a <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span> e igualando a 0. Los estimadores resultan:</p>
<p><span class="math display">\[\hat \beta_1 = \frac{SS_{xy}}{SS_{xx}}\]</span> <span class="math display">\[\hat \beta_0 = \bar y - \hat \beta_1 \bar x \]</span></p>
<p>Donde <span class="math inline">\(SS_{xy}\)</span> y <span class="math inline">\(SS_{xx}\)</span> son:</p>
<p><span class="math display">\[SS{xy}= \sum_{i=1}^{n}(x_i - \bar x)(y_i - \bar y)= \sum_{i=1}^{n}x_i y_i - n \bar x \bar y ,\]</span></p>
<p><span class="math display">\[SS_{xx}= \sum_{i=1}^{n}(x_i - \bar x)^2= \sum_{i=1}^{n} x_i ^2 - n \bar x ^2= n \sigma_x ^2\]</span></p>
<p>Luelo la recta de regresión lineal de <span class="math inline">\(Y\)</span> sobre <span class="math inline">\(X\)</span> es, <span class="math inline">\(\hat Y = \hat \beta_0 - \hat \beta_1 X\)</span></p>
<p><span class="math display">\[\hat Y= \hat \beta_0 + \hat \beta_1 X\]</span></p>
<p>El siguiente parámetro a estimar en el modelo es la varianza de los errores (<span class="math inline">\(\sigma^2\)</span>). A su estimador se le denomina varianza residual y viene dada por:</p>
<p><span class="math display">\[\hat s_R ^2 = \frac {SSE}{n-2}= \frac{\sum_{i=1}^{n} e_i}{n-2}= \frac{SS_{yy}- \hat \beta_1 SS_{xy}}{n-2}\]</span></p>
</div>
<div id="regresion-lineal-multiple" class="section level2">
<h2><span class="header-section-number">2.3</span> Regresión Lineal Múltiple</h2>
<p>El modelo de regresión lineal múltiple es uno de los modelos más utilizados entre todos los modelos estadísticos. En la mayoría de las situaciones prácticas en las que se quiere explicar una variable continua Y se dispone de muchas potenciales variables predictoras. Usualmente, el modelo de regresión lineal simple provee una descripción inadecuada de la respuesta ya que suele suceder que son muchas las variables que ayudan a explicar la respuesta y la afectan de formas distintas e importantes. Entonces es necesario trabajar con modelos más complejos, que contengan variables predictoras adicionales, para proporcionar predicciones más precisas y colaborar en la cuanti􏰀cación del vínculo entre ellas. En este sentido, el modelo de regresión múltiple es una extensión natural del modelo de regresión lineal simple, aunque presenta características propias que es de interés estudiar en detalle. El modelo de regresión múltiple se puede utilizar tanto para datos observacionales como para estudios controlados a partir de ensayos aleatorizados o experimentales.</p>
<p>El modelo de regresión lineal múltiple es un modelo para la variable aleatoria <span class="math inline">\(Y\)</span> cuando se conocen <span class="math inline">\(X_1, X_2, \cdots, X_{p-1}\)</span> las variables regresoras. El modelo es</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1X_{i1}+ \beta_2 X_{i2}+ \cdots + \beta_{p-1}X_{ip-1}+ \varepsilon_i,\]</span></p>
<p>donde <span class="math inline">\(\beta_0, \beta_1, \cdots, \beta_{p-1}\)</span> son parámetros númericos desconocidos, <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1}\)</span> son las variables independientes o predictoras en la i-ésima posición, con <span class="math inline">\(1 ≤ i ≤ n\)</span>, <span class="math inline">\(n\)</span> es el tamaño de muestra, <span class="math inline">\(Y_i\)</span> es la variable dependiente o variable de respuesta en la i-ésima posición y <span class="math inline">\(\varepsilon_i\)</span> es el error en esa misma i-ésima posición. Supongamos que:</p>
<p><span class="math inline">\(\varepsilon_i ∼ N(0, \sigma^2)\)</span>, <span class="math inline">\(1≤i≤n\)</span>, independientes entre sí, es decir,</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen media cero, <span class="math inline">\(E(\varepsilon_i)=0\)</span>.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen todos la misma varianza desconocida que llamaremos <span class="math inline">\(\sigma^2\)</span> y que es el otro parámetro del modelo, <span class="math inline">\(Var (\varepsilon_i) = \sigma^2\)</span>.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> tienen distribución normal.</p>
<p>-Los <span class="math inline">\(\varepsilon_i\)</span> son independientes entre sí, e independientes de las covariables <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1}\)</span>.</p>
<p>Si definimos <span class="math inline">\(X_{i0}=1\)</span>, para todo <span class="math inline">\(i\)</span>, podemos reescribir el modelo de la siguiente manera</p>
<p><span class="math display">\[Y_i=\beta_0 + \beta_1X_{i1}+ \beta_2 X_{i2}+ \cdots + \beta_{p-1}X_{ip-1}+ \varepsilon_i= \sum_{j=0}^{p-1} \beta_jX_{ij}+\varepsilon_i .\]</span> Del hecho de que los <span class="math inline">\(\varepsilon\)</span> son independientes y tienen distribución <span class="math inline">\(N(0,\sigma^2)\)</span> se deduce que condicional a <span class="math inline">\(X_{i1}, X_{i2}, \cdots, X_{ip-1},Y_i ∼ N(\sum_{j=0}^{p-1} \beta_jX_{ij}, \sigma^2)\)</span>, independientes entre sí.</p>
</div>
<div id="modelo-de-regresion-lineal-en-notacion-matricial" class="section level2">
<h2><span class="header-section-number">2.4</span> Modelo de Regresión Lineal en notación matricial</h2>
<p>Ahora presentaremos el modelo en notación matricial. Definimos:</p>
<p><span class="math inline">\(Y=\begin{bmatrix}Y_1 \\ Y_2\\ \vdots \\ Y_n \end{bmatrix}_{n\times 1}\)</span> <span class="math inline">\(X= \begin{bmatrix} 1 &amp;X_{11} &amp; X_{12}&amp; \cdots &amp; X_{1p-1} \\ 1 &amp;X_{21} &amp; X_{22}&amp; \cdots &amp; X_{2p-1} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots&amp; \vdots \\ 1&amp; X_{n1}&amp; X_{n2} &amp; \cdots &amp; X_{np-1}\end{bmatrix}_{n\times p}\)</span> <span class="math inline">\(\beta=\begin{bmatrix} \beta_0 \\ \beta_1\\ \vdots \\ \beta_p-1 \end{bmatrix}_{p\times 1}\)</span> <span class="math inline">\(\varepsilon = \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \vdots \\ \varepsilon_n\end{bmatrix}_{n\times 1}\)</span></p>
<p>Así, el modelo de forma matricial se escribe de la siguiente manera:</p>
<p><span class="math display">\[Y= X \beta + \varepsilon\]</span> Donde,</p>
<p><span class="math inline">\(Y\)</span> es un vector de respuestas, <span class="math inline">\(\beta\)</span> es un vector de parámetros, <span class="math inline">\(X\)</span> es una matriz de covariables, <span class="math inline">\(\varepsilon\)</span> es un vector de variables aleatorias normales independientes con esperanza <span class="math inline">\(E (\varepsilon) = 0\)</span> y matriz de varianzas y covarianzas,</p>
<p><span class="math display">\[Var(\varepsilon)= \begin{bmatrix}\sigma^2 &amp;0 &amp; \cdots &amp; 0 \\ 0 &amp; \sigma^2 &amp; \cdots &amp;0\\ \vdots &amp; \vdots&amp; \ddots&amp; \vdots \\ 0 &amp; 0 &amp; \cdots &amp;\sigma^2 \end{bmatrix}= \sigma^2I\]</span></p>
<p>Tomando condicional a las variables <span class="math inline">\(X\)</span>, la esperanza de <span class="math inline">\(Y\)</span> resulta ser</p>
<p><span class="math display">\[E(Y/X)=X\beta\]</span> y la matriz de covarianza de las <span class="math inline">\(Y\)</span> resulta ser la misma que la de <span class="math inline">\(\varepsilon\)</span></p>
<p><span class="math inline">\(Var(Y/X)= \sigma^2 I\)</span></p>
</div>
<div id="estimacion-de-los-parametros" class="section level2">
<h2><span class="header-section-number">2.5</span> Estimación de los Parámetros</h2>
<p>Usamos el método de mínimos cuadrados para ajustar el modelo. O sea, de􏰀nimos la siguiente función</p>
<p><span class="math display">\[g(b_0, b_1, \cdots, b_{p-1})= \sum_{i=1}^n (Y_i- b_0X_{i0} - b_1X_{i1} - b_2 X_{i2} - \cdots - b_{p-1}X_{ip-1})^2\]</span> y los estimadores <span class="math inline">\(\hat \beta_0, \hat \beta_1, \cdots, \hat \beta_{p-1}\)</span> serán aquellos valores de <span class="math inline">\(b_o, b_1, \cdots, b_{p-1}\)</span> que minimicen a g. Los llamaremos estimadores de mínimos cuadrados y denotaremos el vector coeficiente como <span class="math inline">\(\hat \beta\)</span></p>
<p><span class="math inline">\(\hat \beta=\begin{bmatrix} \hat \beta_0 \\ \hat \beta_1\\ \vdots \\\hat \beta_p-1 \end{bmatrix}_{p\times 1}\)</span></p>
<p>Las ecuaciones de mínimos cuadrados normales para el modelo de regresión lineal general son:</p>
<p><span class="math display">\[X^t X \hat \beta= X^t Y\]</span></p>
<p>donde <span class="math inline">\(X^t\)</span> quiere decir la matriz traspuesta. Los estimadores de mínimos cuadrados son:</p>
<p><span class="math display">\[\hat \beta= (X^t X)^{-1} X^t Y \]</span> Observaciones</p>
<p>-Para encontrar los estimadores de β no se necesita que los errores sean normales. -En el caso de la regresión lineal, los estimadores de mínimos cuadrados de los betas coinciden también con los estimadores de máxima verosimilitud para el modelo antes descripto, es decir, cuando se asume normalidad de los errores.</p>
</div>
<div id="valores-ajustados-y-residuos" class="section level2">
<h2><span class="header-section-number">2.6</span> Valores Ajustados y Residuos</h2>
<p>Denotemos al vector de valores ajustados <span class="math inline">\(\hat Y_i\)</span> por <span class="math inline">\(\hat Y\)</span> y al vector de residuos <span class="math inline">\(e_i= Y_i - \hat Y_i\)</span> por <span class="math inline">\(e\)</span></p>
<p><span class="math inline">\(\hat Y=\begin{bmatrix}\hat Y_1 \\ \hat Y_2\\ \vdots \\ \hat Y_n \end{bmatrix}_{n\times 1}\)</span> <span class="math inline">\(e=\begin{bmatrix}e_1 \\ e_2\\ \vdots \\ e_n \end{bmatrix}_{n\times 1}\)</span></p>
<p>Los valores ajustados se calculan de la siguiente manera</p>
<p><span class="math display">\[\hat Y = X\hat \beta= X(X^t X)^{-1} X^t Y\]</span></p>
<p>Los residuos se escriben matricialmente como:</p>
<p><span class="math display">\[ e= Y - \hat Y = Y - X \hat \beta = Y -X(X^tX)^{-1} X^t Y= (I- X(X^tX)^{-1} X^t)Y\]</span> Renombraremos <span class="math inline">\(H= X(X^tX)^{-1} X^t \in \mathbb{R}^{n \times n}\)</span>.</p>
<p>Tenemos que</p>
<p><span class="math inline">\(\hat Y= HY\)</span> y <span class="math inline">\(e=(I-H)Y\)</span></p>
<p>La matriz de varianzas de los residuos es:</p>
<p><span class="math display">\[ Var(e)= \sigma^2 (I-H)\]</span></p>
</div>
<div id="observaciones" class="section level2">
<h2><span class="header-section-number">2.7</span> Observaciones</h2>
<ol style="list-style-type: decimal">
<li><p>(Residuos): El modelo de regresión lineal impone que los errores <span class="math inline">\(\varepsilon_i\)</span> sean independientes, normales y tengan todos la misma varianza. Los errores no son observables, los residuos <span class="math inline">\(ei\)</span>, que son el correlato empírico de los errores, son observables. Sin embargo, los residuos no son independientes entre sí y sus varianzas no son iguales.</p></li>
<li><p>(Teórica): <span class="math inline">\(H\)</span> y <span class="math inline">\(I-H\)</span> son matrices de proyección (es decir, <span class="math inline">\(H^2=H\)</span> y <span class="math inline">\((I-H)^2=I-H\)</span>). <span class="math inline">\(H\)</span> proyecta el subespacio de <span class="math inline">\(\mathbb{R}^n\)</span> generado por las columnas de <span class="math inline">\(X\)</span>.</p></li>
</ol>
</div>
<div id="modelos-lineales-generalizados-glm" class="section level2">
<h2><span class="header-section-number">2.8</span> Modelos Lineales Generalizados (GLM)</h2>
<div id="componentes-de-un-modelo-lineal-generalizado-glm" class="section level3">
<h3><span class="header-section-number">2.8.1</span> Componentes de un modelo lineal generalizado (GLM)</h3>
<p>Un modelo lineal generalizado tiene tres componentes básicos:</p>
<ul>
<li><p><strong>Componente aleatoria:</strong> Identifica la variable respuesta y su distribución de probabilidad.</p></li>
<li><p><strong>Componente sistemática:</strong> Especifica las variables explicativas (independientes o predictoras) utilizadas en la funci ́on predictora lineal.</p></li>
<li><p><strong>Función link:</strong> Es una función del valor esperado de <span class="math inline">\(Y, E(Y)\)</span>, como una combinación lineal de las variables predictoras.</p></li>
</ul>
<p><strong>Componente aleatoria</strong></p>
<p>La componente aleatoria de un GLM consiste en una variable aleatoria <span class="math inline">\(Y\)</span> con observaciones independientes <span class="math inline">\((y_1, \cdots, y_N )\)</span>. Suponemos la distribución de Y en la familia exponencial natural.</p>
<p><span class="math display">\[f(y_i| \theta_i)= a(\theta_i) b(y_i) e^{[y_iQ(\theta_i)]}\]</span></p>
<p><span class="math inline">\(\theta_i\)</span> varia para los distintos <span class="math inline">\(i\)</span> dependiendo de los valores de las variables predictoras.</p>
<p><span class="math inline">\(Q(\theta)\)</span> recibe el nombre de parámetro natural.</p>
</div>
</div>
<div id="un-problema-de-bandido-k-brasos" class="section level2">
<h2><span class="header-section-number">2.9</span> Un problema de bandido k-brasos</h2>
<p>Consideremos el siguiente problema de aprendizaje. Usted se enfrenta repetidamente a una elecci??n entre k diferentes opciones, o acciones. Despues de tomar una decisi??n recibira una recompensa que depende de la accion tomada (la recompensa no es fija, es decir, sigue una ley de probabilidad para cada de decisi??n), el objetivo es maximizar las ganancias recibidas despues de <span class="math inline">\(n\)</span> repeticiones.</p>
<p>Esta es la forma original del problema del bandido k-brasos, o tambien conocido como m??quina tragamonedas, cada acci??n seleccionada es como un juego de una maquina tragamonedas</p>
<div class="figure">
<img src="~/Reinforcement-learning/monedas.jpg" alt="Serie de maquinas tragamonedas" />
<p class="caption">Serie de maquinas tragamonedas</p>
</div>
<p>En este problema, cada una de las k acciones tiene una esperanza o ganancia media dado que la acci??n es seleccionada; a esto lo llamaremos el valor de la acci??n. Denotaremos la accion seleccionada en el paso <span class="math inline">\(t\)</span> como <span class="math inline">\(A_t\)</span>, y su respectiva recompensa como <span class="math inline">\(R_t\)</span>. El valor de una acci??n arbitraria <span class="math inline">\(a\)</span>, sera denotada por <span class="math inline">\(q_*(a)\)</span>, es la esperanza de las ganancias dado que se selecciono la accion <span class="math inline">\(a\)</span>: <span class="math display">\[q_*(a) \doteq E[R_t|A_t=a]\]</span></p>
<p>Si conocieramos el valor de cada acci??n, entonces este problema fuera trivial, pues siempre escogieramos la acci??n con mayor valor. Supondremos que no las conocemos con presici??n, aunque podriamos estimarlas. Denotaremos el valor estimado de la accion <span class="math inline">\(a\)</span> en el tiempo <span class="math inline">\(t\)</span> como <span class="math inline">\(Q_t(a)\)</span>. Por supuesto, queremos que <span class="math inline">\(Q_t(a)\)</span> sea cercano a <span class="math inline">\(q_*(a)\)</span>. Si nos mantenemos estimando las acciones a lo largo de tiempo en cierto momento obtendremos una acci??n con mayor valor, a estas acciones se les conoce como las acciones ambiciosas. Cuando se selecciona una de estas acciones decimos que estamos explotando el conocimiento actual del sistema. Si en vez de esto seleccionamos una de las acciones no codiciosas decimos que estamos explorando el sistema, esto le hacemos para mejorar la estimacion de las acciones no codiciosas. En general, no sabemos si de verdad la accion codiciosa es la correcta, pues proviene de una estimaci??n, esto implica que la exploracion sea tan importante. Debemos destacar que el balance entre exploracion y explotacion depende de lo sofisticado del problema, y no nos enfocaremos en este libro en este tema.</p>
</div>
<div id="mtodos-de-accin-valor" class="section level2">
<h2><span class="header-section-number">2.10</span> M??todos de acci??n valor</h2>
<p>Iniciaremos esta seccion con algunos m??todos para estimar los valores de las acciones y usar estas para tomar decisiones, recordemos que el verdadero valor de la acci??n es el valor medio de las recompensas cuando la acci??n es seleccionada. Una forma natural de estimar esto es con: <span class="math display">\[Q_t(a)\doteq \frac{\textrm{suma de las ganancias cuando las acciones son } a \textrm{ hasta el tiempo } t}{\textrm{ numero de veces que se tomo la accion }a\textrm{ hasta el tiempo t}}=\frac{\sum_{i=1}^{t}R_i.1_{A_i=a}}{\sum_{i=1}^{t}1_{A_i=a}}\]</span></p>
<p>Este t??cnica para aproximar <span class="math inline">\(q_*(a)\)</span> es la mas sencilla de todas, sabemos que converge al valor real por la ley fuerte de los grandes n??meros. Obviamente esta no es la ??nica ni la mas sencilla forma de estimar el valor verdadero de la acci??n <span class="math inline">\(a\)</span> pero es la mas simple de programar.</p>
<p>La regla mas simple para escoger la mejor accion es seleccionar la acci??n con mayor valor estimado, pero esto en realidad ser??a una decisi??n codiciosa y ya hablamos de las posibles desventajas de seleccionar siempre estas acciones. Esta regla se puede escribor como: <span class="math display">\[A_t\doteq argmax_a \quad Q_t(a)\]</span></p>
<p>Una forma de evitar la explotaci??n del conocimiento de forma tan enf??tica es escoger un porcentaje <span class="math inline">\(\epsilon\)</span> de acciones no codiciosas y el resto de la casos escoger las acciones codiciosas, esto nos ayuda a adaptarnos a cabios paulatinos del sistema, llamaremos a esta regla de escogencia como <span class="math inline">\(\epsilon\)</span>-codicioso. Esta regla implica que la probabilidad de escoger la acci??n ??ptima luego de haber estimado los valores de las acciones es <span class="math inline">\(1-\epsilon\)</span></p>
</div>
<div id="pruebas-sobre-el-problema-del-bandido-de-10-brasos" class="section level2">
<h2><span class="header-section-number">2.11</span> Pruebas sobre el problema del bandido de 10 brasos</h2>
<p>Supongamos que tenemos un problema de 10 acciones posibles, las recompensas que otorga cada accion son estoc??sticas y siguen una ley de probabilidad normal con varianza 1, para escoger la media de cada recompensa simplemente generamos un valor aleatorio normal estandar y varianza siempre 1. Como se ve en la siguiente figura:</p>
<div class="figure">
<img src="~/Reinforcement-learning/codi.png" alt="Serie de maquinas tragamonedas" />
<p class="caption">Serie de maquinas tragamonedas</p>
</div>
<p>Ahora ejucatermos tres reglas de seleccion, codicioso, 0.1-codicioso, 0.01-codisioso, obteniendo la segunda grafica de la imagen anterior.</p>
<p>Esto muestra que debe haber un equil??brio entre explotaci??n y explraci??n</p>
</div>
<div id="aplicacin-progresiva" class="section level2">
<h2><span class="header-section-number">2.12</span> Aplicaci??n progresiva</h2>
<p>Al hablar de costos computacionales el hacer un promedio luego de millones de ejecucuiones puede ocupar mucho espacio en memoria, es decir, un c??lculo tan sencillo como <span class="math inline">\(Q_n=\frac{R_1+R_2+...+R_n}{n}\)</span> puede tardar mucho tiempo. Este problema puede ser f??cilmente resuelto con la siguiente manipulaci??n algebraica.<span class="math display">\[\begin{align}
 Q_{n+1} &amp;= \frac{1}{n}\sum_{i=1}^{n}R_{i} = \frac{1}{n}(R_n+\sum_{i=1}^{n-1}R_{i})=\frac{1}{n}(R_n+(n-1)\frac{1}{n-1}\sum_{i=1}^{n-1}R_{i}) \\
  &amp;= \frac{1}{n}(R_n+(n-1)Q_n)=\frac{1}{n}(R_n+(nQ_n-Q_n)) \\
  &amp;= Q_n+\frac{1}{n}(R_n-Q_n)
\end{align}\]</span></p>
<p>As?? basta simplemente con guardar la estimaci??n anterior, la ??ltima recompensa y el valor de la ejecuci??n actual. De esta forma un algoritmo para estimar los valores de las acciones de una forma relativamente ??ptima ser??a:</p>
<ul>
<li>iniciamos, para <span class="math inline">\(a \in \{ 1,2...,k\}\)</span></li>
<li><span class="math inline">\(Q(a) = 0\)</span></li>
<li><p><span class="math inline">\(N(a) = 0\)</span></p></li>
<li>Repetimos por siempre</li>
<li><p><span class="math display">\[A = \left\{
\begin{array}{c l}
 argmax_a\quad Q(a) \qquad \textrm{con probabilidad }1-\epsilon \\
 \textrm{Una accion aleatoria } a\textrm{ con probabilidad }\epsilon \\
\end{array}
\right.\]</span></p></li>
<li><span class="math display">\[R=bandit(A)\]</span></li>
<li><span class="math display">\[N(A)=N(A)+1\]</span></li>
<li><p><span class="math display">\[Q(A)=Q(A)+\frac{1}{N(A)}[R-Q(A)]\]</span></p></li>
</ul>
<p>En general la regla es de le forma “Nueva est. = Vieja est. + par??metro[Recompensa - Vieja est.]”</p>
</div>
<div id="problemas-no-estacionarios" class="section level2">
<h2><span class="header-section-number">2.13</span> Problemas no estacionarios</h2>
<p>En general estos m??todos que se basan en promedios son adecuados para problemas estacionarios, es decir, cuando las probabilidades de las recompensas no varian con respecto al tiempo. Pero muy frecuentemente los problemas a los que nos enfrentamos son no estacionarios. En tales casos, tiene sentido dar m??s peso a las recompensas recientes que a las recompensas pasadas. Una forma de hacer este ajuste y la mas popular es ajustar el parametro de paso, escogiendolo de forma adecuada <span class="math inline">\(\alpha \in (0,1]\)</span>, de la siguiente forma:<span class="math display">\[\begin{align}
 Q_{n+1} &amp;= Q_n+\alpha(R_n-Q_n)\\
 &amp;= \alpha R_n+(1-\alpha)Q_n\\
 &amp;=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}] \\
&amp;= \alpha R_n+(1-\alpha)\alpha R_{n-1}+(1-\alpha)^{2}Q_{n-1} \\
  &amp;= (1-\alpha)^{n}Q_{1}+\sum_{i=1}^n\alpha(1-\alpha)^{n-i}R_i
\end{align}\]</span></p>
<p>Nos damos cuenta que el t??rmino <span class="math inline">\(\alpha(1-\alpha)^{n-i}\)</span> hace que las recompensas tiendan a cero luego de muchos pasos, es decir, <span class="math inline">\(R_i\)</span> decrece exponencialmente.</p>
<p>Algunas veces es conveniente variar el parametro de paso, a medida que aumenta el <span class="math inline">\(n\)</span>. Sea <span class="math inline">\(\alpha_n(a)\)</span> el par??metro de paso usado cuando se selecciona por <span class="math inline">\(n\)</span>-esima vez la acci??n <span class="math inline">\(a\)</span>. Si escogemos simplemente <span class="math inline">\(\alpha_n(a)=\frac{1}{n}\)</span> obtenemos el m??todo de los promedios, lo cual nos garantiza la convergencia verdadera por la ley fuerte de los grandes n??meros, pero no cualquier escogencia nos asegura la convergencia. Es bien conocido por el area de la aproximacion estoc??stica que las condiciones para que halla convergencia con probabilidad 1 son:<span class="math display">\[\sum_{i=1}^n\alpha_n(a)=\infty \quad \textrm{y}\quad\sum_{i=1}^n\alpha_n^2(a) &lt; \infty\]</span>La primera condici??n se requiere para garantizar que los pasos sean lo suficientemente grandes como para eventualmente superar cualquier condici??n inicial o fluctuaciones aleatorias. La segunda condici??n garantiza que eventualmente los pasos son lo suficientemente peque??os como para asegurar la convergencia.</p>
<p>Notemos que estas condiciones se cumplen para el caso en que <span class="math inline">\(a_n(a)=\frac{1}{n}\)</span>, para el caso en que <span class="math inline">\(a_n(a)\)</span> es una constante se cumple la primera condici??n, pero la segunda no, esto implica que la convergencia nunca ocurre completamente y continua variando en respuesta a nuevas ganancias recibidas. Como ya hemos mencionado, los problemas no estacinarios son los que nos interesan, por lo que mantener que el algoritmo sea sensible a estas variaciones de las recompensas es fundamental. En general las condiciones anteriores son te??ricas y se usan en investigaciones, el cual no ser?? el enfoque de este libro.</p>
</div>
<div id="ptimos-valores-iniciales" class="section level2">
<h2><span class="header-section-number">2.14</span> ??ptimos valores iniciales</h2>
<p>En todo los m??todos que hemos discutido, en parte, todos ellos dependen del valor inicial de la estimaci??n, en estad??stica esto significa que los m??todos son sesgados. Para el caso en que usamos promedios para la estimaci??n el sesgo desaparece una vez todos los casos han sido seleccionados, pero en los m??todos con constante <span class="math inline">\(\alpha\)</span> el sesgo disminuye en el transcurso del tiempo pero no desaparece. La desventaja es que la escogencia de las condiciones iniciales se convierte en la decisi??n particular del usuario de un conjunto de parametros iniciales, as?? sea ponerlas todas en cero. La parte positiva es que con la ayuda de la experiencia podemos proponer buenos valores iniciales para la estimaci??n.</p>
<p>Usar de forma inteligente valores iniciales puede fomentar la exploraci??n, supongamos que tenemos un problema de bandido 10 brazos, donde las recompensas provienen de una normal estandar, si escogemos como valores iniciales 5 en vez de 0 como es lo usal, estaremos sobre-estimando las ganancias, por lo cual aseguraremos que el agente pase por todas las acciones posibles en un trancurso de tiempo r??pido. En la siguiente figura vemos esta comparaci??n.</p>
<div class="figure">
<img src="~/Reinforcement-learning/optimas.png" alt="Comparacion al escoger valores iniciales optimos y realistas" />
<p class="caption">Comparacion al escoger valores iniciales optimos y realistas</p>
</div>
<p>Estas t??cnicas que se basan en estimular la exploraci??n ser??n llamadas m??todos de valores iniciales optimistas. Este truco puede ser ideal para fomentar la exploraci??n en problemas estacionarios, pero esta lejos de ser realmente util en problemas no estacionarios pues ellos van cambiando su comportamiento y una estimacion previa puede no ser de utilidad en el trancurso del tiempo. De hecho, es poco probable que cualquier m??todo que se centre en las condiciones iniciales de manera especial ayude en el caso general no estacionario. El inicio ocurre s??lo una vez, y por lo tanto no debemos enfocarnos demasiado en ??l. Esta cr??tica se aplica tambi??n a los m??todos de promedios, que tambi??n tratan el comienzo del tiempo como un evento especial, promediando todas las recompensas subsiguientes con igual peso. Sin embargo, todos estos m??todos son muy simples, sin embargo uno de ellos, o una simple combinaci??n de ellos, es a menudo adecuado en la pr??ctica.</p>
</div>
<div id="cota-superior-de-confianza-en-la-seleccin-de-acciones-csc" class="section level2">
<h2><span class="header-section-number">2.15</span> Cota superior de confianza en la selecci??n de acciones (CSC)</h2>
<p>La exploraci??n es necesaria debido a que siempre hay incertidumbre acerca de las estimaciones de los valores de las acciones. Las acciones codiciosas pueden ser mejores en el tiempo actual pero puede ocultar la mejora de otras acciones en problemas no estacionarios, es por esto que metodos <span class="math inline">\(\epsilon\)</span>-codiciosos, sacan a la luz acciones que hn permanecido acultas pero que pudieran ser m??s productivas en ese momento, pero este m??todo no discrimina entre las acciones no codiciosas. Seria mejor seleccionar entre las acciones no codiciosas las que tengan mas potencial de ser ??ptimas, teniendo en cuenta la maximizaci??n de sus acciones y la incertidumbre sobre las mismas. Una forma efectiva de hacer esto es seleccionar la acci??n no codiciosa, con la siguiente regla:<span class="math display">\[A_t=argmax\bigg{[}Q_{t}(a)+\sqrt[c]{\frac{ln(t)}{N()}}\bigg{]}\]</span> donde <span class="math inline">\(ln(t)\)</span> denota el logaritmo neperiano evaluado en <span class="math inline">\(t\)</span>, <span class="math inline">\(N_t(a)\)</span> denota el n??mero de veces que la acci??n <span class="math inline">\(a\)</span> ha sido seleccionada hasta el paso <span class="math inline">\(t\)</span> y el n??mero <span class="math inline">\(c &gt; 0\)</span> controla grado de exploraci??n.</p>
<p>La idea de esta selecci??n es que el t??rmino de la raiz sea una medida de incertidumbre o varianza en la estimaci??n del valor. Por lo que la cantidad que se esta maximizando es una especie cota superior sobre el posible valor real de la acci??n <span class="math inline">\(a\)</span>, donde <span class="math inline">\(c\)</span> determina el nivel de confianza. Cada vez que se selecciona <span class="math inline">\(a\)</span> la incertidubre se reduce: <span class="math inline">\(N_t(a)\)</span> incrementa y como se encuentra en el denominador el segundo termino de la suma decredce, lo cual reduce la varianza. El uso del logaritmo significa que los aumentos se reducen con el tiempo, pero son ilimitadas. Todas las acciones ser??n seleccionadas, pero las acciones con valores estimados m??s bajos, o que ya han sido seleccionadas con frecuencia, ser??n seleccionadas con frecuencia decreciente en el tiempo.</p>
<div class="figure">
<img src="~/Reinforcement-learning/ganancias.png" alt="Promedio de ganancias al escoger las acciones con el m??todo CSC y 0,1-codicioso" />
<p class="caption">Promedio de ganancias al escoger las acciones con el m??todo CSC y 0,1-codicioso</p>
</div>
</div>
<div id="algoritmo-del-gradiente" class="section level2">
<h2><span class="header-section-number">2.16</span> Algoritmo del gradiente</h2>
<p>Hasta ahora hemos estudiado m??todos que estiman los valores de las acciones y usan esta estimaci??n para seleccionar acciones. Esto frecuentemente es un buen enfoque, pero no el ??nico. En esta secci??n consideraremos aprender una preferencia num??rica para cada acci??n <span class="math inline">\(a\)</span>, la cual denotaremos <span class="math inline">\(H_t(a)\)</span>, la mayor preferencia es la acci??n mas tomada, pero la preferencia no tiene interpretaci??n en t??rminos de ganancias. ??nicamnete la preferencia relativa de una acci??n sobre otra es importante. Si agregamos 100 a cada acci??n esta no afectaran las probabilidades de acci??n, las cuales son determinadas por las distribuciones Gibbs o Boltzmann, como sigue: <span class="math display">\[P\{ A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^{k}e^{H_t(b)}}=\pi_t(a)\]</span> Donde <span class="math inline">\(\pi_t(a)\)</span> es la probabilidad de tomar la acci??n <span class="math inline">\(a\)</span> en el tiempo <span class="math inline">\(t\)</span>. Inicialmente todas las preferencias son iguales (por ejemplo, <span class="math inline">\(H_1(a)=0\)</span> para toda acci??n <span class="math inline">\(a\)</span>) es decir todas las acciones tienen la misma preferencia.</p>
<p>Hay un algoritmo de aprendizaje basado en la idea del descenso del gradiente estoc??stico. En cada paso, despues de seleccionar la acci??n <span class="math inline">\(A_t\)</span> y recibir la recompensa <span class="math inline">\(R_t\)</span>, las preferencias son actualizadas as??:<span class="math display">\[H_{t+1}(A_t)=H_{t}(A_t)+\alpha(R_t-\overline{R_t})(1-\pi_t(A_t))\]</span> y <span class="math display">\[H_{t+1}(a)=H_{t}(a)+\alpha(R_t-\overline{R_t})(\pi_t(a))\quad\textrm{para todo }a\neq A_t\]</span> donde <span class="math inline">\(\alpha &gt; 0\)</span> es el par??metro de paso, y <span class="math inline">\(\overline{R_t}\)</span> es el promedio de todas las recompensas haste el tiempo <span class="math inline">\(t\)</span>. <span class="math inline">\(\overline{R_t}\)</span> sirve como un umbral con el que se compara las recompensas. Si <span class="math inline">\(R_t\)</span> es mayor que el umbral la probabilidad de seleccionar la accion <span class="math inline">\(A_t\)</span> aumenta y viceversa.</p>
</div>
<div id="investigacin-asociativa" class="section level2">
<h2><span class="header-section-number">2.17</span> Investigaci??n asociativa</h2>
<p>Hasta ahora en este cap??tulo solo hemos considerado problemas no asociativos, es decir, no hay necesidad de asociar diferentes acciones con diferentes situaciones.En estas tareas, el alumno trata de encontrar la mejor acci??n cuando la tarea es estacionaria, o trata de econtrar la mejor acci??n a medida que el entorno cambia con el tiempo cuando la tarea no es estacionaria. Sin embargo, en Reinforcement Learning hay mas de una situaci??n y la meta, es aprender una pol??tica: una funci??n que tenga como dominio situaciones y rango de acciones ??ptimas en esas situaciones. Para prepararnos para esto discutiremos brevemente como las tareas no asociativas se extienden a ajustes asosiativos.</p>
<p>Por ejemplo, supongamos que hay varios problemas de bandidos de k-brasos, y que en cada paso te enfrentas a uno de ellos aleatoriamente, es decir cambia de un paso a otro al azar. Pareciera que estamos en un problema no estacionario en el que el valor de las acciones cambia aleatoriamente y estariamos tentados a usar uno de los m??todos vistos anteriormente para intentar afrontar esto, pero a menos que las acciones cambien sus valores de forman no tan drastica estariamos utilazando m??todos que se adaptarian de forma muy lenta a las distintas situaciones que afrontamos. Supongamos que estamos en dos problemas de k brazos y dependiendo del alguna condicion usamos aproximaciones diferentes, es decir si se cumple la condicion 1 usar la aproximacion 1 si se cumplen la condicion 2 usar la aproximaci??n 2. Con una pol??tica correcta uno pudiera hacer esta selecci??n de una forma mas adecuada en el caso de no tener informaci??n clara que permita distinguir que condici??n se cumple.</p>
<p>Esto es un ejemplo de busqueda asosiativa, llamada as?? por que estimula el intenta y error en momentos en el que se esta aprendiendo buscar mejores acciones, y asociar estas acciones con las situaciones en las cuales ellas son mejores. La tarea de buscar asociaciones es llamada bandido contextual en la literatura. En conclusi??n esto se puede considerar como el problema del Reinforcement Learning semi-completo, pues involucra el aprendizaje de una politica, se nota la diferencia con el problema de k-brasos en el cual las acciones son afectadas por la ganancia inmediata, si se permite que las acciones afecten tanto a la siguiente situaci??n como a la recompensa, entonces tenemos el problema de Reinforcement Learning completo. Esto ser?? presentado en el siguiente cap??tulo.</p>

</div>
</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="introduccion.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="procesos-de-decision-de-markov-finitos.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Analisis_y_Medida/edit/master/bookdown/100-capitulo1.Rmd",
"text": "Edit"
},
"download": ["Reinforcement Learning.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
