<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Reinforcement Learning</title>
  <meta name="description" content="Reinforcement Learning">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Reinforcement Learning" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://synergy.vision/Reinforcement-learning/" />
  <meta property="og:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />
  
  <meta name="github-repo" content="synergyvision/Reinforcement-learning/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Reinforcement Learning" />
  
  
  <meta name="twitter:image" content="http://synergy.vision/Reinforcement-learning/images/cover.png" />

<meta name="author" content="Synergy Vision">


<meta name="date" content="2019-01-18">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="metodos-de-montecarlo.html">
<link rel="next" href="bootstrapping-en-n-pasos.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="https://cdn.datacamp.com/datacamp-light-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./"><img src="images/logovision-black.png" width="160"></img></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prefacio</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#por-que-leer-este-libro"><i class="fa fa-check"></i>¿Por qué leer este libro?</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#informacion-sobre-los-programas-y-convenciones"><i class="fa fa-check"></i>Información sobre los programas y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practicas-interactivas-con-r"><i class="fa fa-check"></i>Prácticas interactivas con R</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#agradecimientos"><i class="fa fa-check"></i>Agradecimientos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="acerca-del-autor.html"><a href="acerca-del-autor.html"><i class="fa fa-check"></i>Acerca del Autor</a></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#ejemplos"><i class="fa fa-check"></i><b>1.2</b> Ejemplos</a></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#elementos-del-reinforcement-learning"><i class="fa fa-check"></i><b>1.3</b> Elementos del Reinforcement Learning</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#limitaciones-y-alcance"><i class="fa fa-check"></i><b>1.4</b> Limitaciones y alcance</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#un-ejemplo-clasico-tres-en-linea."><i class="fa fa-check"></i><b>1.5</b> Un ejemplo clásico: tres en linea.</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>2</b> Modelos Lineales</a><ul>
<li class="chapter" data-level="2.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal</a></li>
<li class="chapter" data-level="2.2" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros-del-modelo"><i class="fa fa-check"></i><b>2.2</b> Estimación de los parámetros del modelo</a></li>
<li class="chapter" data-level="2.3" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-lineal-multiple"><i class="fa fa-check"></i><b>2.3</b> Regresión Lineal Múltiple</a></li>
<li class="chapter" data-level="2.4" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelo-de-regresion-lineal-en-notacion-matricial"><i class="fa fa-check"></i><b>2.4</b> Modelo de Regresión Lineal en notación matricial</a></li>
<li class="chapter" data-level="2.5" data-path="modelos-lineales.html"><a href="modelos-lineales.html#estimacion-de-los-parametros"><i class="fa fa-check"></i><b>2.5</b> Estimación de los Parámetros</a></li>
<li class="chapter" data-level="2.6" data-path="modelos-lineales.html"><a href="modelos-lineales.html#valores-ajustados-y-residuos"><i class="fa fa-check"></i><b>2.6</b> Valores Ajustados y Residuos</a></li>
<li class="chapter" data-level="2.7" data-path="modelos-lineales.html"><a href="modelos-lineales.html#observaciones"><i class="fa fa-check"></i><b>2.7</b> Observaciones</a></li>
<li class="chapter" data-level="2.8" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generalizados-glm"><i class="fa fa-check"></i><b>2.8</b> Modelos Lineales Generalizados (GLM)</a><ul>
<li class="chapter" data-level="2.8.1" data-path="modelos-lineales.html"><a href="modelos-lineales.html#componentes-de-un-modelo-lineal-generalizado-glm"><i class="fa fa-check"></i><b>2.8.1</b> Componentes de un modelo lineal generalizado (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="modelos-lineales.html"><a href="modelos-lineales.html#modelos-lineales-generelizados-para-datos-binarios"><i class="fa fa-check"></i><b>2.9</b> Modelos Lineales Generelizados para datos binarios</a></li>
<li class="chapter" data-level="2.10" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-logistica"><i class="fa fa-check"></i><b>2.10</b> Regresión Logística</a></li>
<li class="chapter" data-level="2.11" data-path="modelos-lineales.html"><a href="modelos-lineales.html#regresion-probit"><i class="fa fa-check"></i><b>2.11</b> Regresión Probit</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html"><i class="fa fa-check"></i><b>3</b> Procesos de decision de Markov finitos</a><ul>
<li class="chapter" data-level="3.1" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#el-agente-un-interface-del-entorno"><i class="fa fa-check"></i><b>3.1</b> El agente, Un interface del entorno</a></li>
<li class="chapter" data-level="3.2" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#metas-y-recompensas."><i class="fa fa-check"></i><b>3.2</b> Metas y recompensas.</a></li>
<li class="chapter" data-level="3.3" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#retornos-y-episodios"><i class="fa fa-check"></i><b>3.3</b> Retornos y episodios</a></li>
<li class="chapter" data-level="3.4" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#notacion-unificada-tanto-para-tareas-episodicas-y-continuas."><i class="fa fa-check"></i><b>3.4</b> Notación unificada tanto para tareas episodicas y continuas.</a></li>
<li class="chapter" data-level="3.5" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#politicas-y-funciones-de-valor"><i class="fa fa-check"></i><b>3.5</b> Políticas y funciones de valor</a></li>
<li class="chapter" data-level="3.6" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#funciones-de-valor-y-politicas-optimas"><i class="fa fa-check"></i><b>3.6</b> Funciones de valor y políticas optimas</a></li>
<li class="chapter" data-level="3.7" data-path="procesos-de-decision-de-markov-finitos.html"><a href="procesos-de-decision-de-markov-finitos.html#optimalidad-y-aproximacion"><i class="fa fa-check"></i><b>3.7</b> Optimalidad y aproximación</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html"><i class="fa fa-check"></i><b>4</b> Métodos de Montecarlo</a><ul>
<li class="chapter" data-level="4.1" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#prediccion-con-monte-carlo"><i class="fa fa-check"></i><b>4.1</b> Predicción con Monte Carlo</a></li>
<li class="chapter" data-level="4.2" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#estimacion-de-monte-carlo-de-los-valores-de-accion"><i class="fa fa-check"></i><b>4.2</b> Estimación de Monte Carlo de los Valores de Acción</a></li>
<li class="chapter" data-level="4.3" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control"><i class="fa fa-check"></i><b>4.3</b> Métodos de Monte Carlo con control</a></li>
<li class="chapter" data-level="4.4" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#metodos-de-monte-carlo-con-control-sin-iniciar-exploracion"><i class="fa fa-check"></i><b>4.4</b> Métodos de Monte Carlo con control sin iniciar exploración</a></li>
<li class="chapter" data-level="4.5" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#predicciones-no-politicas-via-muestreos-de-importancia."><i class="fa fa-check"></i><b>4.5</b> Predicciones no políticas via muestreos de importancia.</a></li>
<li class="chapter" data-level="4.6" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#implementacion-incremental"><i class="fa fa-check"></i><b>4.6</b> Implementación incremental</a></li>
<li class="chapter" data-level="4.7" data-path="metodos-de-montecarlo.html"><a href="metodos-de-montecarlo.html#monte-carlo-no-politico-con-control"><i class="fa fa-check"></i><b>4.7</b> Monte Carlo no político con control</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html"><i class="fa fa-check"></i><b>5</b> Aprendizaje por Diferencia Temporal</a><ul>
<li class="chapter" data-level="5.1" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#prediccion"><i class="fa fa-check"></i><b>5.1</b> Predicción</a></li>
<li class="chapter" data-level="5.2" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#ventajas-de-los-metodos-de-prediccion-de-td"><i class="fa fa-check"></i><b>5.2</b> Ventajas de los métodos de predicción de TD</a></li>
<li class="chapter" data-level="5.3" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#calidad-de-td0"><i class="fa fa-check"></i><b>5.3</b> Calidad de TD(0)</a></li>
<li class="chapter" data-level="5.4" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-td-politico-con-control"><i class="fa fa-check"></i><b>5.4</b> Sarsa: TD político con control</a></li>
<li class="chapter" data-level="5.5" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#q-learning-td-no-politico-con-control"><i class="fa fa-check"></i><b>5.5</b> Q-Learning: TD no político con control</a></li>
<li class="chapter" data-level="5.6" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sarsa-esparada"><i class="fa fa-check"></i><b>5.6</b> Sarsa esparada</a></li>
<li class="chapter" data-level="5.7" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#sesgo-de-maximizacion-y-doble-aprendizaje"><i class="fa fa-check"></i><b>5.7</b> Sesgo de maximización y doble aprendizaje</a></li>
<li class="chapter" data-level="5.8" data-path="aprendizaje-por-diferencia-temporal.html"><a href="aprendizaje-por-diferencia-temporal.html#juegos-afterstates-y-otros-casos-especiales"><i class="fa fa-check"></i><b>5.8</b> Juegos, afterstates y otros casos especiales</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping en <span class="math inline">\(n\)</span>-pasos</a><ul>
<li class="chapter" data-level="6.1" data-path="bootstrapping-en-n-pasos.html"><a href="bootstrapping-en-n-pasos.html#prediccion-de-td-en-n-pasos"><i class="fa fa-check"></i><b>6.1</b> Predicción de TD en <span class="math inline">\(n\)</span> pasos</a></li>
</ul></li>
<li class="appendix"><span><b>Apéndice</b></span></li>
<li class="chapter" data-level="A" data-path="software-tools.html"><a href="software-tools.html"><i class="fa fa-check"></i><b>A</b> Software Tools</a><ul>
<li class="chapter" data-level="A.1" data-path="software-tools.html"><a href="software-tools.html#r-and-r-packages"><i class="fa fa-check"></i><b>A.1</b> R and R packages</a></li>
<li class="chapter" data-level="A.2" data-path="software-tools.html"><a href="software-tools.html#pandoc"><i class="fa fa-check"></i><b>A.2</b> Pandoc</a></li>
<li class="chapter" data-level="A.3" data-path="software-tools.html"><a href="software-tools.html#latex"><i class="fa fa-check"></i><b>A.3</b> LaTeX</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aprendizaje-por-diferencia-temporal" class="section level1">
<h1><span class="header-section-number">Capítulo 5</span> Aprendizaje por Diferencia Temporal</h1>
<p>Si tuviéramos que identificar una idea central y novedosa para el Reinforcement Learning, sin duda sería el aprendizaje por diferencia temporal (TD). El aprendizaje de TD es una combinación de ideas de métodos de Monte Carlo e ideas de programación dinámica (DP). Al igual que los métodos de Monte Carlo, los métodos de TD pueden aprender directamente de la experiencia en bruto sin un modelo de la dinámica del entorno. Al igual que DP, los métodos de TD actualizan las estimaciones basadas en parte en otras estimaciones aprendidas, sin esperar un resultado final (bootstrap). La relación entre los métodos TD, DP y Monte Carlo es un tema recurrente en la teoría del Reinforcement Learning; este capítulo es el comienzo de nuestra exploración del mismo. Antes de que terminemos, veremos que estas ideas y métodos se mezclan entre sí y pueden combinarse de muchas maneras. En particular, en el Capítulo 7 introducimos los algoritmos de <span class="math inline">\(n\)</span> pasos, que proporcionan un puente entre los métodos TD y Monte Carlo, y en el Capítulo 12 introducimos el algoritmo TD(<span class="math inline">\(\lambda\)</span>), que los une a la perfección.</p>
<div id="prediccion" class="section level2">
<h2><span class="header-section-number">5.1</span> Predicción</h2>
<p>Tanto los métodos de TD como los de Monte Carlo utilizan la experiencia para resolver el problema de la predicción. Dada una cierta experiencia siguiendo una política <span class="math inline">\(\pi\)</span>, ambos métodos actualizan su estimación <span class="math inline">\(V\)</span> de <span class="math inline">\(v_\pi\)</span> para los estados no terminales <span class="math inline">\(S_t\)</span> que ocurren en esa experiencia. En términos generales, los métodos de Monte Carlo esperan hasta que se conozca el regreso después de la visita, y luego utilizan ese regreso como objetivo para <span class="math inline">\(V(S_t)\)</span>. Un método sencillo de Monte Carlo para cada visita, adecuado para entornos no estacionarios, es el siguiente:<span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha[G_t-V(S_t)]\]</span> donde <span class="math inline">\(G_t\)</span> es el retorno actual después del tiempo t, y <span class="math inline">\(\alpha\)</span> es un parámetro de tamaño de paso constante. Llamaremos a este método MC <span class="math inline">\(\alpha\)</span>-constante. Mientras que los métodos de Monte Carlo deben esperar hasta el final del episodio para determinar el incremento a <span class="math inline">\(V(S_t)\)</span> (sólo entonces se conoce <span class="math inline">\(G_t\)</span>), los métodos de TD sólo necesitan esperar hasta el siguiente paso de tiempo. En el momento <span class="math inline">\(t + 1\)</span> forman inmediatamente un objetivo y hacen una actualización útil utilizando la recompensa observada <span class="math inline">\(R_{t+1}\)</span> y la estimación <span class="math inline">\(V(S_{t+1})\)</span>. El método más simple de TD hace la actualización: <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})- V(S_t)]\]</span> inmediatamente en la transición a <span class="math inline">\(S_{t+1}\)</span> y recibiendo <span class="math inline">\(R_{t+1}\)</span>. En efecto, el objetivo de la actualización de Monte Carlo es <span class="math inline">\(G_t\)</span>, mientras que el objetivo de la actualización de TD es <span class="math inline">\(R_{t+1} + V (S_{t+1})\)</span>. Este método de TD se llama TD(0), o TD de un paso, porque es un caso especial de los métodos TD(<span class="math inline">\(\lambda\)</span>) y TD de <span class="math inline">\(n\)</span> pasos desarrollados en el Capítulo 12 y el Capítulo 7. El cuadro que figura a continuación muestra la versión completa de la forma TD(0).</p>
<ul>
<li>Iniciamos la política <span class="math inline">\(\pi\)</span> a ser evaluada</li>
<li>Iniciamos <span class="math inline">\(V(s)\)</span> arbatrariamente.</li>
<li>Repetimos (Para cada episodio)
<ul>
<li>Iniciamos <span class="math inline">\(S\)</span></li>
<li>Repetimos (para cada paso del episodio):
<ul>
<li><span class="math inline">\(A \leftarrow\)</span> una acción dada por <span class="math inline">\(\pi\)</span> por <span class="math inline">\(S\)</span></li>
<li>toma la acción <span class="math inline">\(A\)</span>, observa <span class="math inline">\(R\)</span> y <span class="math inline">\(S&#39;\)</span></li>
<li><span class="math inline">\(V(S_t)\leftarrow V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})- V(S_t)]\)</span></li>
<li><span class="math inline">\(S\leftarrow S&#39;\)</span></li>
</ul></li>
<li>Hata el estado terminal <span class="math inline">\(S\)</span></li>
</ul></li>
</ul>
<p>Dado que TD(0) basa su actualización en parte en una estimación existente, decimos que se trata de un método de bootstrapping, como DP. Sabemos por el Capítulo 3 que</p>
<p><span class="math display">\[\begin{equation} \label{eq1}
\begin{split}
v_\pi(s) &amp; = E_\pi[G_t|S_t=s] \\
 &amp; = E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]\\
 &amp; = E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
\end{split}
\end{equation}\]</span></p>
<p>A grandes rasgos, los métodos de Monte Carlo utilizan una estimación de <span class="math inline">\(E_\pi[G_t|S_t=s]\)</span> como objetivo, mientras que los métodos de DP utilizan una estimación de <span class="math inline">\(E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\)</span> como objetivo. El objetivo de Monte Carlo es una estimación porque no se conoce el valor esperado en <span class="math inline">\(E_\pi[G_t|S_t=s]\)</span>; se utiliza un rendimiento de muestra en lugar del rendimiento real esperado. El objetivo de la DP es una estimación no debido a los valores esperados, que se supone que son completamente proporcionados por un modelo del entorno, sino porque <span class="math inline">\(v(S_{t+1})\)</span> no se conoce y la estimación actual, <span class="math inline">\(V(S_{t+1})\)</span>, es usado en su lugar. El objetivo de TD es una estimación por ambas razones: muestrea los valores esperados en <span class="math inline">\(E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\)</span> y utiliza la estimación actual <span class="math inline">\(V\)</span> en lugar de la verdadera <span class="math inline">\(v_\pi\)</span>. Así, los métodos de TD combinan el muestreo de Monte Carlo con el bootstrapping de DP. Como veremos, con cuidado e imaginación esto nos puede llevar mucho tiempo para obtener las ventajas de los métodos de Monte Carlo y DP.</p>
<p>En la próxima imagen mostramos el resumen para calcular TD(0). La estimación del valor del nodo de estado en la parte superior del diagrama se actualiza sobre la base de la transición de una muestra de éste al estado inmediatamente posterior. Nos referimos a las actualizaciones de TD y Monte Carlo como actualizaciones de muestra porque implican mirar hacia delante a un estado sucesor de muestra (o par de acción-estado), usando el valor del sucesor y la recompensa a lo largo del camino para calcular un valor de respaldo, y luego actualizando el valor del estado original (o par de acción-estado) en consecuencia. Las actualizaciones de la muestra son un fechador de las actualizaciones esperadas de los métodos DP, ya que se basan en un único sucesor de la muestra y no en una distribución completa de todos los sucesores posibles. Las actualizaciones de las muestras difieren de las actualizaciones esperadas de los métodos DP en que se basan en un único sucesor de la muestra y no en una distribución completa de todos los sucesores posibles.</p>
<div class="figure">
<img src="~/Reinforcement-learning/td.png" alt="" />
<p class="caption"></p>
</div>
<p>Por último, tenga en cuenta que la cantidad entre paréntesis en la actualización de TD(0) es una especie de error, ya que mide la diferencia entre el valor estimado de <span class="math inline">\(S_t\)</span> y la mejor estimación <span class="math inline">\(R_{t+1} + \gamma V (S_{t+1})\)</span>. Esta cantidad, llamada el error de TD, surge de varias formas a lo largo del Reinforcement Learning:<span class="math display">\[\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\]</span> Note que el error de TD en cada momento es el error en la estimación hecha en ese momento. Debido a que el error de TD depende del siguiente estado y de la siguiente recompensa, en realidad no está disponible hasta un paso más adelante. Es decir, <span class="math inline">\(\delta_t\)</span> es el error en <span class="math inline">\(V(S_t)\)</span>, disponible en el tiempo <span class="math inline">\(t + 1\)</span>. También tenga en cuenta que si la matriz <span class="math inline">\(V\)</span> no cambia durante el episodio (como no lo hace en los métodos de Monte Carlo), entonces el error de Monte Carlos puede ser escrito como la suma de errores TD:</p>
<span class="math display">\[\begin{equation} 
\begin{split}
G_t-V(S_t) &amp; = R_{t+1}+ \gamma G_{t+1}-V(S_{t+1})+\gamma V(S_{t+1}) \\
 &amp; = \delta_t+\gamma(G_{t+1}-V(S_{t+1})) \\
 &amp; = \delta_t + \gamma \delta_{t+1}+\gamma^2 (G_{t+2}-V(S_{t+2}))\\
 &amp; = \delta_t + \gamma \delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t} (G_{T}-V(S_{T}))\\
 &amp; = \delta_t + \gamma \delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t} (0-0))\\
 &amp; = \sum^{T-1}_{k=t}\gamma^{k-t}\delta_k
 
\end{split}
\end{equation}\]</span>
<p>Esta identidad no es exacta si V se actualiza durante el episodio (como sucede en TD(0)), pero si el tamaño del paso es pequeño, puede que aún se mantenga de formna aproximada. Las generalizaciones de esta identidad juegan un papel importante en la teoría y los algoritmos del aprendizaje de la diferencia temporal.</p>
<p>Ejemplo: Cada día que conduces a casa desde el trabajo, tratas de predecir por cuánto tiempo se necesita para llegar a casa. Cuando dejas tu lugar, anotas la hora, el día de la semana, el tiempo, y cualquier otra cosa que pueda ser relevante. Digamos que este viernes te vas exactamente a las 6 en punto, y estimas que tardarás 30 minutos en llegar a casa. Cuando llegas a tu coche son las 6:05, y te das cuenta de que está empezando a llover. El trafico es a menudo más lento en la lluvia, por lo que se estima que tardaras 35 minutos. a partir de entonces, o un total de 40 minutos. Quince minutos más tarde has completado la parte de la carretera de su viaje a tiempo. Al salir a una carretera secundaria se reduce la estimación del viaje total a 35 minutos. Desafortunadamente, en este punto te quedas atascado detrás de un camión lento, y la carretera es demasiado estrecha para pasar. Terminas teniendo que seguir al camión hasta que giras en la calle lateral, ya son las 6:40. Tres minutos después estás en casa. La secuencia de estados, tiempos y predicciones es así como sigue:</p>
<table>
<thead>
<tr class="header">
<th align="center">State</th>
<th align="center">Tiempo transcurrido</th>
<th align="center">tiempo previsto para irse</th>
<th align="center">tiempo total previsto</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">saliendo de la oficina, el viernes a las 6:00</td>
<td align="center">0</td>
<td align="center">30</td>
<td align="center">30</td>
</tr>
<tr class="even">
<td align="center">llegar al carro lloviendo</td>
<td align="center">5</td>
<td align="center">35</td>
<td align="center">40</td>
</tr>
<tr class="odd">
<td align="center">saliendo de la carretera</td>
<td align="center">20</td>
<td align="center">15</td>
<td align="center">35</td>
</tr>
<tr class="even">
<td align="center">2ª carretera, detrás del camión</td>
<td align="center">30</td>
<td align="center">10</td>
<td align="center">40</td>
</tr>
<tr class="odd">
<td align="center">entrando a la calle de la casa</td>
<td align="center">40</td>
<td align="center">3</td>
<td align="center">43</td>
</tr>
<tr class="even">
<td align="center">llegar a casa</td>
<td align="center">43</td>
<td align="center">0</td>
<td align="center">43</td>
</tr>
</tbody>
</table>
<p>Las recompensas en este ejemplo son los tiempos transcurridos en cada tramo del viaje. No estamos descontando (<span class="math inline">\(\gamma\)</span> = 1), y por lo tanto el retorno para cada estado es el tiempo real para salir de ese estado. El valor de cada estado es el tiempo que se espera que transcurra. La segunda columna de números da el valor estimado actual para cada estado encontrado.</p>
<p>Una manera sencilla de ver el funcionamiento de los métodos de Monte Carlo es graficar el tiempo total previsto (la última columna) sobre la secuencia, como en la siguiente figura (izquierda). Las flechas muestran los cambios en las predicciones recomendadas por el método MC constante, para <span class="math inline">\(\alpha\)</span> = 1. Estos son exactamente los errores entre el valor estimado (tiempo previsto para ir) en cada estado y el retorno real (tiempo real para ir). Por ejemplo, cuando salió de la carretera pensó que sólo le tomaría 15 minutos más llegar a casa, pero de hecho le tomó 23 minutos. La primera ecuación del capitulo se aplica en este punto y determina un incremento en el tiempo estimado para salir de la carretera. El error, <span class="math inline">\(G_t-V (S_t)\)</span>, en este momento es de ocho minutos. Supongamos que el tamaño del parametro de paso, <span class="math inline">\(\alpha\)</span>, es <span class="math inline">\(\frac{1}{2}\)</span>. Entonces el tiempo previsto para salir de la autopista se modificará al alza en cuatro minutos como resultado de esta experiencia. Este es probablemente un cambio demasiado grande en este caso; el camión fue probablemente sólo un golpe de suerte. En cualquier caso, el cambio sólo se puede hacer en línea, es decir, después de haber llegado a casa. Sólo en este momento se conoce alguna de las retornos reales.</p>
<div class="figure">
<img src="~/Reinforcement-learning/td1.png" alt="" />
<p class="caption"></p>
</div>
<p>¿Es necesario esperar hasta que se conozca el resultado final antes de comenzar el aprendizaje? Suponga que otro día, al salir de su oficina, calcula que le llevará 30 minutos conducir hasta su casa, pero luego se queda atascado en un atasco de tráfico masivo. Veinticinco minutos después de salir de la oficina, usted todavía se encuentra en la carretera. Ahora usted estima que le tomará otros 25 minutos llegar a casa, para un total de 50 minutos. Mientras espera en el tráfico, ya sabe que su estimación inicial de 30 minutos era demasiado optimista. ¿Debe esperar hasta que llegue a casa antes de aumentar su estimación para el estado inicial? Según el enfoque de Monte Carlo, hay que hacerlo, porque todavía no se conoce el verdadero retorno.</p>
<p>De acuerdo con un enfoque de TD, por otro lado, usted aprendería inmediatamente, cambiando su estimación inicial de 30 minutos a 50 minutos. De hecho, cada estimación se desplazaría hacia la estimación que le sigue inmediatamente. Volviendo a nuestro primer día de conducción, la Figura anterior (derecha) muestra los cambios en las predicciones recomendadas por la regla de TD (estas son las modificaciones realizadas por la regla si <span class="math inline">\(\alpha\)</span>= 1). Cada error es proporcional al cambio en el tiempo de la predicción, es decir, a las diferencias temporales en las predicciones.</p>
<p>Además de darle algo que hacer mientras espera en el tráfico, hay varias razones computacionales por las que es ventajoso aprender basado en sus predicciones actuales en lugar de esperar hasta la terminación cuando se sabe el retorno real. Discutiremos brevemente algunos de ellos en la siguiente sección.</p>
</div>
<div id="ventajas-de-los-metodos-de-prediccion-de-td" class="section level2">
<h2><span class="header-section-number">5.2</span> Ventajas de los métodos de predicción de TD</h2>
<p>Los métodos de TD actualizan sus estimaciones basándose en parte en otras estimaciones. Aprenden una estimación a partir de una estimación. ¿Esto es algo bueno de hacer? ¿Qué ventajas tienen los métodos de TD sobre los métodos Monte Carlo y DP? Desarrollar y responder a estas preguntas se llevará el resto de este libro y mucho más. En esta sección anticipamos brevemente algunas de las respuestas.</p>
<p>Obviamente, los métodos de TD tienen una ventaja sobre los métodos de DP en el sentido de que no requieren un modelo del entorno, de su recompensa y de las distribuciones de probabilidad del siguiente estado.</p>
<p>La siguiente ventaja más obvia de los métodos de TD sobre los métodos de Monte Carlo es que se implementan naturalmente en línea, de forma totalmente incremental. Con los métodos Monte Carlo hay que esperar hasta el final de un episodio, porque sólo entonces se conoce el retorno, mientras que con los métodos TD sólo hay que esperar un paso de tiempo. Sorprendentemente, a menudo esto resulta ser una consideración crítica. Algunas aplicaciones tienen episodios muy largos, por lo que retrasar todo el aprendizaje hasta el final del episodio es demasiado lento. Otras aplicaciones son tareas continuas y no tienen ningún episodio. Por último, como hemos señalado en el capítulo anterior, algunos métodos de Monte Carlo deben ignorar o descartar los episodios en los que se llevan a cabo acciones experimentales, lo que puede ralentizar enormemente el aprendizaje. Los métodos de TD son mucho menos susceptibles a estos problemas porque aprenden de cada transición independientemente de las acciones subsiguientes que se tomen.</p>
<p>Pero, ¿son los métodos de TD sólidos? Ciertamente es conveniente aprender una estimación de la siguiente, sin esperar un resultado real, pero ¿podemos garantizar la convergencia hacia la respuesta correcta? Afortunadamente la respuesta es sí. Para cualquier política fija <span class="math inline">\(\pi\)</span>, TD(0) ha demostrado que converge a <span class="math inline">\(v_\pi\)</span>, en la medida de un parámetro de tamaño de paso constante si es suficientemente pequeño, y con probabilidad 1 si el parámetro de tamaño de paso disminuye de acuerdo con las condiciones habituales de aproximación estocástica. La mayoría de las pruebas de convergencia se aplican sólo al caso basado en tablas del algoritmo presentado anteriormente, pero algunos también se aplican al caso de la aproximación general de la función lineal. Estos resultados se discuten en un contexto más general en el Capítulo 9.</p>
<p>Si los métodos de TD y Monte Carlo convergen asintóticamente a las predicciones correctas, entonces la siguiente pregunta natural es: “¿Cuál es el primero?”. En otras palabras, ¿qué método aprende más rápido? ¿Qué es lo que hace más eficiente el uso de datos limitados? En la actualidad, esta es una pregunta abierta en el sentido de que nadie ha sido capaz de demostrar matemáticamente que un método converge más rápido que el otro. De hecho, ni siquiera está claro cuál es la forma formal más apropiada de formular esta pregunta. En la práctica, sin embargo, se ha comprobado que los métodos de TD convergen más rápidamente que los métodos MC-<span class="math inline">\(\alpha\)</span> constante.</p>
</div>
<div id="calidad-de-td0" class="section level2">
<h2><span class="header-section-number">5.3</span> Calidad de TD(0)</h2>
<p>Suponga que sólo hay disponible una cantidad mínima de experiencia, digamos 10 episodios o 100 pasos temporales. En este caso, un enfoque común con los métodos de aprendizaje incremental es presentar la experiencia repetidamente hasta que el método converja. Dada una función de valor aproximado, <span class="math inline">\(V\)</span> , los incrementos especificados por <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha[G_t-V(S_t)]\]</span> y <span class="math display">\[V(S_t)\leftarrow V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})- V(S_t)]\]</span> se calculan para cada paso de tiempo <span class="math inline">\(t\)</span> en el que se visita un estado no terminal, pero la función de valor se cambia sólo una vez, por la suma de todos los incrementos. A continuación, toda la experiencia disponible se procesa de nuevo con la nueva función de valor para producir un nuevo incremento global, y así sucesivamente, hasta que la función de valor converge. Llamamos a esto actualización por lotes porque las actualizaciones se realizan sólo después de procesar cada lote completo de datos de entrenamiento.</p>
<p>En la actualiación por lotes, TD(0) converge de forma determinística a una respuesta única independiente del parámetro de paso, <span class="math inline">\(\alpha\)</span>, siempre y cuando sea tomado lo suficientemente pequeño. los métodos MC-<span class="math inline">\(\alpha\)</span> constante convergen deterministicamente bajo las mismas condiciones, pero no a lo mismo. Entender estas dos respuestas nos ayudará a entender la diferencia entre los dos métodos. Bajo la actualización normal, los métodos no se mueven hasta el final de sus respectivas respuestas por lotes, pero en cierto sentido toman medidas en estas direcciones. Antes de tratar de entender las dos respuestas en general, para todas las tareas posibles, primero veamos algunos ejemplos.</p>
<p><strong>Ejemplo: Random Walk.</strong></p>
<div class="figure">
<img src="~/Reinforcement-learning/rw.png" alt="" />
<p class="caption"></p>
</div>
<p>Un proceso de recompensa de Markov (MRP), es un proceso sin acciones. A menudo usaremos MRP cuando nos centramos en el problema de predicción, en el que no es necesario distinguir la dinámica debida al entorno de las debidas al agente. En este MRP, todos los episodios comienzan en el estado central. A menudo usaremos MRP cuando nos centramos en el problema de predicción, en el que no es necesario distinguir la dinámica debida al entorno de las acciones debidas al agente. En este MRP, todos los episodios comienzan en el estado central, C, luego se mueve a la izquierda o a la derecha de paso en paso, con igual probabilidad. Los episodios terminan en el extremo izquierdo o en el extremo derecho. Cuando un episodio termina a la derecha, se produce una recompensa de +1; todas las demás recompensas son cero. Debido a que esta tarea e discontinua, el verdadero valor de cada estado es la probabilidad de terminar a la derecha si comienza a partir de ese estado. Por lo tanto, el valor verdadero del estado central es <span class="math inline">\(v_\pi\)</span>(C) = 0.5. Los verdaderos valores de todos los estados, de la A a la E, son <span class="math inline">\(\frac{1}{6}\)</span>, <span class="math inline">\(\frac{2}{6}\)</span>, <span class="math inline">\(\frac{3}{6}\)</span>, <span class="math inline">\(\frac{4}{6}\)</span> y <span class="math inline">\(\frac{5}{6}\)</span>.</p>
<p>Las versiones de actualización por lotes de TD(0) y MC <span class="math inline">\(\alpha\)</span>-constante se aplicaron de la siguiente manera al ejemplo de predicción de caminata aleatoria. Después de cada nuevo episodio, todos los episodios vistos hasta ahora se trataron como un lote. Fueron presentados repetidamente al algoritmo, ya sea TD(0) o MC <span class="math inline">\(\alpha\)</span>-constante, con <span class="math inline">\(\alpha\)</span> lo suficienetemente pequeño para asegurar la convergencia. A continuación, se comparó la función de valor resultante con <span class="math inline">\(v_\pi\)</span>, y el error cuadratico medio a través de los 5 estados (y a través de 100 repeticiones independientes de todo el experimento) fue graficado para obtener las curvas de aprendizaje mostradas en la siguiente figura. Nótese que el método de TD por lotes fue consistentemente mejor que el método Monte Carlo por lotes.</p>
<div class="figure">
<img src="~/Reinforcement-learning/mcvstd.png" alt="" />
<p class="caption"></p>
</div>
<p>En el entrenamientos por lotes, MC <span class="math inline">\(\alpha-\)</span>constante converge hacia los valores, <span class="math inline">\(V(s)\)</span>, que son promedios de muestra de los rendimientos reales experimentados después de visitar cada estado. Estas son estimaciones óptimas en el sentido de que minimizan el error de los rendimientos reales en el conjunto de entrenamiento. En este sentido, es sorprendente que el método de TD por lotes haya sido capaz de funcionar mejor de acuerdo con la medida del error promedio que se muestra en la figura anterior. ¿Cómo es posible que el TD por lotes funcionara mejor que este método óptimo? La respuesta es que el método de Monte Carlo es óptimo sólo de forma limitada, y que TD es óptimo de una manera que es más relevante para predecir el retorno. Pero primero desarrollemos nuestra intuicion sobre los diferentes tipos de optimización a través de otro ejemplo.</p>
<p><strong>Ejemplo:</strong> Colóquese ahora en el papel de predictor de retornos para un proceso de recompensa Markov desconocido. Suponga que observa los siguientes ocho episodios: <img src="~/Reinforcement-learning/ejem2.png" alt="" /> Esto significa que el primer episodio comenzó en el estado A, pasó a B con una recompensa de 0, y luego terminó en B con una recompensa de 0. Los otros siete episodios fueron aún más cortos, comenzando desde B y terminando inmediatamente. Dada esta serie de datos, ¿cuáles diría que son las predicciones óptimas, los mejores valores para las estimaciones <span class="math inline">\(V\)</span>(A) y <span class="math inline">\(V\)</span>(B)? Probablemente todo el mundo estaría de acuerdo en que el valor óptimo para <span class="math inline">\(V\)</span>(B) es <span class="math inline">\(\frac{3}{4}\)</span> , porque seis de las ocho veces en el estado B el proceso terminó inmediatamente con un retorno de 1, y las otras dos veces en B el proceso terminó inmediatamente con un retorno de 0.</p>
<p>Pero, ¿cuál es el valor óptimo para la estimación <span class="math inline">\(V\)</span>(A) teniendo en cuenta estos datos? Aquí hay dos respuestas razonables. Una es observar que el 100% de las veces que el proceso estuvo en estado A pasó inmediatamente a B (con una recompensa de 0); y como ya hemos decidido que B tiene valor <span class="math inline">\(\frac{3}{4}\)</span>, por lo tanto A debe tener valor <span class="math inline">\(\frac{3}{4}\)</span> también. Una forma de ver esta respuesta es que se basa en la primera modelización del proceso de Markov, en este caso como se muestra en la siguiente figura, y luego en el cálculo de las estimaciones correctas dadas por el modelo, que de hecho en este caso da <span class="math inline">\(V\)</span>(A) = <span class="math inline">\(\frac{3}{4}\)</span>. Esta es también la respuesta que da el lote TD(0).</p>
<p><img src="~/Reinforcement-learning/bucle.png" alt="" /> La otra respuesta razonable es simplemente observar que hemos visto A una vez y el retorno que le siguió fue 0; por lo tanto estimamos <span class="math inline">\(V\)</span>(A) como 0. Esta es la respuesta que los métodos de Monte Carlo dan. Note que también es la respuesta que da el error mínimo al cuadrado en los datos de entrenamiento. De hecho, no da ningún error en los datos. Pero aún así esperamos que la primera respuesta sea mejor. Si el proceso es Markov, esperamos que la primera respuesta produzca menos error en los datos futuros, aunque la respuesta de Monte Carlo es mejor en los datos existentes.</p>
<p>El ejemplo anterior ilustra una diferencia general entre las estimaciones encontradas por los métodos TD(0) y Monte Carlo. Los métodos Monte Carlo por lotes siempre indican las estimaciones que minimizan el error cuadratico medio en el conjunto de entrenamiento, mientras que el lote TD(0) siempre contiene las estimaciones que serían exactamente correctas para el modelo de máxima verosimilitud del proceso de Markov. En general, la estimación por máxima verosimilitud den un parámetro es el valor del parámetro cuya probabilidad de generar los datos es más grande. En este caso, la estimación de la máxima verosimilitud es el modelo del proceso de Markov formado de manera obvia a partir de los episodios observados: la probabilidad de transición estimada de <span class="math inline">\(i\)</span> a <span class="math inline">\(j\)</span> es la fracción de transiciones observadas de <span class="math inline">\(i\)</span> que fueron a <span class="math inline">\(j\)</span>, y la recompensa esperada asociada es la de las recompensas observadas en esas transiciones. Dado este modelo, podemos calcular la estimación de la función de valor que sería exactamente correcta si el modelo fuera exactamente correcto. Esto se denomina estimación de equivalencia por certeza porque equivale a suponer que la estimación del proceso subyacente se conocía con certeza en lugar de ser aproximada. En general, el lote TD(0) converge hacia la estimación de equivalencia por certeza.</p>
<p>Esto ayuda a explicar por qué los métodos de TD convergen más rápidamente que los métodos de Monte Carlo. En forma de lote, TD(0) es más rápido que los métodos de Monte Carlo porque calcula la verdadera estimación de equivalencia por certeza. Esto explica la ventaja de TD(0) que se muestra en los resultados del lote en la tarea de caminata aleatoria. La relación con la estimación de la equivalencia ppor certeza también puede explicar en parte la ventaja de la velocidad de los TD(0) no por lotes. Aunque los métodos nque no son por lotes no logran ni la equivalencia dpor certeza ni las estimaciones por reducción de mínimos cuadrados, se puede entender que se mueven aproximadamente en estas direcciones. El TD(0) que no es por lotes puede ser más rápido que el MC <span class="math inline">\(\alpha\)</span>-constante porque se está moviendo hacia una mejor estimación, a pesar de que no está llegando hasta el final. En la actualidad no se puede decir nada más denso sobre la relativa eficiencia de los métodos de TD en línea y Monte Carlo.</p>
</div>
<div id="sarsa-td-politico-con-control" class="section level2">
<h2><span class="header-section-number">5.4</span> Sarsa: TD político con control</h2>
<p>Pasamos ahora al uso de métodos de predicción de TD para el problema de control. Como de costumbre, seguimos el patrón de iteración de políticas generalizadas (GPI), sólo que esta vez utilizando métodos de TD para la parte de evaluación o predicción. Al igual que con los métodos de Monte Carlo, nos enfrentamos a la necesidad de negociar con la exploración y la explotación, y de nuevo los enfoques caen en dos clases principales: sobre políticas y sobre no políticas. En esta sección presentamos un método de control político de TD.</p>
<p>El primer paso es aprender una función de valor-acción en lugar de una función de valor estado. En particular, para un método político debemos estimar <span class="math inline">\(q_\pi(s, a)\)</span> para el actual comportamiento de la política <span class="math inline">\(\pi\)</span> y para todos los estados s y acciones a. Esto puede hacerse usando esencialmente el mismo método de TD descrito anteriormente para el aprendizaje de <span class="math inline">\(v_\pi\)</span>. Recordemos que un episodio consiste en una secuencia alterna de estados y pares de estados-acción</p>
<div class="figure">
<img src="~/Reinforcement-learning/sarsa.png" alt="" />
<p class="caption"></p>
</div>
<p>En la sección anterior consideramos las transiciones de estado a estado y aprendimos los valores de los estados. Ahora consideramos las transiciones del par de acción-estado al par de acción-estado, y aprendemos los valores de los pares de acción-estado. Formalmente estos casos son idénticos: ambos son cadenas de Markov con un proceso de recompensa. Los teoremas que aseguran la convergencia de los valores de estado bajo TD(0) también se aplican al algoritmo correspondiente para los valores de acción:</p>
<p><span class="math display">\[Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\alpha Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\]</span></p>
<p>Esta actualización se realiza después de cada transición desde una estado <span class="math inline">\(S_t\)</span> no terminal. Si <span class="math inline">\(S_{t+1}\)</span> es terminal, entonces <span class="math inline">\(Q(S_{t+1},A_{t+1})\)</span> es definido como cero. Esta regla usa cada elemento de un vector quintuple <span class="math inline">\((S_t, A_t, R, S_{t+1}, A_{t+1})\)</span>, que hacen una transición de un estado a otro. Este vector quíntuple da lugar al nombre de Sarsa para el algoritmo. El diagrama de respaldo de Sarsa es el que se muestra acontinuaciuón.</p>
<div class="figure">
<img src="~/Reinforcement-learning/sarsa1.png" alt="" />
<p class="caption"></p>
</div>
<p>Es sencillo diseñar un algoritmo de control de políticas basado en el método de predicción de Sarsa. Como en todos los métodos politicos, continuamente estimamos <span class="math inline">\(q_\pi\)</span> para el comportamiento de la política <span class="math inline">\(\pi\)</span>, y al mismo tiempo cambiamos hacia la codicia con respecto a <span class="math inline">\(q\)</span>. La forma general del algoritmo de control de Sarsa se da acontinuación.</p>
<p><strong>Sarsa, para estimar <span class="math inline">\(Q\approx q_{*}\)</span></strong></p>
<ul>
<li><p>Inicializamos <span class="math inline">\(Q(s,a)\)</span>, para todo <span class="math inline">\(s\in S\)</span>, <span class="math inline">\(a\in A(s)\)</span>, arbitrariamente, y <span class="math inline">\(Q\)</span>(estado terminal,<span class="math inline">\(\cdotp\)</span>)=0</p></li>
<li><p>Repetimos (Para cada episodio):</p>
<ul>
<li>Iniciamos <span class="math inline">\(S\)</span> _ Escogemos <span class="math inline">\(A\)</span> a partir de <span class="math inline">\(S\)</span> usando una politica resultanten de <span class="math inline">\(Q\)</span> (por ejemplo <span class="math inline">\(\epsilon\)</span>-codicioso)</li>
<li>Repetimos (para cada episodio):
<ul>
<li>Tomamos una acción <span class="math inline">\(A\)</span>, observamos <span class="math inline">\(R\)</span>, <span class="math inline">\(S&#39;\)</span></li>
<li>Escogemos <span class="math inline">\(A&#39;\)</span> a partir <span class="math inline">\(S&#39;\)</span> usando una politica resultanten de <span class="math inline">\(Q\)</span> (por ejemplo <span class="math inline">\(\epsilon\)</span>-codicioso)</li>
<li><span class="math inline">\(Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\alpha Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\)</span></li>
<li><span class="math inline">\(S \leftarrow S&#39;,\quad A\leftarrow A&#39;\)</span></li>
</ul></li>
<li>Hasta que <span class="math inline">\(S\)</span> sea terminal</li>
</ul></li>
</ul>
<p>Las propiedades de convergencia del algoritmo de Sarsa dependen de la naturaleza de la dependencia de la política proveniente de Q. Por ejemplo, se podrían utilizar políticas “codiciosas” o “blandas”. Sarsa converge con probabilidad 1 hacia una política óptima y una función de valor-acción siempre y cuando todos los pares de acción-estado sean visitados un número mínimo de veces y la política converja en el límite de la política codiciosa.</p>
<p><strong>Ejemplo:</strong> En la siguiente figura se muestra una cuadrícula estándar, con estados de inicio y de meta, pero con una diferencia: hay un viento cruzado hacia arriba a través de la mitad de la cuadrícula. Las acciones son las cuatro estándar: arriba, abajo, derecha e izquierda, pero en la región media los siguientes estados resultantes se desplazan hacia arriba por un viento“, cuya fuerza varía de una columna a otra. La fuerza del viento se indica debajo de cada columna, en número de celdas desplazadas hacia arriba. Por ejemplo, si usted está una celda a la derecha de la meta, entonces la acción a la izquierda le lleva a la celda justo encima de la meta. Tratemos esto como una tarea episódica no descontada, con recompensas constantes de 1 hasta que se alcance el estado de meta.</p>
<p><img src="~/Reinforcement-learning/gridd22.png" alt="" /> El gráfico en la figura muestra el resultado de aplicar el algoritmo Sarsa <span class="math inline">\(\epsilon\)</span>-codicioso a estsa prueba, con <span class="math inline">\(\epsilon=1\)</span>, <span class="math inline">\(\alpha=0.5\)</span> y valores inicial <span class="math inline">\(Q(s,a)=0\)</span> para todos <span class="math inline">\(s,a\)</span>. La pendiente creciente del gráfico muestra que el objetivo se alcanza cada vez más rápidamente con el paso del tiempo. En 8000 pasos de tiempo, la política codiciosa era desde hace mucho tiempo óptima (una trayectoria a partir de ella se muestra en el recuadro); Continuando la exploración <span class="math inline">\(\epsilon\)</span>-codiciosa se mantuvo la duración media del episodio en unos 17 pasos, dos más que el mínimo de 15. Tenga en cuenta que los métodos de Monte Carlo no se pueden ser utilizados fácilmente en esta tarea porque la finalización no está garantizada para todas las políticas. Si alguna vez se encontrara una política que causara que el agente permaneciera en el mismo estado, entonces el siguiente episodio nunca terminaría. Los métodos de aprendizaje paso a paso como el de Sarsa no tienen este problema porque aprenden rápidamente durante el episodio que tales políticas son deficientes, y cambian a otra cosa.</p>
</div>
<div id="q-learning-td-no-politico-con-control" class="section level2">
<h2><span class="header-section-number">5.5</span> Q-Learning: TD no político con control</h2>
<p>Uno de los primeros avances en el aprendizaje de refuerzo fue el desarrollo de un algoritmo de control de TD no político conocido como Q-learning. Definido por:<span class="math display">\[Q(S_t,A_t) \leftarrow Q(S_t,A_t)+ \alpha[R_{t+1}+\gamma max_{a}Q(S_{t+1},a)-Q(S_t,A_t)]\]</span></p>
<p>En este caso, la función del valor de la acción aprendida, <span class="math inline">\(Q\)</span>, se aproxima directamente a <span class="math inline">\(q_*\)</span>, la función del valor de la acción óptima, independientemente de la política que se siga. Esto simplifica drásticamente el análisis del algoritmo y permite realizar pruebas de convergencia tempranas. La política todavía tiene un efecto en el sentido de que determina qué pares de acciones de estado se visitan y actualizan. Como observamos en el Capítulo 5, este es un requisito mínimo para que cualquier método garantice un comportamiento óptimo. Bajo este supuesto y una variante de las condiciones habituales de aproximación estocástica sobre la sucesión de los parámetros paso, se ha demostrado que <span class="math inline">\(Q\)</span> converge con probabilidad 1 a <span class="math inline">\(q_*\)</span>.</p>
<p><strong>Q-learning para estimar <span class="math inline">\(\pi\approx \pi_*\)</span></strong></p>
<ul>
<li>Iniciamos <span class="math inline">\(Q(s,a)\)</span>, para todo <span class="math inline">\(s\in S\)</span>, <span class="math inline">\(a\in A(s)\)</span>, arbitrariamente, y <span class="math inline">\(Q\)</span>(estado terminal, <span class="math inline">\(\cdotp\)</span>)=0</li>
<li>Repetimos (para cada episodio):
<ul>
<li>Iniciamos <span class="math inline">\(S\)</span></li>
<li>Repetimos (Para cada paso del episodio):
<ul>
<li>Escogemos <span class="math inline">\(A\)</span> de <span class="math inline">\(S\)</span> usando la política obtenida mediante <span class="math inline">\(Q\)</span> (por ejemplo usando, <span class="math inline">\(\epsilon\)</span>-codiciosos)</li>
<li>Tomamos una acción <span class="math inline">\(A\)</span>, Obervamos <span class="math inline">\(R\)</span>, <span class="math inline">\(S&#39;\)</span></li>
<li><span class="math inline">\(Q(S_t,A_t) \leftarrow Q(S_t,A_t)+ \alpha[R_{t+1}+\gamma max_{a}Q(S_{t+1},a)-Q(S_t,A_t)]\)</span></li>
<li><span class="math inline">\(S\leftarrow S&#39;\)</span></li>
</ul></li>
<li>Hasta que <span class="math inline">\(S\)</span> sea un estrado terminal.</li>
</ul></li>
</ul>
<p>¿Cuál es el diagrama de respaldo para Q-learning? La regla actualiza un par acción-estado, por lo que el nodo superior, la raíz de la actualización, debe ser un nodo de acción pequeño y completo. La actualización es también de los nodos de acción, maximizando todas las acciones posibles en el siguiente estado. Por lo tanto, los nodos inferiores del diagrama de respaldo deben ser todos estos nodos de acción</p>
<p><strong>Ejemplo:</strong> Este ejemplo se compara Sarsa y <span class="math inline">\(Q\)</span>-learning, destacando la diferencia entre los métodos políticos (Sarsa) y no políticos (Q-learning). Considere la cuadricula mostrado en la parte superior de la siguiente figura. Esta es una tarea estándar, episódica y no descontada, con estados de inicio y objetivo, y las acciones usuales que causan movimiento hacia arriba, hacia abajo, hacia la derecha y hacia la izquierda. La recompensa es de 1 en todas las transiciones, excepto en las de la región marcada como “El Acantilado”. Al entrar en esta región se obtiene una recompensa de 100 y el agente regresa instantáneamente al comienzo.</p>
<div class="figure">
<img src="~/Reinforcement-learning/ql.png" alt="" />
<p class="caption"></p>
</div>
<p>La parte inferior de la Figura anterior muestra el rendimiento de los métodos Sarsa y Q-learning con la selección de acciones <span class="math inline">\(\epsilon\)</span>-codiciosas, <span class="math inline">\(\epsilon=0.1\)</span> Después de una transición inicial, <span class="math inline">\(Q\)</span>-learning aprende valores para la política óptima, lo que viaja a lo largo del borde del cruadro. Desafortunadamente, esto resulta en que ocasionalmente caiga sobre el cuadro debido a la “selección de acciones codiciosas”. Sarsa, por su parte, tiene en cuenta la selección de acciones y aprende el camino más largo pero más seguro a través de la parte superior de la cuadricula. Aunque el <span class="math inline">\(Q\)</span>-learning aprende realmente los valores de la política óptima, su rendimiento es peor que el de Sarsa, que aprende la política de los bordes. Por supuesto, si &quot; se redujeran gradualmente, entonces ambos métodos convergerían asintóticamente hacia la política óptima.</p>
</div>
<div id="sarsa-esparada" class="section level2">
<h2><span class="header-section-number">5.6</span> Sarsa esparada</h2>
<p>Considere el algoritmo de aprendizaje que es igual que <span class="math inline">\(Q\)</span>-learning, excepto que en lugar del máximo en los pares de estado-acción, utiliza el valor esperado, teniendo en cuenta la probabilidad de que cada acción se realice bajo la política actual. Es decir, considere el algoritmo con la regla de actualización: <span class="math inline">\(\begin{equation} \begin{split} Q(S_t,A_t) &amp; = Q(S_t,A_t)+\alpha[R_{t+1}+\gamma E[Q(S_{t+1},A_{t+1})|S_{t+1}]-Q(S_t,A_t)] \\  &amp; = Q(S_t,A_t)+\alpha[R_{t+1}+\gamma \sum_a \pi(a|S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)] \end{split} \end{equation}\)</span></p>
<p>pero que por lo demás sigue el esquema del aprendizaje <span class="math inline">\(Q\)</span> learning. Dado el proximo estado, <span class="math inline">\(S_{t+1}\)</span>, este algoritmo se mueve deterministicamnete en la misma direccion como Sarsa en esperanza, y por esto se llama <em>Esperanza de Sarsa</em>, el diagrama de respaldo se muestra en la siguiente figura</p>
<div class="figure">
<img src="~/Reinforcement-learning/se.png" alt="" />
<p class="caption"></p>
</div>
<p>El algoritmo de Sarsa esperado, es mas complejo computacionalmente que Sarsa pero, en retorno, el elimina el sesgo ocacionado por la escogencia aleatoria de <span class="math inline">\(A_{t+1}\)</span>, dada la misma cantidad de experiencia uno pudiera esperar que el desarrollo fuera ligeramente mejor que el de Sarsa, y en efecto generalmente lo es. La siguiente imagen muestra un resu men sobre los métodos aplicados al ejemplo anterior, en el cual se comparan el Sarsa esperado con Sarsa y <span class="math inline">\(Q\)</span>-learning. El Sarsa Esperado tiene claramente la ventaja. Ademas, Sarsa esperado muestra una mejora significativa con respecto a Sarsa en una amplia gama de valores en el parámetro de paso. En el ejemplo anterior, las transiciones de estado son todas determinista y toda la aleatoriedad proviene de la política. En tales casos, Sarsa esperado puede fijar con seguridad <span class="math inline">\(\alpha\)</span>= 1 sin sufrir ninguna degradación de la convergencia asintótica. mientras que Sarsa sólo puede tener un buen rendimiento a largo plazo con un pequeño valor de <span class="math inline">\(\alpha\)</span> , en el que el retorno a corto plazo es deficiente. En este y otros ejemplos hay una ventaja empírica consistente de la Sarsa esperado sobre el Sarsa.</p>
<div class="figure">
<img src="~/Reinforcement-learning/esa.png" alt="" />
<p class="caption"></p>
</div>
<p>En estos resultados, Sarsa esperada fue usado como un método político, pero en general podría utilizar una política diferente a la política objetivo <span class="math inline">\(\pi\)</span> para generar comportamiento, en cuyo caso se convierte en un algoritmo no política. Por ejemplo, supongamos que <span class="math inline">\(\pi\)</span> es la política codiciosa mientras que el comportamiento es más exploratorio; entonces Sarsa esperado es exactamente Q-learning. En este sentido, Sarsa esperado absorbe y generaliza el <span class="math inline">\(Q\)</span>-learning a la vez que mejora de forma fiable con respecto a Sarsa. Excepto por el pequeño costo computacional adicional, Sarsa esperado puede dominar completamente los otros dos algoritmos de control de TD más conocidos.</p>
</div>
<div id="sesgo-de-maximizacion-y-doble-aprendizaje" class="section level2">
<h2><span class="header-section-number">5.7</span> Sesgo de maximización y doble aprendizaje</h2>
<p>Todos los algoritmos de control que hemos discutido hasta ahora involucran la maximización en la construcción de sus políticas de objetivos. Por ejemplo, en <span class="math inline">\(Q\)</span>-learning la política de objetivos es la política codiciosa dados los valores de acción actuales, que se definida con un máximo, y en Sarsa la política es a menudo <span class="math inline">\(\epsilon\)</span>-codiciosa, lo que también implica una operación de maximización. En estos algoritmos, se utiliza implícitamente un valor máximo sobreestimado como estimación del valor máximo, lo que puede dar lugar a un sesgo positivo del agente. Para ver por qué, considere un solo estado s donde hay muchas acciones a cuyos valores verdaderos, <span class="math inline">\(q(s,a)\)</span>, son todos cero pero cuyos valores estimados, <span class="math inline">\(Q(s, a)\)</span>, son inciertos y por lo tanto distribuidos algunos por encima y otros por debajo de cero. El máximo de los valores reales es cero, pero el máximo de las estimaciones es positivo, un sesgo positivo. A esto lo llamamos sesgo de maximización.</p>
<p><strong>Ejemplo de sesgo de maximización:</strong></p>
<p>El pequeño MDP que se muestra en la proxima figura proporciona un ejemplo simple de cómo el sesgo de maximización puede dañar el rendimiento de los algoritmos de control de TD. El MDP tiene dos estados no terminales A y B. Los episodios siempre comienzan en A con la posibilidad de elegir entre dos acciones, izquierda y derecha. La acción correcta pasa inmediatamente al estado terminal con una recompensa y un retorno de cero. La acción izquierda pasa a B, también con una recompensa de cero, de la cual hay muchas acciones posibles, todas las cuales causan la terminación inmediata con una recompensa extraída de una distribución normal con una media de <span class="math inline">\(-0.1\)</span> y una varianza de <span class="math inline">\(1\)</span>. Por lo tanto, el retorno esperado para cualquier trayectoria que comience con izquierda es de <span class="math inline">\(-0.1\)</span>, y por lo tanto, llevarla a la izquierda en el estado A es siempre un error. Sin embargo, nuestros métodos de control pueden favorecer a la izquierda debido al sesgo de maximización que hace que B parezca tener un valor positivo. La Figura anterior muestra que el <span class="math inline">\(Q\)</span>-learning con “selección de acciones <span class="math inline">\(\epsilon\)</span>-codiciosas” aprende inicialmente a favorecer fuertemente de la acción de la izquierda en este ejemplo. Incluso asintoticamente, el <span class="math inline">\(Q\)</span>-learning realiza la acción de la izquierda un 5% más a menudo de lo que es óptimo en nuestros ajustes de parámetros (<span class="math inline">\(\epsilon = 0.1\)</span>, <span class="math inline">\(\alpha= 0.1\)</span>, <span class="math inline">\(\gamma = 1\)</span>).</p>
<div class="figure">
<img src="~/Reinforcement-learning/ejemsa.png" alt="" />
<p class="caption"></p>
</div>
<p>¿Existen algoritmos que eviten el sesgo de maximización? Para empezar, consideremos un caso de bandido en el que tenemos estimaciones ruidosas del valor de cada una de las acciones, obtenidas como muestra de los promedios de las recompensas recibidas en todos los juegos con cada acción. Como hemos discutido anteriormente, habrá un sesgo de maximización positivo si utilizamos el máximo de las estimaciones como una estimación del máximo de los valores reales. Una forma de ver el problema se debe al uso de las mismas muestras (jugadas) tanto para determinar la maximización y para estimar su valor. Supongamos que dividimos las jugadas en dos grupos y las usamos para aprender dos estimaciones independientes, llámelos <span class="math inline">\(Q_1(a)\)</span> y <span class="math inline">\(Q_2(a)\)</span>, cada uno una estima el valor verdadero <span class="math inline">\(q(a)\)</span>, para todos <span class="math inline">\(a \in A\)</span>. Nosotros podriamos usar una estimación, por ejemplo <span class="math inline">\(Q_1(a)\)</span>, para determinar la acción maximizadora <span class="math inline">\(A^*=argmax_aQ_1(a)\)</span> y el otro termino <span class="math inline">\(Q_2\)</span>, para proporcionar la estimación de su valor, <span class="math inline">\(Q_2(A^*)=Q_2(argmax_aQ_1(a))\)</span>. Esta estimación será entonces imparcial en el sentido de que <span class="math inline">\(E[Q_2(A^*)]=q(A^*)\)</span>. También podemos repetir el proceso con el papel de las dos estimaciones a la inversa para obtener una segunda estimación imparcial <span class="math inline">\(Q_1(argmax_aQ_2(a))\)</span>. Esta es la idea del doble aprendizaje. Tengamos en cuenta que aunque aprendemos dos estimaciones, sólo se actualiza una estimación en cada juego, el aprendizaje doble duplica los requisitos de memoria, pero no aumenta la cantidad de cálculo por paso. La idea del doble aprendizaje se extiende naturalmente a los algoritmos para MDPs completos. Por ejemplo, el algoritmo de doble aprendizaje análogo al <span class="math inline">\(Q\)</span>-learning, llamado Doble <span class="math inline">\(Q\)</span>-learning, divide los pasos de tiempo en dos, quizás tirando una moneda en cada paso. Si la moneda sale cara, la actualización es<span class="math display">\[Q_1(S_t,A_t)\leftarrow Q_1(S_t,A_t) + \alpha[R_{t+1}+\gamma Q_2(S_{t+1},argmax_aQ_1(S_{t+1},a)-Q_1(S_{t},A_t))]\]</span></p>
<p>Si la moneda sale cruz, entonces la misma actualización se hace con <span class="math inline">\(Q_1\)</span> y <span class="math inline">\(Q_2\)</span> conmutado, de modo que Q2 se actualiza. Las dos funciones de valor aproximado se tratan de forma completamente simétrica. La política del comportamiento puede utilizar ambas estimaciones de valor de la acción. Por ejemplo, una “política <span class="math inline">\(\epsilon\)</span>-codiciosa para el Doble <span class="math inline">\(Q\)</span>-learning podría basarse sobre la media (o la suma) de las dos estimaciones del valor de la acción. Un algoritmo completo para el Double Q-learning se da a continuación. Este es el algoritmo utilizado para producir los resultados de la Figura anterior. En ese ejemplo, El doble aprendizaje parece eliminar el daño causado por el sesgo de maximización. Por supuesto que también hay dobles versiones de Sarsa y sarsa esperado.</p>
<p><strong>Doble <span class="math inline">\(Q\)</span>-Learning</strong></p>
<ul>
<li>Iniciamos <span class="math inline">\(Q_1(s,a)\)</span> y <span class="math inline">\(Q_2(s,a)\)</span>, para todo <span class="math inline">\(s\in S\)</span>, <span class="math inline">\(a\in A(s)\)</span>, arbitrariamente.</li>
<li>Iniciamos <span class="math inline">\(Q_1(\)</span>estado terminal,<span class="math inline">\(\cdot)=Q_1(\)</span>estado terminal,<span class="math inline">\(\cdot)=0\)</span></li>
<li>Repetimos (para cada episodio):
<ul>
<li>Iniciamos <span class="math inline">\(S\)</span></li>
<li>Repetimos (para cada paso del episodio)
<ul>
<li>Escogemos <span class="math inline">\(A\)</span> desde <span class="math inline">\(S\)</span> usando una política obtenida a partir <span class="math inline">\(Q_1\)</span> y <span class="math inline">\(Q_2\)</span></li>
<li>Tomamos lam acción <span class="math inline">\(A\)</span>, observamos <span class="math inline">\(R\)</span>, <span class="math inline">\(S&#39;\)</span></li>
<li>Con probabilidad 0.5<span class="math display">\[Q_1(S_t,A_t)\leftarrow Q_1(S_t,A_t) + \alpha[R_{t+1}+\gamma Q_2(S_{t+1},argmax_aQ_1(S_{t+1},a)-Q_1(S_{t},A_t))]\]</span></li>
<li>Sino <span class="math display">\[Q_2(S_t,A_t)\leftarrow Q_2(S_t,A_t) + \alpha[R_{t+1}+\gamma Q_1(S_{t+1},argmax_aQ_2(S_{t+1},a)-Q_2(S_{t},A_t))]\]</span></li>
<li><span class="math inline">\(S\leftarrow S&#39;\)</span></li>
</ul></li>
<li>Hasta que <span class="math inline">\(S\)</span> sea terminal.</li>
</ul></li>
</ul>
</div>
<div id="juegos-afterstates-y-otros-casos-especiales" class="section level2">
<h2><span class="header-section-number">5.8</span> Juegos, afterstates y otros casos especiales</h2>
<p>En este libro tratamos de presentar un enfoque uniforme para una amplia clase de tareas, pero por supuesto siempre hay tareas excepcionales que son mejor tratadas de una manera especializada. Por ejemplo, nuestro enfoque general implica el aprendizaje de una función de valor-acción, pero en el Capítulo 1 presentamos un método de TD para aprender a jugar al tic-tac-tac-toe que aprendió algo mucho más parecido a una función de valor de estado. Si miramos de cerca ese ejemplo, se hace evidente que la función aprendida no tiene ni una función de valor-acción ni una función de valor de estado en el sentido usual. Una función convencional de valor estado evalúa los estados en los que el agente tiene la opción de seleccionar una acción, pero la función de valor estado utilizada en tic-tac-toe evalúa las posiciones del tablero después de que el agente haya hecho su movimiento. Llamemos a estos estados subsiguientes, y a las funciones de valor por encima de estas, funciones de valor de estado afterstate. Los estados posteriores son útiles cuando tenemos conocimiento de una parte inicial de la dinámica del entorno, pero no necesariamente de la dinámica completa. Por ejemplo, en los juegos normalmente conocemos los efectos inmediatos de nuestros movimientos. Sabemos para cada posible jugada de ajedrez cuál será la posición resultante, pero no cómo responderá nuestro oponente. Las funciones de valor afterstate son una forma natural de aprovechar este tipo de conocimiento y, por lo tanto, producir un aprendizaje más eficiente.</p>
<p>La razón por la que es más eficiente diseñar algoritmos en términos afterstate es evidente en el ejemplo del tic-tac-toe. Una función convencional de valor de acción trazaría un mapa desde las posiciones y movimientos hasta una estimación del valor. Pero muchos pares de posición producen la misma posición resultante, como en este ejemplo:</p>
<div class="figure">
<img src="~/Reinforcement-learning/tict.png" alt="" />
<p class="caption"></p>
</div>
<p>En tales casos las posición son diferentes pero producen la misma posición posterior, y por lo tanto deben tener el mismo valor. Una función convencional de valor acción tendría que evaluar por separado ambos pares, mientras que una función de valor afterstate evaluaría inmediatamente ambos por igual. Cualquier aprendizaje sobre la posición mover par a la izquierda se transferiría inmediatamente al par a la derecha.</p>
<p>Los afterstate surgen en muchas tareas, no sólo en los juegos. Por ejemplo, en las tareas en cola hay acciones tales como asignar clientes a los servidores, rechazar clientes o descartar información. En tales casos, las acciones se niegan en términos de sus efectos inmediatos, que son completamente conocidos.</p>
<p>Es imposible describir todos los tipos posibles de problemas especializados y los correspondientes problemas especializados y sus algoritmos de aprendizaje. Sin embargo, los principios desarrollados en este libro deben aplicarse ampliamente. Por ejemplo, los métodos afterstate se siguen describiendo adecuadamente en términos de la iteración generalizada de políticas, con una política y (afterstate) interactuando esencialmente de la misma manera. En muchos casos uno todavía se enfrentará a la elección entre métodos de políticos y métodos no políticos para gestionar la necesidad de una exploración persistente.</p>

</div>
</div>
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-78759535-1', 'auto');
ga('send', 'pageview');  
</script>
            </section>

          </div>
        </div>
      </div>
<a href="metodos-de-montecarlo.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bootstrapping-en-n-pasos.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/synergyvision/Analisis_y_Medida/edit/master/bookdown/203-capitulo5.Rmd",
"text": "Edit"
},
"download": ["Reinforcement Learning.pdf"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
